{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS-ML Phase 1: Hyperparameter Tuning\n",
    "\n",
    "Questo notebook esegue il tuning degli iperparametri per un algoritmo specifico.\n",
    "\n",
    "## Parametri\n",
    "\n",
    "- `TARGET_ALGO`: Algoritmo da ottimizzare (random_forest, xgboost, lightgbm)\n",
    "- `TUNING_METHOD`: Metodo di ricerca (random, bayesian)\n",
    "- `N_ITERATIONS`: Numero iterazioni/trials\n",
    "- `CV_FOLDS`: Fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: xgboost\n",
      "Iterations: None\n",
      "CV: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURAZIONE\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_ALGO = 'xgboost'  # random_forest, xgboost, lightgbm\n",
    "\n",
    "# probabilmente ha senso ignorae il numero di trials e lasciare girare optuna con il limite di tempo (ne fa più che riesce almeno)\n",
    "N_ITERATIONS = 5000\n",
    "\n",
    "# sicurezza massima: CV=5\n",
    "# esplorare meglio lo spazio: CV=3 (più trials nello stesso tempo)\n",
    "CV_FOLDS = 5\n",
    "TASK = 'binary'\n",
    "TIMEOUT_HOURS = 11\n",
    "MAX_RUNTIME_HOURS = 11.5\n",
    "\n",
    "print(f\"Target: {TARGET_ALGO}\")\n",
    "print(f\"Iterations: {N_ITERATIONS}\")\n",
    "print(f\"CV: {CV_FOLDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente: local\n",
      "Project root: /home/enea/Desktop/NIDS-ML-SSR2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP AMBIENTE\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"/kaggle/input\").exists():\n",
    "    ENV = \"kaggle\"\n",
    "    PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "    DATA_INPUT = Path(\"/kaggle/input\")\n",
    "else:\n",
    "    ENV = \"local\"\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Ambiente: {ENV}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLONE REPOSITORY\n",
    "# ============================================================================\n",
    "\n",
    "REPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    print(f\"Cloning {REPO_URL}...\")\n",
    "    !rm -rf temp_repo 2>/dev/null\n",
    "    !git clone --depth 1 {REPO_URL} temp_repo 2>/dev/null\n",
    "    !cp -r temp_repo/* {PROJECT_ROOT}/ 2>/dev/null\n",
    "    !rm -rf temp_repo\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    if (PROJECT_ROOT / \"requirements.txt\").exists():\n",
    "        !pip install -q -r {PROJECT_ROOT}/requirements.txt 2>/dev/null\n",
    "        !pip install -q optuna 2>/dev/null\n",
    "\n",
    "for d in [\"data/raw\", \"data/processed\", \"artifacts\", \"tuning_results\", \"logs\"]:\n",
    "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV disponibili: 8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COPIA DATASET CSV\n",
    "# ============================================================================\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    patterns = [\"cicids2017\", \"cic-ids\", \"ids2017\", \"network-intrusion\"]\n",
    "    for p in DATA_INPUT.iterdir():\n",
    "        if \"pcap\" in p.name.lower():\n",
    "            continue\n",
    "        if any(pat in p.name.lower() for pat in patterns) and list(p.glob(\"**/*.csv\")):\n",
    "            print(f\"Dataset CSV: {p.name}\")\n",
    "            import shutil\n",
    "            for csv in p.glob(\"**/*.csv\"):\n",
    "                dest = PROJECT_ROOT / \"data\" / \"raw\" / csv.name\n",
    "                if not dest.exists():\n",
    "                    shutil.copy(csv, dest)\n",
    "            break\n",
    "\n",
    "print(f\"CSV disponibili: {len(list((PROJECT_ROOT / 'data' / 'raw').glob('*.csv')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati gia processati.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "processed_test = PROJECT_ROOT / \"data\" / \"processed\" / \"test.parquet\"\n",
    "\n",
    "if not processed_test.exists():\n",
    "    print(\"Preprocessing...\")\n",
    "    !python src/preprocessing.py --balance-ratio 2.0 --n-jobs 4\n",
    "else:\n",
    "    print(\"Dati gia processati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts gia presenti.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "if not (PROJECT_ROOT / \"artifacts\" / \"scaler.pkl\").exists():\n",
    "    print(\"Feature engineering...\")\n",
    "    !python src/feature_engineering.py --n-features 30 --n-jobs 4\n",
    "else:\n",
    "    print(\"Artifacts gia presenti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "Modello:      xgboost\n",
      "Metrica:      70% F2-Score + 30% Latency (composite)\n",
      "Task:         binary\n",
      "CV:           5\n",
      "Max Latency:  1.0ms/sample\n",
      "CPU:          14/16\n",
      "N trials:     100\n",
      "\n",
      "1. Caricamento dati...\n",
      "2026-01-25 17:10:39 | INFO     | Caricati: train=706,632, val=151,422, test=151,422\n",
      "2. Preparazione feature...\n",
      "2026-01-25 17:10:39 | INFO     | Caricati artifacts da /home/enea/Desktop/NIDS-ML-SSR2/artifacts\n",
      "   Shape: (706632, 30)\n",
      "   NOTA: Misura latency durante CV, rallenta il processo\n",
      "2026-01-25 17:10:39 | INFO     | Bayesian Optimization (Optuna): 100 trials, cv=5\n",
      "2026-01-25 17:10:39 | INFO     | Metrica: 70% F2-Score + 30% Latency\n",
      "2026-01-25 17:10:39 | INFO     | Max latency constraint: 1.0ms/sample\n",
      "\u001b[33m[W 2026-01-25 17:11:00,892]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 1935, 'max_depth': 20, 'learning_rate': 0.06504856968981275, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182, 'min_child_weight': 2, 'gamma': 0.2904180608409973, 'reg_alpha': 0.6245760287469893, 'reg_lambda': 0.002570603566117598, 'scale_pos_weight': 7.372653200164409} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/src/hyperparameter_tuning.py\", line 267, in objective\n",
      "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scorer, n_jobs=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 651, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 373, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 91, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 184, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1808, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-01-25 17:11:00,896]\u001b[0m Trial 0 failed with value None.\u001b[0m        \n",
      "  0%|                                                   | 0/100 [00:21<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/src/hyperparameter_tuning.py\", line 526, in <module>\n",
      "    main()\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/src/hyperparameter_tuning.py\", line 487, in main\n",
      "    results = tune_bayesian_optuna(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/src/hyperparameter_tuning.py\", line 276, in tune_bayesian_optuna\n",
      "    study.optimize(\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/study.py\", line 490, in optimize\n",
      "    _optimize(\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 68, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 165, in _optimize_sequential\n",
      "    frozen_trial_id = _run_trial(study, func, catch)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 263, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/src/hyperparameter_tuning.py\", line 267, in objective\n",
      "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scorer, n_jobs=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 651, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 373, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 91, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 184, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1808, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/enea/Desktop/NIDS-ML-SS^C\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "timeout_seconds = int(TIMEOUT_HOURS * 3600)\n",
    "\n",
    "\n",
    "!python src/hyperparameter_tuning.py \\\n",
    "    --model {TARGET_ALGO} \\\n",
    "    #--n-trials {N_ITERATIONS} \\\n",
    "    --cv {CV_FOLDS} \\\n",
    "    --task {TASK} \\\n",
    "    --timeout {timeout_seconds} \\\n",
    "    --n-jobs 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORE: File tuning non trovato!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICA OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "tuning_file = PROJECT_ROOT / \"tuning_results\" / f\"{TARGET_ALGO}_best.json\"\n",
    "\n",
    "if tuning_file.exists():\n",
    "    with open(tuning_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TUNING COMPLETATO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModello: {data['model_type']}\")\n",
    "    print(f\"Metodo: {data['tuning_method']}\")\n",
    "    print(f\"Best score: {data['best_score']:.4f}\")\n",
    "    print(f\"\\nBest params:\")\n",
    "    for k, v in data['best_params'].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nFile salvato: {tuning_file}\")\n",
    "else:\n",
    "    print(\"ERRORE: File tuning non trovato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SALVA OUTPUT PER FASE 2\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = PROJECT_ROOT / \"phase1_output\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if tuning_file.exists():\n",
    "    import shutil\n",
    "    shutil.copy(tuning_file, output_dir / f\"{TARGET_ALGO}_best.json\")\n",
    "    print(f\"Output copiato in: {output_dir}\")\n",
    "    print(f\"\\nProssimo step: Eseguire phase2_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
