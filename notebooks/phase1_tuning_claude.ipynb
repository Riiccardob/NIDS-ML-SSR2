{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è NIDS-ML - Pipeline Completa (Pre-Training)\n",
    "\n",
    "**Versione:** 2.0 - Ottimizzata e Modulare\n",
    "\n",
    "### üìã Pipeline Steps:\n",
    "1. **Environment Detection** - Rileva Kaggle/Locale\n",
    "2. **Repository Setup** - Download `src/` (solo Kaggle)\n",
    "3. **Dataset Management** - Importa dataset raw\n",
    "4. **Preprocessing** - Pulizia e trasformazione dati\n",
    "5. **Feature Engineering** - Statistical preprocessing + RobustScaler + RF feature selection\n",
    "6. **Validation** - Verifica artifacts e dataset pronti\n",
    "\n",
    "### ‚ú® Caratteristiche:\n",
    "- ‚úÖ **Auto-detection**: Kaggle vs Locale\n",
    "- ‚úÖ **Path Management**: Gestione automatica path dataset e script\n",
    "- ‚úÖ **Clean Run**: Cancella risultati precedenti (configurabile)\n",
    "- ‚úÖ **Checkpoints**: Salvataggio intermedio per debug\n",
    "- ‚úÖ **Modular**: Ogni step √® indipendente e riutilizzabile\n",
    "\n",
    "---\n",
    "\n",
    "**Prossimi Notebook:**\n",
    "- `nids_training_random_forest.ipynb` - Tuning Random Forest\n",
    "- `nids_training_xgboost.ipynb` - Tuning XGBoost\n",
    "- `nids_training_lightgbm.ipynb` - Tuning LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## üîß 1. CONFIGURAZIONE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURAZIONE GLOBALE\n",
    "# ==========================================\n",
    "\n",
    "# --- Clean Run ---\n",
    "CLEAN_RUN = True  # Se True: cancella data/processed e artifacts prima di iniziare\n",
    "\n",
    "# --- Repository (solo Kaggle) ---\n",
    "REPO_URL = \"https://github.com/riiccardob/nids-ml-ssr2\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "# --- Dataset Paths ---\n",
    "# Su Kaggle: path del dataset Input\n",
    "KAGGLE_DATASET_PATH = \"/kaggle/input/network-intrusion-dataset/\"\n",
    "\n",
    "# In Locale: path relativo al repo\n",
    "LOCAL_DATASET_PATH = \"data/raw\"  # Assumendo che il dataset sia gi√† in data/raw\n",
    "\n",
    "# --- Feature Engineering Config ---\n",
    "USE_STATISTICAL = True   # Statistical preprocessing (CONSIGLIATO)\n",
    "USE_ROBUST = True        # RobustScaler (CONSIGLIATO)\n",
    "N_FEATURES = 30          # Numero feature da selezionare\n",
    "RF_ESTIMATORS = 100      # Alberi Random Forest per feature importance\n",
    "\n",
    "# ==========================================\n",
    "# VALIDAZIONE\n",
    "# ==========================================\n",
    "\n",
    "if N_FEATURES < 5 or N_FEATURES > 100:\n",
    "    raise ValueError(f\"‚ùå N_FEATURES deve essere tra 5 e 100 (valore: {N_FEATURES})\")\n",
    "\n",
    "if RF_ESTIMATORS < 10 or RF_ESTIMATORS > 1000:\n",
    "    raise ValueError(f\"‚ùå RF_ESTIMATORS deve essere tra 10 e 1000 (valore: {RF_ESTIMATORS})\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ CONFIGURAZIONE PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Clean Run:              {CLEAN_RUN}\")\n",
    "print(f\"Statistical Preproc:    {USE_STATISTICAL}\")\n",
    "print(f\"RobustScaler:           {USE_ROBUST}\")\n",
    "print(f\"Feature Selection:      Random Forest ({N_FEATURES} features, {RF_ESTIMATORS} trees)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-section",
   "metadata": {},
   "source": [
    "## üåç 2. ENVIRONMENT DETECTION & SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# RILEVAMENTO AMBIENTE\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Rileva Kaggle\n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '') != ''\n",
    "\n",
    "# Determina working directory\n",
    "if IS_KAGGLE:\n",
    "    WORKING_DIR = Path(\"/kaggle/working\")\n",
    "    DATASET_SOURCE = KAGGLE_DATASET_PATH\n",
    "    print(\"üöÄ Ambiente: KAGGLE\")\n",
    "else:\n",
    "    WORKING_DIR = Path.cwd()\n",
    "    DATASET_SOURCE = LOCAL_DATASET_PATH\n",
    "    print(\"üíª Ambiente: LOCALE\")\n",
    "\n",
    "print(f\"üìÇ Working Directory: {WORKING_DIR}\")\n",
    "print(f\"üì¶ Dataset Source:    {DATASET_SOURCE}\")\n",
    "\n",
    "# Setup cartelle di lavoro\n",
    "DIRS_STRUCTURE = [\n",
    "    \"data/raw\",\n",
    "    \"data/processed\",\n",
    "    \"artifacts\",\n",
    "    \"logs/timing\",\n",
    "    \"models\"\n",
    "]\n",
    "\n",
    "for directory in DIRS_STRUCTURE:\n",
    "    dir_path = WORKING_DIR / directory\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Struttura cartelle creata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "repo-section",
   "metadata": {},
   "source": [
    "## üì• 3. REPOSITORY SETUP (Solo Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "repo-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SETUP REPOSITORY & REQUIREMENTS\n",
    "# ==========================================\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"üì• Setup Repository per Kaggle...\")\n",
    "    \n",
    "    # Percorsi\n",
    "    repo_zip = \"repo.zip\"\n",
    "    repo_folder = f\"nids-ml-ssr2-{BRANCH}\"\n",
    "    src_dir = WORKING_DIR / \"src\"\n",
    "    \n",
    "    # Download solo se src non esiste\n",
    "    if not src_dir.exists():\n",
    "        print(f\"  Downloading {REPO_URL}...\")\n",
    "        \n",
    "        # Download ZIP\n",
    "        download_url = f\"{REPO_URL}/archive/refs/heads/{BRANCH}.zip\"\n",
    "        os.system(f\"wget -q {download_url} -O {repo_zip}\")\n",
    "        \n",
    "        # Estrazione\n",
    "        os.system(f\"unzip -qo {repo_zip}\")\n",
    "        \n",
    "        # Sposta solo src/\n",
    "        extracted_src = Path(repo_folder) / \"src\"\n",
    "        if extracted_src.exists():\n",
    "            shutil.move(str(extracted_src), str(src_dir))\n",
    "            print(f\"  ‚úÖ Cartella src/ importata\")\n",
    "        \n",
    "        # Sposta requirements.txt se presente\n",
    "        extracted_req = Path(repo_folder) / \"requirements.txt\"\n",
    "        if extracted_req.exists():\n",
    "            shutil.move(str(extracted_req), str(WORKING_DIR / \"requirements.txt\"))\n",
    "        \n",
    "        # Cleanup\n",
    "        if Path(repo_folder).exists():\n",
    "            shutil.rmtree(repo_folder)\n",
    "        if Path(repo_zip).exists():\n",
    "            os.remove(repo_zip)\n",
    "    else:\n",
    "        print(\"  ‚è© src/ gi√† presente, skip download\")\n",
    "    \n",
    "    # Installazione dipendenze\n",
    "    requirements_file = WORKING_DIR / \"requirements.txt\"\n",
    "    if requirements_file.exists():\n",
    "        print(\"  üì¶ Installazione dipendenze...\")\n",
    "        os.system(f\"pip install -q -r {requirements_file}\")\n",
    "        print(\"  ‚úÖ Dipendenze installate\")\n",
    "    \n",
    "    # Aggiungi src/ al path\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print(f\"‚úÖ Repository pronto: {src_dir}\")\n",
    "    \n",
    "else:\n",
    "    # Locale: verifica che src/ esista\n",
    "    src_dir = WORKING_DIR / \"src\"\n",
    "    if not src_dir.exists():\n",
    "        print(\"‚ö†Ô∏è  ATTENZIONE: cartella src/ non trovata!\")\n",
    "        print(\"    Assicurati di essere nella root del repository.\")\n",
    "        raise FileNotFoundError(f\"src/ non trovata in {WORKING_DIR}\")\n",
    "    \n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print(f\"‚úÖ Repository locale: {src_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-section",
   "metadata": {},
   "source": [
    "## üßπ 4. CLEAN RUN MANAGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CLEAN RUN - Pulizia Output Precedenti\n",
    "# ==========================================\n",
    "\n",
    "CLEAN_DIRS = [\n",
    "    \"data/processed\",\n",
    "    \"artifacts\"\n",
    "]\n",
    "\n",
    "if CLEAN_RUN:\n",
    "    print(\"üßπ CLEAN_RUN = True: Pulizia cartelle di output...\")\n",
    "    \n",
    "    for directory in CLEAN_DIRS:\n",
    "        dir_path = WORKING_DIR / directory\n",
    "        if dir_path.exists():\n",
    "            print(f\"  üóëÔ∏è  Rimozione: {directory}\")\n",
    "            shutil.rmtree(dir_path)\n",
    "            dir_path.mkdir(parents=True)\n",
    "    \n",
    "    print(\"‚úÖ Pulizia completata - Pipeline parte da zero\")\n",
    "else:\n",
    "    print(\"‚è© CLEAN_RUN = False: Mantengo risultati precedenti (se esistono)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-section",
   "metadata": {},
   "source": [
    "## üì¶ 5. DATASET IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# IMPORT DATASET RAW\n",
    "# ==========================================\n",
    "\n",
    "RAW_DIR = WORKING_DIR / \"data/raw\"\n",
    "\n",
    "print(f\"üì¶ Import Dataset in {RAW_DIR}...\")\n",
    "\n",
    "# Conta CSV gi√† presenti\n",
    "existing_csv = list(RAW_DIR.glob(\"*.csv\"))\n",
    "\n",
    "if len(existing_csv) > 0:\n",
    "    print(f\"  ‚úÖ Dataset gi√† presente: {len(existing_csv)} file CSV\")\n",
    "    for csv_file in existing_csv:\n",
    "        print(f\"     - {csv_file.name}\")\n",
    "else:\n",
    "    print(f\"  üì• Importazione dataset da {DATASET_SOURCE}...\")\n",
    "    \n",
    "    if IS_KAGGLE:\n",
    "        # Kaggle: copia da /kaggle/input\n",
    "        if not Path(DATASET_SOURCE).exists():\n",
    "            raise FileNotFoundError(f\"Dataset non trovato: {DATASET_SOURCE}\")\n",
    "        \n",
    "        copied_count = 0\n",
    "        for root, dirs, files in os.walk(DATASET_SOURCE):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".csv\"):\n",
    "                    src_path = Path(root) / file\n",
    "                    dst_path = RAW_DIR / file\n",
    "                    \n",
    "                    if not dst_path.exists():\n",
    "                        shutil.copy2(src_path, dst_path)\n",
    "                        copied_count += 1\n",
    "                        print(f\"     ‚úì Copiato: {file}\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Importati {copied_count} file CSV\")\n",
    "    else:\n",
    "        # Locale: verifica che esistano CSV\n",
    "        if not RAW_DIR.exists():\n",
    "            raise FileNotFoundError(f\"Cartella dataset non trovata: {RAW_DIR}\")\n",
    "        \n",
    "        local_csv = list(RAW_DIR.glob(\"*.csv\"))\n",
    "        if len(local_csv) == 0:\n",
    "            raise FileNotFoundError(f\"Nessun CSV trovato in {RAW_DIR}\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Trovati {len(local_csv)} file CSV locali\")\n",
    "\n",
    "# Verifica finale\n",
    "final_csv = list(RAW_DIR.glob(\"*.csv\"))\n",
    "print(f\"\\n‚úÖ Dataset pronto: {len(final_csv)} file CSV in {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-section",
   "metadata": {},
   "source": [
    "## üîÑ 6. PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîÑ STEP 1: PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verifica se gi√† fatto\n",
    "processed_dir = WORKING_DIR / \"data/processed\"\n",
    "required_files = [\"train.parquet\", \"val.parquet\", \"test.parquet\"]\n",
    "all_exist = all((processed_dir / f).exists() for f in required_files)\n",
    "\n",
    "if all_exist and not CLEAN_RUN:\n",
    "    print(\"‚è© Preprocessing gi√† completato (file parquet esistenti)\")\n",
    "    print(\"   Per rieseguire: imposta CLEAN_RUN = True\")\n",
    "else:\n",
    "    print(\"üöÄ Avvio preprocessing.py...\\n\")\n",
    "    \n",
    "    # Esegui preprocessing\n",
    "    os.chdir(WORKING_DIR)\n",
    "    result = os.system(\"python src/preprocessing.py\")\n",
    "    \n",
    "    if result != 0:\n",
    "        raise RuntimeError(f\"‚ùå Preprocessing fallito con exit code {result}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Preprocessing completato con successo\")\n",
    "\n",
    "# Verifica output\n",
    "for file in required_files:\n",
    "    file_path = processed_dir / file\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ‚úì {file:<20} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File mancante: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-eng-section",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 7. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚öôÔ∏è  STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verifica se gi√† fatto\n",
    "artifacts_dir = WORKING_DIR / \"artifacts\"\n",
    "required_artifacts = [\n",
    "    \"scaler.pkl\",\n",
    "    \"selected_features.json\",\n",
    "    \"feature_importances.json\"\n",
    "]\n",
    "all_exist = all((artifacts_dir / f).exists() for f in required_artifacts)\n",
    "\n",
    "ready_dir = processed_dir\n",
    "ready_files = [\"train_ready.parquet\", \"val_ready.parquet\", \"test_ready.parquet\"]\n",
    "ready_exist = all((ready_dir / f).exists() for f in ready_files)\n",
    "\n",
    "if all_exist and ready_exist and not CLEAN_RUN:\n",
    "    print(\"‚è© Feature Engineering gi√† completato\")\n",
    "    print(\"   Per rieseguire: imposta CLEAN_RUN = True\")\n",
    "else:\n",
    "    print(\"üöÄ Avvio feature_engineering.py...\\n\")\n",
    "    \n",
    "    # Costruisci comando\n",
    "    cmd_parts = [\"python\", \"src/feature_engineering.py\"]\n",
    "    \n",
    "    if USE_STATISTICAL:\n",
    "        cmd_parts.append(\"--use-statistical\")\n",
    "    \n",
    "    if USE_ROBUST:\n",
    "        cmd_parts.append(\"--use-robust\")\n",
    "    \n",
    "    cmd_parts.extend([\n",
    "        \"--n-features\", str(N_FEATURES),\n",
    "        \"--rf-estimators\", str(RF_ESTIMATORS)\n",
    "    ])\n",
    "    \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Comando: {cmd}\\n\")\n",
    "    \n",
    "    # Esegui\n",
    "    os.chdir(WORKING_DIR)\n",
    "    result = os.system(cmd)\n",
    "    \n",
    "    if result != 0:\n",
    "        raise RuntimeError(f\"‚ùå Feature Engineering fallito con exit code {result}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature Engineering completato con successo\")\n",
    "\n",
    "# Verifica output\n",
    "print(\"\\nüìã Artifacts generati:\")\n",
    "for artifact in required_artifacts:\n",
    "    artifact_path = artifacts_dir / artifact\n",
    "    if artifact_path.exists():\n",
    "        size_kb = artifact_path.stat().st_size / 1024\n",
    "        print(f\"  ‚úì {artifact:<30} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Artifact mancante: {artifact_path}\")\n",
    "\n",
    "print(\"\\nüìã Dataset pronti per training:\")\n",
    "for file in ready_files:\n",
    "    file_path = ready_dir / file\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ‚úì {file:<30} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File mancante: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## ‚úÖ 8. VALIDATION & SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# VALIDAZIONE FINALE & SUMMARY\n",
    "# ==========================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ VALIDAZIONE PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Verifica dataset ready\n",
    "print(\"\\n1. Dataset Ready:\")\n",
    "for dataset_name in [\"train\", \"val\", \"test\"]:\n",
    "    file_path = processed_dir / f\"{dataset_name}_ready.parquet\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"  {dataset_name.upper():5} - Shape: {df.shape[0]:>7,} samples x {df.shape[1]:>2} features\")\n",
    "\n",
    "# 2. Feature selezionate\n",
    "print(\"\\n2. Feature Selection:\")\n",
    "with open(artifacts_dir / \"selected_features.json\") as f:\n",
    "    selected_features = json.load(f)\n",
    "print(f\"  Selezionate: {len(selected_features)} features\")\n",
    "\n",
    "# 3. Top-10 importances\n",
    "print(\"\\n3. Top-10 Feature Importances:\")\n",
    "with open(artifacts_dir / \"feature_importances.json\") as f:\n",
    "    importances = json.load(f)\n",
    "\n",
    "sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (feat, score) in enumerate(sorted_features, 1):\n",
    "    print(f\"  {i:2}. {feat:<40} {score:.6f}\")\n",
    "\n",
    "# 4. Scaler info\n",
    "print(\"\\n4. Scaler:\")\n",
    "scaler_type = \"RobustScaler\" if USE_ROBUST else \"StandardScaler\"\n",
    "print(f\"  Tipo: {scaler_type}\")\n",
    "statistical_info_path = artifacts_dir / \"statistical_preprocessing_info.json\"\n",
    "if statistical_info_path.exists():\n",
    "    with open(statistical_info_path) as f:\n",
    "        stat_info = json.load(f)\n",
    "    print(f\"  Statistical Preprocessing: ATTIVO\")\n",
    "    if 'summary' in stat_info:\n",
    "        summary = stat_info['summary']\n",
    "        print(f\"    - Feature ridotte: {summary.get('reduction_percent', 0):.1f}%\")\n",
    "        print(f\"    - Varianza rimossa: {stat_info.get('step1_variance', {}).get('removed_count', 0)}\")\n",
    "        print(f\"    - Correlazione rimossa: {stat_info.get('step2_correlation', {}).get('removed_count', 0)}\")\n",
    "else:\n",
    "    print(f\"  Statistical Preprocessing: DISATTIVO\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ PIPELINE COMPLETATA CON SUCCESSO!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìç Prossimi Step:\")\n",
    "print(\"  1. Tuning Random Forest:  nids_training_random_forest.ipynb\")\n",
    "print(\"  2. Tuning XGBoost:        nids_training_xgboost.ipynb\")\n",
    "print(\"  3. Tuning LightGBM:       nids_training_lightgbm.ipynb\")\n",
    "print(\"\\nüíæ Output Disponibili:\")\n",
    "print(f\"  - Dataset: {processed_dir}\")\n",
    "print(f\"  - Artifacts: {artifacts_dir}\")\n",
    "print(f\"  - Logs: {WORKING_DIR / 'logs'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## üíæ 9. EXPORT ARTIFACTS (Opzionale - Solo Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXPORT ARTIFACTS - Solo Kaggle\n",
    "# ==========================================\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"üì¶ Creazione archivio artifacts per download...\")\n",
    "    \n",
    "    OUTPUT_ZIP = \"pipeline_artifacts.zip\"\n",
    "    DIRS_TO_EXPORT = [\"artifacts\", \"data/processed\"]\n",
    "    \n",
    "    existing_dirs = [d for d in DIRS_TO_EXPORT if (WORKING_DIR / d).exists()]\n",
    "    \n",
    "    if existing_dirs:\n",
    "        # Crea zip\n",
    "        dirs_str = \" \".join(existing_dirs)\n",
    "        os.chdir(WORKING_DIR)\n",
    "        os.system(f\"zip -qr {OUTPUT_ZIP} {dirs_str}\")\n",
    "        \n",
    "        zip_path = WORKING_DIR / OUTPUT_ZIP\n",
    "        if zip_path.exists():\n",
    "            size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"‚úÖ Archivio creato: {OUTPUT_ZIP} ({size_mb:.1f} MB)\")\n",
    "            print(\"   Scarica il file dall'interfaccia Kaggle (Output > {OUTPUT_ZIP})\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Errore nella creazione dell'archivio\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nessun artifact da esportare\")\n",
    "else:\n",
    "    print(\"üíª Ambiente locale: export non necessario\")\n",
    "    print(f\"   Artifacts disponibili in: {artifacts_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 6376134,
     "sourceType": "datasetVersion",
     "datasetId": 3674161
    }
   ],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
