{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS-ML: Sniffer Validation Pipeline\n",
    "\n",
    "## Scopo\n",
    "Questo notebook valida che lo sniffer funzioni correttamente testando i modelli trainati sui CSV CIC-IDS2017.\n",
    "\n",
    "## Fasi\n",
    "1. **Setup**: Clona repo, installa dipendenze\n",
    "2. **Calibrazione**: Verifica feature alignment\n",
    "3. **Evaluation CSV**: Testa modelli su tutti i giorni\n",
    "4. **Confronto**: Ranking modelli per performance\n",
    "5. **Test PCAP** (opzionale): Se disponibile\n",
    "\n",
    "## Output\n",
    "- Metriche per ogni modello/CSV\n",
    "- Ranking modelli\n",
    "- Report downloadabile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURAZIONE\n",
    "# ============================================================================\n",
    "\n",
    "# GitHub repo\n",
    "GITHUB_REPO = \"https://github.com/tuouser/NIDS-ML-SSR2.git\"\n",
    "GITHUB_BRANCH = \"main\"\n",
    "\n",
    "# Quali modelli testare: 'all', 'xgboost', 'lightgbm', 'random_forest', 'best'\n",
    "MODEL_TYPE = 'all'\n",
    "\n",
    "# Task\n",
    "TASK = 'binary'\n",
    "\n",
    "# Sample size per CSV (None = tutto, numero = sample)\n",
    "# Consigliato: None per test completi, 100000 per test veloci\n",
    "SAMPLE_SIZE = None\n",
    "\n",
    "# Test anche su PCAP se disponibile\n",
    "TEST_PCAP = False\n",
    "\n",
    "print(\"Configurazione:\")\n",
    "print(f\"  Model type: {MODEL_TYPE}\")\n",
    "print(f\"  Task: {TASK}\")\n",
    "print(f\"  Sample: {SAMPLE_SIZE or 'tutto'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Rileva ambiente\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    ENV = 'kaggle'\n",
    "    PROJECT_ROOT = Path('/kaggle/working/NIDS-ML-SSR2')\n",
    "elif 'COLAB_GPU' in os.environ:\n",
    "    ENV = 'colab'\n",
    "    PROJECT_ROOT = Path('/content/NIDS-ML-SSR2')\n",
    "else:\n",
    "    ENV = 'local'\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "print(f\"Ambiente: {ENV}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo se necessario\n",
    "if ENV in ['kaggle', 'colab'] and not PROJECT_ROOT.exists():\n",
    "    !git clone --branch {GITHUB_BRANCH} {GITHUB_REPO} {PROJECT_ROOT}\n",
    "    print(f\"Repo clonato in {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(\"Repo gi√† presente o ambiente locale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Installa dipendenze se necessario\n",
    "if ENV in ['kaggle', 'colab']:\n",
    "    !pip install -q scapy\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link dataset Kaggle\n",
    "if ENV == 'kaggle':\n",
    "    DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "    DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Dataset CIC-IDS2017 CSV\n",
    "    kaggle_csv = Path(\"/kaggle/input/cicids2017\")\n",
    "    if kaggle_csv.exists():\n",
    "        for f in kaggle_csv.glob(\"*.csv\"):\n",
    "            dest = DATA_RAW / f.name\n",
    "            if not dest.exists():\n",
    "                os.symlink(f, dest)\n",
    "        print(f\"CSV linkati: {len(list(DATA_RAW.glob('*.csv')))}\")\n",
    "    \n",
    "    # Dataset PCAP (se disponibile)\n",
    "    kaggle_pcap = Path(\"/kaggle/input/cicids2017-pcap\")\n",
    "    if kaggle_pcap.exists():\n",
    "        PCAP_DIR = kaggle_pcap\n",
    "        print(f\"PCAP disponibili: {len(list(PCAP_DIR.glob('*.pcap')))}\")\n",
    "    else:\n",
    "        PCAP_DIR = None\n",
    "        print(\"PCAP non disponibili\")\n",
    "else:\n",
    "    DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "    PCAP_DIR = PROJECT_ROOT / \"data\" / \"pcap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica CSV disponibili\n",
    "csv_files = sorted(DATA_RAW.glob(\"*.csv\"))\n",
    "print(f\"\\nCSV disponibili ({len(csv_files)}):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024**2)\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica modelli disponibili\n",
    "from src.model_versioning import list_model_versions\n",
    "\n",
    "print(\"\\nModelli disponibili:\")\n",
    "for mt in ['xgboost', 'lightgbm', 'random_forest']:\n",
    "    versions = list_model_versions(model_type=mt, task=TASK)\n",
    "    if versions:\n",
    "        print(f\"\\n  {mt.upper()}:\")\n",
    "        for v in versions:\n",
    "            print(f\"    - {v['version_id']}\")\n",
    "\n",
    "# Best model\n",
    "best_path = PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\"\n",
    "if best_path.exists():\n",
    "    meta_path = best_path.parent / \"metadata.json\"\n",
    "    if meta_path.exists():\n",
    "        with open(meta_path) as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"\\n  BEST MODEL: {meta.get('best_model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Calibrazione Feature\n",
    "\n",
    "Verifica che le feature nel CSV siano quelle attese dal modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.feature_engineering import load_artifacts\n",
    "\n",
    "def analyze_csv_quick(csv_path, sample=5000):\n",
    "    \"\"\"\n",
    "    Analisi rapida di un CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, low_memory=False, nrows=sample*2)\n",
    "    df.columns = df.columns.str.strip()  # Rimuovi spazi\n",
    "    \n",
    "    if len(df) > sample:\n",
    "        df = df.sample(n=sample, random_state=42)\n",
    "    \n",
    "    # Label\n",
    "    label_col = None\n",
    "    for col in df.columns:\n",
    "        if 'label' in col.lower():\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    result = {\n",
    "        'file': csv_path.name,\n",
    "        'rows': len(df),\n",
    "        'cols': len(df.columns),\n",
    "    }\n",
    "    \n",
    "    if label_col:\n",
    "        counts = df[label_col].value_counts()\n",
    "        result['benign'] = counts.get('BENIGN', 0)\n",
    "        result['attacks'] = len(df) - result['benign']\n",
    "        result['attack_types'] = [l for l in counts.index if l != 'BENIGN']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Analizza tutti i CSV\n",
    "print(\"=\"*70)\n",
    "print(\"ANALISI CSV\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "csv_info = []\n",
    "for csv_path in csv_files:\n",
    "    info = analyze_csv_quick(csv_path)\n",
    "    csv_info.append(info)\n",
    "    \n",
    "    attacks = info.get('attacks', 0)\n",
    "    attack_types = ', '.join(info.get('attack_types', [])[:3])\n",
    "    print(f\"\\n{info['file'][:50]}\")\n",
    "    print(f\"  Righe: {info['rows']:,} | Attacchi: {attacks:,}\")\n",
    "    if attack_types:\n",
    "        print(f\"  Tipi: {attack_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica feature del modello\n",
    "try:\n",
    "    scaler, selected_features, _, scaler_columns = load_artifacts()\n",
    "    print(f\"\\nFeature del modello: {len(selected_features)}\")\n",
    "    print(f\"Colonne scaler: {len(scaler_columns)}\")\n",
    "    \n",
    "    # Verifica presenza in un CSV\n",
    "    test_csv = csv_files[0]\n",
    "    df_test = pd.read_csv(test_csv, nrows=10)\n",
    "    df_test.columns = df_test.columns.str.strip()\n",
    "    \n",
    "    missing = [f for f in selected_features if f not in df_test.columns]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è  Feature mancanti: {missing[:5]}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Tutte le feature presenti nei CSV\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Errore caricamento artifacts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluation su CSV\n",
    "\n",
    "Testa i modelli su ogni CSV e calcola metriche reali (F1, Recall, FPR).\n",
    "\n",
    "### Cosa significano le metriche?\n",
    "\n",
    "| Metrica | Significato | Valore ideale |\n",
    "|---------|-------------|---------------|\n",
    "| **F1** | Media armonica di Precision e Recall | > 0.90 |\n",
    "| **Recall** | % di attacchi rilevati | > 0.95 |\n",
    "| **Precision** | % di alert che sono veri attacchi | > 0.90 |\n",
    "| **FPR** | % di traffico benigno classificato come attacco | < 0.02 |\n",
    "\n",
    "### Interpretazione per CSV\n",
    "\n",
    "| CSV | Attacchi | F1 atteso | Note |\n",
    "|-----|----------|-----------|------|\n",
    "| Monday | 0 | 0.00 | CORRETTO! Nessun attacco, F1=0 √® giusto. Guarda FPR. |\n",
    "| Tuesday | Brute Force | > 0.85 | Attacchi SSH/FTP |\n",
    "| Wednesday | DoS | > 0.95 | Attacchi volumetrici, facili da rilevare |\n",
    "| Thursday | Web Attack | > 0.80 | Pi√π difficili |\n",
    "| Friday | DDoS, Botnet | > 0.95 | Attacchi volumetrici |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "def test_model_on_csv(csv_path, model_path, scaler, selected_features, scaler_columns, \n",
    "                      task='binary', sample_size=None):\n",
    "    \"\"\"\n",
    "    Testa un modello su un CSV e restituisce metriche.\n",
    "    \"\"\"\n",
    "    # Carica modello\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Carica CSV\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    original_size = len(df)\n",
    "    \n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Label\n",
    "    label_col = None\n",
    "    for col in df.columns:\n",
    "        if 'label' in col.lower():\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    if not label_col:\n",
    "        return {'error': 'Label column not found'}\n",
    "    \n",
    "    # Prepara y\n",
    "    if task == 'binary':\n",
    "        y_true = (df[label_col].str.strip().str.upper() != 'BENIGN').astype(int)\n",
    "    else:\n",
    "        y_true = df[label_col]\n",
    "    \n",
    "    # Prepara X\n",
    "    for col in scaler_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    X = df[scaler_columns].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    # Trasforma\n",
    "    X_scaled = pd.DataFrame(scaler.transform(X), columns=scaler_columns)\n",
    "    X_selected = pd.DataFrame(\n",
    "        X_scaled[selected_features].values,\n",
    "        columns=list(selected_features)\n",
    "    )\n",
    "    \n",
    "    # Predici\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_selected)\n",
    "    pred_time = time.time() - start\n",
    "    \n",
    "    # Metriche\n",
    "    if task == 'binary':\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Solo una classe\n",
    "            if y_true.sum() == 0:  # Solo benign\n",
    "                tn, fp, fn, tp = len(y_true) - (y_pred == 1).sum(), (y_pred == 1).sum(), 0, 0\n",
    "            else:  # Solo attack\n",
    "                tn, fp, fn, tp = 0, 0, (y_pred == 0).sum(), (y_pred == 1).sum()\n",
    "        \n",
    "        return {\n",
    "            'csv': csv_path.name,\n",
    "            'total_samples': original_size,\n",
    "            'tested_samples': len(df),\n",
    "            'attacks_in_data': int((y_true == 1).sum()),\n",
    "            'benign_in_data': int((y_true == 0).sum()),\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "            'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            'fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0,\n",
    "            'fnr': float(fn / (fn + tp)) if (fn + tp) > 0 else 0,\n",
    "            'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn),\n",
    "            'pred_time_sec': pred_time\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'csv': csv_path.name,\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "            'f1_weighted': float(f1_score(y_true, y_pred, average='weighted', zero_division=0))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raccogli modelli da testare\n",
    "models_to_test = []\n",
    "\n",
    "if MODEL_TYPE == 'best':\n",
    "    best_path = PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\"\n",
    "    if best_path.exists():\n",
    "        models_to_test.append(('best_model', best_path))\n",
    "\n",
    "elif MODEL_TYPE == 'all':\n",
    "    for mt in ['xgboost', 'lightgbm', 'random_forest']:\n",
    "        versions = list_model_versions(model_type=mt, task=TASK)\n",
    "        for v in versions:\n",
    "            name = f\"{mt}/{v['version_id']}\"\n",
    "            models_to_test.append((name, v['model_path']))\n",
    "\n",
    "else:\n",
    "    versions = list_model_versions(model_type=MODEL_TYPE, task=TASK)\n",
    "    for v in versions:\n",
    "        name = f\"{MODEL_TYPE}/{v['version_id']}\"\n",
    "        models_to_test.append((name, v['model_path']))\n",
    "\n",
    "print(f\"Modelli da testare: {len(models_to_test)}\")\n",
    "for name, path in models_to_test:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Esegui test su tutti i CSV\n",
    "all_results = {}  # {model_name: {csv_name: metrics}}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION SU CSV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model_path in models_to_test:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# MODELLO: {model_name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        print(f\"\\n  Testing: {csv_path.name[:40]}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            result = test_model_on_csv(\n",
    "                csv_path, model_path, \n",
    "                scaler, selected_features, scaler_columns,\n",
    "                task=TASK, sample_size=SAMPLE_SIZE\n",
    "            )\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"ERRORE: {result['error']}\")\n",
    "            else:\n",
    "                f1 = result['f1']\n",
    "                recall = result['recall']\n",
    "                fpr = result['fpr']\n",
    "                attacks = result['attacks_in_data']\n",
    "                \n",
    "                print(f\"F1={f1:.4f} | Recall={recall:.4f} | FPR={fpr:.4f} | Attacks={attacks:,}\")\n",
    "                model_results[csv_path.name] = result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE: {e}\")\n",
    "    \n",
    "    all_results[model_name] = model_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluation completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tabelle Riepilogative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabella F1 per modello/CSV\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABELLA F1 SCORE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Header\n",
    "csv_names = [f.name[:25] for f in csv_files]\n",
    "header = f\"{'Modello':<35}\"\n",
    "for name in csv_names:\n",
    "    header += f\" | {name[:10]:^10}\"\n",
    "header += \" | MEDIA\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Righe\n",
    "model_averages = []\n",
    "\n",
    "for model_name in all_results:\n",
    "    row = f\"{model_name:<35}\"\n",
    "    f1_scores = []\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        csv_name = csv_path.name\n",
    "        if csv_name in all_results[model_name]:\n",
    "            f1 = all_results[model_name][csv_name].get('f1', 0)\n",
    "            f1_scores.append(f1)\n",
    "            row += f\" | {f1:^10.4f}\"\n",
    "        else:\n",
    "            row += f\" | {'N/A':^10}\"\n",
    "    \n",
    "    # Media (escludi Monday se F1=0 per mancanza attacchi)\n",
    "    valid_f1 = [f for f in f1_scores if f > 0]\n",
    "    avg = np.mean(valid_f1) if valid_f1 else 0\n",
    "    row += f\" | {avg:.4f}\"\n",
    "    model_averages.append((model_name, avg))\n",
    "    \n",
    "    print(row)\n",
    "\n",
    "print(\"-\" * len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabella FPR (False Positive Rate)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABELLA FPR (False Positive Rate) - pi√π basso √® meglio\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Header\n",
    "header = f\"{'Modello':<35}\"\n",
    "for name in csv_names:\n",
    "    header += f\" | {name[:10]:^10}\"\n",
    "header += \" | MEDIA\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for model_name in all_results:\n",
    "    row = f\"{model_name:<35}\"\n",
    "    fpr_scores = []\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        csv_name = csv_path.name\n",
    "        if csv_name in all_results[model_name]:\n",
    "            fpr = all_results[model_name][csv_name].get('fpr', 0)\n",
    "            fpr_scores.append(fpr)\n",
    "            # Colora se FPR alto\n",
    "            fpr_str = f\"{fpr:.4f}\"\n",
    "            if fpr > 0.05:\n",
    "                fpr_str = f\"*{fpr:.3f}*\"  # Evidenzia\n",
    "            row += f\" | {fpr_str:^10}\"\n",
    "        else:\n",
    "            row += f\" | {'N/A':^10}\"\n",
    "    \n",
    "    avg = np.mean(fpr_scores) if fpr_scores else 0\n",
    "    row += f\" | {avg:.4f}\"\n",
    "    print(row)\n",
    "\n",
    "print(\"-\" * len(header))\n",
    "print(\"* = FPR > 5% (alto)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANKING FINALE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANKING MODELLI (per F1 medio)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (name, avg) in enumerate(model_averages, 1):\n",
    "    # Calcola anche FPR medio\n",
    "    fpr_list = []\n",
    "    for csv_name, metrics in all_results.get(name, {}).items():\n",
    "        fpr_list.append(metrics.get('fpr', 0))\n",
    "    avg_fpr = np.mean(fpr_list) if fpr_list else 0\n",
    "    \n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"  \"\n",
    "    print(f\"{medal} #{i:2} {name:<40} F1={avg:.4f}  FPR={avg_fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dettaglio per CSV con pochi attacchi (es. Monday)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETTAGLIO CSV CON POCHI/NESSUN ATTACCO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNOTA: F1=0 su Monday √® CORRETTO perch√© non ci sono attacchi.\")\n",
    "print(\"      Quello che conta √® il FPR (falsi positivi).\")\n",
    "\n",
    "for model_name in list(all_results.keys())[:3]:  # Top 3 modelli\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for csv_name, metrics in all_results[model_name].items():\n",
    "        if 'Monday' in csv_name or metrics.get('attacks_in_data', 0) == 0:\n",
    "            fpr = metrics.get('fpr', 0)\n",
    "            fp = metrics.get('fp', 0)\n",
    "            tn = metrics.get('tn', 0)\n",
    "            print(f\"  {csv_name[:30]}: FPR={fpr:.4f} (FP={fp:,}, TN={tn:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Salvataggio Risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva risultati\n",
    "reports_dir = PROJECT_ROOT / \"reports\"\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'task': TASK,\n",
    "    'sample_size': SAMPLE_SIZE,\n",
    "    'models_tested': len(models_to_test),\n",
    "    'csv_tested': len(csv_files),\n",
    "    'ranking': [{'model': name, 'f1_avg': avg} for name, avg in model_averages],\n",
    "    'detailed_results': all_results\n",
    "}\n",
    "\n",
    "output_path = reports_dir / \"sniffer_evaluation_results.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Risultati salvati in: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Download Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "if ENV in ['kaggle', 'colab']:\n",
    "    zip_path = PROJECT_ROOT / \"sniffer_evaluation_output.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        # Reports\n",
    "        for f in reports_dir.glob(\"*.json\"):\n",
    "            z.write(f, f\"reports/{f.name}\")\n",
    "    \n",
    "    print(f\"ZIP creato: {zip_path.name} ({zip_path.stat().st_size/1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Ambiente locale - risultati nelle cartelle del progetto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Riepilogo Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RIEPILOGO SNIFFER EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModelli testati: {len(models_to_test)}\")\n",
    "print(f\"CSV testati: {len(csv_files)}\")\n",
    "\n",
    "if model_averages:\n",
    "    best_model, best_f1 = model_averages[0]\n",
    "    print(f\"\\nMiglior modello: {best_model}\")\n",
    "    print(f\"  F1 medio: {best_f1:.4f}\")\n",
    "\n",
    "# Check risultati\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"CHECKLIST VALIDAZIONE:\")\n",
    "\n",
    "# Monday FPR\n",
    "monday_ok = True\n",
    "for model_name, results in all_results.items():\n",
    "    for csv_name, metrics in results.items():\n",
    "        if 'Monday' in csv_name:\n",
    "            if metrics.get('fpr', 1) > 0.02:\n",
    "                monday_ok = False\n",
    "                break\n",
    "\n",
    "print(f\"  [{'‚úì' if monday_ok else '‚úó'}] FPR su Monday < 2%\")\n",
    "\n",
    "# F1 su Friday\n",
    "friday_ok = any(\n",
    "    metrics.get('f1', 0) > 0.90\n",
    "    for results in all_results.values()\n",
    "    for csv_name, metrics in results.items()\n",
    "    if 'Friday' in csv_name and 'DDos' in csv_name\n",
    ")\n",
    "print(f\"  [{'‚úì' if friday_ok else '‚úó'}] F1 su Friday-DDos > 90%\")\n",
    "\n",
    "# Almeno un modello buono\n",
    "good_model = best_f1 > 0.85 if model_averages else False\n",
    "print(f\"  [{'‚úì' if good_model else '‚úó'}] Almeno un modello con F1 medio > 85%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if monday_ok and friday_ok and good_model:\n",
    "    print(\"‚úì VALIDAZIONE PASSATA - I modelli sono pronti per l'uso!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  VALIDAZIONE PARZIALE - Controlla i risultati sopra\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
