{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ NIDS-ML Phase 1: Hyperparameter Tuning (Optimized)\n",
    "\n",
    "Questo notebook esegue il tuning degli iperparametri in modo efficiente su Kaggle.\n",
    "\n",
    "### âœ¨ Caratteristiche\n",
    "- **Git Sparse Checkout**: Scarica solo il codice sorgente necessario (`src/`) senza clonare l'intera cronologia o file inutili.\n",
    "- **Smart Dataset Linking**: Usa Symlinks invece di copiare i dati, risparmiando spazio e tempo di avvio.\n",
    "- **Single Model Focus**: Ottimizza un modello alla volta per massimizzare l'utilizzo delle 12 ore di runtime di Kaggle.\n",
    "- **Robust Output**: Salva e zippa automaticamente i risultati per il download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. CONFIGURAZIONE PARAMETRI\n",
    "# ============================================================================\n",
    "\n",
    "# --- Selezione Modello ---\n",
    "# Scegli UNO solo per sessione per evitare timeout (random_forest, xgboost, lightgbm)\n",
    "TARGET_MODEL = 'xgboost'\n",
    "\n",
    "# --- Strategia di Tuning ---\n",
    "TUNING_METHOD = 'bayesian'  # 'random' o 'bayesian'\n",
    "\n",
    "# Parametri Ricerca\n",
    "N_ITER   = 50      # Usato solo se method='random' (numero totale combinazioni)\n",
    "N_TRIALS = 100     # Usato solo se method='bayesian' (numero tentativi Optuna)\n",
    "TIMEOUT_HOURS = 11 # Timeout sicurezza (Kaggle killa a 12h)\n",
    "\n",
    "# Validazione\n",
    "CV_FOLDS = 5       # Cross-Validation folds\n",
    "\n",
    "# --- Repo Config ---\n",
    "REPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "print(f\"ðŸŽ¯ Target: {TARGET_MODEL} ({TUNING_METHOD})\")\n",
    "print(f\"â±ï¸  Timeout: {TIMEOUT_HOURS}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. SETUP AMBIENTE & GIT SPARSE CHECKOUT\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Rilevamento Ambiente\n",
    "KAGGLE_ENV = Path(\"/kaggle/input\").exists()\n",
    "PROJECT_ROOT = Path(\"/kaggle/working\") if KAGGLE_ENV else Path.cwd()\n",
    "\n",
    "if KAGGLE_ENV:\n",
    "    print(\"â˜ï¸  Running on Kaggle Environment\")\n",
    "    \n",
    "    # Pulizia preventiva se necessario\n",
    "    if (PROJECT_ROOT / \"src\").exists():\n",
    "        shutil.rmtree(PROJECT_ROOT / \"src\")\n",
    "    if (PROJECT_ROOT / \"temp_repo\").exists():\n",
    "        shutil.rmtree(PROJECT_ROOT / \"temp_repo\")\n",
    "\n",
    "    # --- GIT SPARSE CHECKOUT (Opzione B) ---\n",
    "    print(f\"â¬‡ï¸  Cloning source code from {REPO_URL}...\")\n",
    "    # 1. Clone 'vuoto' (senza file, solo metadata minimi)\n",
    "    !git clone --filter=blob:none --no-checkout {REPO_URL} temp_repo\n",
    "    \n",
    "    # 2. Configura Sparse Checkout per scaricare SOLO 'src' e 'requirements.txt'\n",
    "    %cd temp_repo\n",
    "    !git sparse-checkout init --cone\n",
    "    !git sparse-checkout set src requirements.txt\n",
    "    \n",
    "    # 3. Checkout del branch\n",
    "    !git checkout {BRANCH}\n",
    "    \n",
    "    # 4. Sposta i file nella root di lavoro e pulisci\n",
    "    %cd ..\n",
    "    !mv temp_repo/src .\n",
    "    !mv temp_repo/requirements.txt .\n",
    "    !rm -rf temp_repo\n",
    "    \n",
    "    print(\"âœ… Code setup complete (src/ downloaded).\")\n",
    "    \n",
    "    # Installazione dipendenze\n",
    "    print(\"ðŸ“¦ Installing dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pip install -q optuna  # Necessario per bayesian\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ’» Running Local Environment\")\n",
    "    # Assumiamo che in locale siamo giÃ  nella root del progetto\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent # Risale di un livello se siamo in 'notebooks/'\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Creazione cartelle output\n",
    "for d in [\"data/raw\", \"data/processed\", \"artifacts\", \"tuning_results\", \"logs\"]:\n",
    "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATASET SETUP (SYMLINKS)\n",
    "# ============================================================================\n",
    "\n",
    "raw_dir = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "count = 0\n",
    "\n",
    "if KAGGLE_ENV:\n",
    "    print(\"ðŸ”— Creating Symlinks for Dataset...\")\n",
    "    input_dir = Path(\"/kaggle/input\")\n",
    "    \n",
    "    # Cerca ricorsivamente file CSV o Parquet negli input di Kaggle\n",
    "    for p in input_dir.glob(\"**/*\"):\n",
    "        if p.is_file() and p.suffix in ['.csv', '.parquet']:\n",
    "            dest = raw_dir / p.name\n",
    "            if not dest.exists():\n",
    "                try:\n",
    "                    os.symlink(p, dest)\n",
    "                    print(f\"  -> Linked: {p.name}\")\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ Error linking {p.name}: {e}\")\n",
    "else:\n",
    "    # In locale conta solo i file esistenti\n",
    "    count = len(list(raw_dir.glob(\"*.csv\"))) + len(list(raw_dir.glob(\"*.parquet\")))\n",
    "\n",
    "print(f\"\\nâœ… Dataset pronto: {count} file disponibili in data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# Controlla se il preprocessing Ã¨ giÃ  stato fatto per risparmiare tempo in caso di riavvio sessione\n",
    "if not (PROJECT_ROOT / \"data\" / \"processed\" / \"test.parquet\").exists():\n",
    "    print(\"âš™ï¸  Avvio Preprocessing...\")\n",
    "    # Parametri: balance-ratio 2.0 per ridurre overfitting su classi maggioritarie\n",
    "    !python src/preprocessing.py --balance-ratio 2.0 --n-jobs -1\n",
    "else:\n",
    "    print(\"â© Dati processati trovati. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "if not (PROJECT_ROOT / \"artifacts\" / \"selected_features.json\").exists():\n",
    "    print(\"ðŸ§ª Avvio Feature Engineering...\")\n",
    "    !python src/feature_engineering.py --n-features 30 --n-jobs -1\n",
    "else:\n",
    "    print(\"â© Artifacts feature engineering trovati. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"ðŸ”¥ START TUNING: {TARGET_MODEL.upper()} using {TUNING_METHOD.upper()}\")\n",
    "\n",
    "# Costruzione comando dinamico\n",
    "cmd = f\"python src/hyperparameter_tuning.py --model {TARGET_MODEL} --cv {CV_FOLDS} --task binary --n-jobs -1\"\n",
    "\n",
    "if TUNING_METHOD == 'bayesian':\n",
    "    # Converte ore in secondi per lo script\n",
    "    timeout_sec = int(TIMEOUT_HOURS * 3600)\n",
    "    cmd += f\" --method bayesian --n-trials {N_TRIALS} --timeout {timeout_sec}\"\n",
    "else:\n",
    "    cmd += f\" --method random --n-iter {N_ITER}\"\n",
    "\n",
    "print(f\"Running command: {cmd}\\n\")\n",
    "\n",
    "# Esecuzione\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. VERIFICA RISULTATI E PACKAGING\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "print(\"ðŸ“Š Checking Results...\")\n",
    "\n",
    "# Cerca l'ultimo file JSON generato per il modello target\n",
    "results_path = PROJECT_ROOT / \"tuning_results\" / TARGET_MODEL\n",
    "json_files = list(results_path.glob(\"*.json\"))\n",
    "\n",
    "if json_files:\n",
    "    # Prende il piÃ¹ recente\n",
    "    latest_file = max(json_files, key=os.path.getctime)\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(f\"\\nðŸ† BEST RESULT FOUND ({latest_file.name}):\")\n",
    "    print(f\"   Score: {data.get('best_score', 'N/A'):.4f}\")\n",
    "    print(f\"   Best Params: {json.dumps(data.get('best_params', {}), indent=2)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nessun file di risultati trovato. Il tuning potrebbe essere fallito.\")\n",
    "\n",
    "print(\"\\nðŸ“¦ Packaging results for download...\")\n",
    "# Zippa cartelle tuning_results e logs per facile download da Kaggle\n",
    "!zip -r phase1_results.zip tuning_results logs artifacts > /dev/null\n",
    "print(\"âœ… Created 'phase1_results.zip'. Download this file from the Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
