{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS-ML: Network Intrusion Detection System\n",
    "\n",
    "Pipeline completa: preprocessing, feature engineering, training, evaluation e test su PCAP.\n",
    "\n",
    "**Requisiti Kaggle**:\n",
    "- Dataset CIC-IDS2017 (CSV)\n",
    "- Dataset `cicids2017-test-pcap` (opzionale, per test PCAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"/kaggle/input\").exists():\n",
    "    ENV = \"kaggle\"\n",
    "    PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "    DATA_INPUT = Path(\"/kaggle/input\")\n",
    "elif Path(\"/content\").exists():\n",
    "    ENV = \"colab\"\n",
    "    PROJECT_ROOT = Path(\"/content/NIDS-ML\")\n",
    "    DATA_INPUT = Path(\"/content\")\n",
    "else:\n",
    "    ENV = \"local\"\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Ambiente: {ENV}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data input: {DATA_INPUT}\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\n",
    "\n",
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        print(f\"Cloning project from {REPO_URL}...\")\n",
    "        !git clone {REPO_URL} temp_repo\n",
    "        !cp -r temp_repo/* {PROJECT_ROOT}/\n",
    "        !rm -rf temp_repo\n",
    "        print(\"Project cloned.\")\n",
    "        if (PROJECT_ROOT / \"requirements.txt\").exists():\n",
    "            !pip install -q -r {PROJECT_ROOT}/requirements.txt\n",
    "    else:\n",
    "        print(\"Project already loaded.\")\n",
    "\n",
    "if (PROJECT_ROOT / \"src\").exists():\n",
    "    print(f\"\\nModuli disponibili:\")\n",
    "    for f in sorted((PROJECT_ROOT / \"src\").glob(\"*.py\")):\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cartella src/ non trovata!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for d in [\"data/raw\", \"data/processed\", \"artifacts\", \"models\", \"logs\", \"reports\"]:\n",
    "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Directory create.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    dataset_patterns = [\"cicids2017\", \"cic-ids-2017\", \"cicids\", \"ids2017\", \"network-intrusion-dataset\"]\n",
    "    dataset_path = None\n",
    "    for p in DATA_INPUT.iterdir():\n",
    "        name_lower = p.name.lower()\n",
    "        if \"pcap\" in name_lower:\n",
    "            continue\n",
    "        if any(pat in name_lower for pat in dataset_patterns):\n",
    "            if list(p.glob(\"**/*.csv\")):\n",
    "                dataset_path = p\n",
    "                break\n",
    "    if dataset_path:\n",
    "        print(f\"Dataset CSV trovato: {dataset_path}\")\n",
    "        raw_dir = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "        for csv in dataset_path.glob(\"**/*.csv\"):\n",
    "            dest = raw_dir / csv.name\n",
    "            if not dest.exists():\n",
    "                !cp \"{csv}\" \"{dest}\"\n",
    "        print(f\"CSV copiati in: {raw_dir}\")\n",
    "    else:\n",
    "        print(\"ERRORE: Dataset CIC-IDS2017 non trovato!\")\n",
    "\n",
    "csv_files = list((PROJECT_ROOT / \"data\" / \"raw\").glob(\"*.csv\"))\n",
    "print(f\"\\nCSV disponibili: {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024**2)\n",
    "    print(f\"  - {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.preprocessing import main as preprocessing_main\n",
    "from src.preprocessing import load_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.argv = ['preprocessing.py', '--balance-ratio', '2.0', '--n-jobs', '4']\n",
    "preprocessing_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, val, test, mappings = load_processed_data()\n",
    "print(f\"Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.feature_engineering import main as feature_engineering_main\n",
    "from src.feature_engineering import load_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sys.argv = ['feature_engineering.py', '--n-features', '30', '--rf-estimators', '100', '--n-jobs', '4']\n",
    "feature_engineering_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler, selected_features, importances, scaler_columns = load_artifacts()\n",
    "print(f\"Feature selezionate: {len(selected_features)}\")\n",
    "print(f\"Colonne scaler: {len(scaler_columns)}\")\n",
    "print(f\"\\nTop 10 feature:\")\n",
    "for i, feat in enumerate(selected_features[:10]):\n",
    "    print(f\"  {i+1:2}. {feat}: {importances[feat]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Modelli (Definitivo)\n",
    "\n",
    "Parametri: `--n-iter 50 --cv 5` (250 fit per modello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.training.random_forest import main as rf_main\n",
    "from src.training.xgboost_model import main as xgb_main\n",
    "from src.training.lightgbm_model import main as lgbm_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random Forest\n",
    "sys.argv = ['random_forest.py', '--task', 'binary', '--n-iter', '50', '--cv', '5', '--n-jobs', '4']\n",
    "rf_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# XGBoost\n",
    "sys.argv = ['xgboost_model.py', '--task', 'binary', '--n-iter', '50', '--cv', '5', '--n-jobs', '4']\n",
    "xgb_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM\n",
    "sys.argv = ['lightgbm_model.py', '--task', 'binary', '--n-iter', '50', '--cv', '5', '--n-jobs', '4']\n",
    "lgbm_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(\"Modelli addestrati:\")\n",
    "for model_dir in (PROJECT_ROOT / \"models\").iterdir():\n",
    "    if model_dir.is_dir() and model_dir.name != \"best_model\":\n",
    "        results_file = model_dir / \"results_binary.json\"\n",
    "        if results_file.exists():\n",
    "            with open(results_file) as f:\n",
    "                results = json.load(f)\n",
    "            metrics = results.get('validation_metrics', {})\n",
    "            print(f\"\\n  {model_dir.name}: Acc={metrics.get('accuracy', 0):.4f}, F1={metrics.get('f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.evaluation import main as evaluation_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/random_forest/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/xgboost/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/lightgbm/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confronto e Selezione Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.compare_models import main as compare_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# max-fpr: False Positive Rate massimo accettabile (2%)\n",
    "# max-latency-ms: Latenza massima per predizione (1ms)\n",
    "sys.argv = ['compare_models.py', '--max-fpr', '0.02', '--max-latency-ms', '1.0']\n",
    "compare_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model_dir = PROJECT_ROOT / \"models\" / \"best_model\"\n",
    "if best_model_dir.exists():\n",
    "    print(f\"Best model: {best_model_dir}\")\n",
    "    for f in best_model_dir.iterdir():\n",
    "        print(f\"  - {f.name}\")\n",
    "    metadata_file = best_model_dir / \"metadata.json\"\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file) as f:\n",
    "            print(f\"\\nBest: {json.load(f).get('best_model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizzazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "scorecard_img = PROJECT_ROOT / \"models\" / \"best_model\" / \"scorecard_comparison.png\"\n",
    "if scorecard_img.exists():\n",
    "    print(\"Scorecard:\")\n",
    "    display(Image(filename=str(scorecard_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model_name = \"lightgbm\"\n",
    "metadata_file = PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\"\n",
    "if metadata_file.exists():\n",
    "    with open(metadata_file) as f:\n",
    "        best_model_name = json.load(f).get('best_model', 'lightgbm')\n",
    "\n",
    "report_dir = PROJECT_ROOT / \"reports\" / best_model_name\n",
    "if report_dir.exists():\n",
    "    for img_name in [\"confusion_matrix_binary.png\", \"roc_curve_binary.png\"]:\n",
    "        img_path = report_dir / img_name\n",
    "        if img_path.exists():\n",
    "            print(f\"\\n{img_name}:\")\n",
    "            display(Image(filename=str(img_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Test Sniffer su PCAP\n",
    "\n",
    "Usa la funzione `analyze_pcap_file()` da `src/sniffer.py`.\n",
    "\n",
    "Dataset: `cicids2017-test-pcap` con `test_pcap/*.pcap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cerca dataset PCAP\n",
    "pcap_patterns = [\"pcap\", \"test-pcap\", \"test_pcap\"]\n",
    "pcap_dataset_path = None\n",
    "pcap_files = []\n",
    "\n",
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    for p in DATA_INPUT.iterdir():\n",
    "        if any(pat in p.name.lower() for pat in pcap_patterns):\n",
    "            found = list(p.glob(\"**/*.pcap\")) + list(p.glob(\"**/*.pcapng\"))\n",
    "            if found:\n",
    "                pcap_dataset_path = p\n",
    "                pcap_files = found\n",
    "                break\n",
    "\n",
    "if pcap_files:\n",
    "    print(f\"Dataset PCAP: {pcap_dataset_path}\")\n",
    "    for f in pcap_files:\n",
    "        print(f\"  - {f.name}: {f.stat().st_size/(1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(\"Dataset PCAP non trovato (opzionale).\")\n",
    "    print(\"Per testare, aggiungi dataset 'cicids2017-test-pcap' al notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import funzione analyze_pcap_file da sniffer.py\n",
    "if pcap_files:\n",
    "    from sniffer_old import analyze_pcap_file\n",
    "    print(\"Funzione analyze_pcap_file importata da src/sniffer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Analizza tutti i PCAP usando la funzione del progetto\n",
    "all_pcap_results = []\n",
    "\n",
    "if pcap_files:\n",
    "    for pcap_path in pcap_files:\n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"# PCAP: {pcap_path.name}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Usa la funzione analyze_pcap_file da sniffer.py\n",
    "        result = analyze_pcap_file(\n",
    "            pcap_path=str(pcap_path),\n",
    "            model_path=None,  # Usa best_model di default\n",
    "            threshold=0.5,    # Soglia standard\n",
    "            timeout=60,       # Timeout flusso 60s\n",
    "            min_packets=2,    # Minimo 2 pacchetti\n",
    "            verbose=False,    # Non stampare ogni flusso\n",
    "            progress_interval=50000  # Progress ogni 50k pacchetti\n",
    "        )\n",
    "        all_pcap_results.append(result)\n",
    "else:\n",
    "    print(\"Nessun PCAP da analizzare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Riepilogo finale PCAP\n",
    "if all_pcap_results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RIEPILOGO TEST PCAP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tot_pkt = sum(r['packets_processed'] for r in all_pcap_results)\n",
    "    tot_fl = sum(r['flows_analyzed'] for r in all_pcap_results)\n",
    "    tot_att = sum(r['attacks_detected'] for r in all_pcap_results)\n",
    "    tot_ben = sum(r['benign_detected'] for r in all_pcap_results)\n",
    "    \n",
    "    print(f\"\\nTotale:\")\n",
    "    print(f\"  Pacchetti:  {tot_pkt:,}\")\n",
    "    print(f\"  Flussi:     {tot_fl:,}\")\n",
    "    print(f\"  Attacchi:   {tot_att:,} ({tot_att/tot_fl*100:.1f}%)\" if tot_fl > 0 else \"\")\n",
    "    print(f\"  Benigni:    {tot_ben:,}\")\n",
    "    \n",
    "    print(f\"\\n{'PCAP':<35} {'Packets':>12} {'Flows':>10} {'Attacks':>10} {'Rate':>8}\")\n",
    "    print(\"-\"*77)\n",
    "    for r in all_pcap_results:\n",
    "        pct = r.get('detection_rate', 0)\n",
    "        print(f\"{r['pcap']:<35} {r['packets_processed']:>12,} {r['flows_analyzed']:>10,} {r['attacks_detected']:>10,} {pct:>7.1f}%\")\n",
    "    \n",
    "    # Salva risultati\n",
    "    results_path = PROJECT_ROOT / \"reports\" / \"pcap_test_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(all_pcap_results, f, indent=2, default=str)\n",
    "    print(f\"\\nRisultati salvati: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizzazione distribuzione probabilita\n",
    "if all_pcap_results:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Grafico 1: Attacchi vs Benigni per PCAP\n",
    "    pcap_names = [r['pcap'][:25] for r in all_pcap_results]\n",
    "    attacks = [r['attacks_detected'] for r in all_pcap_results]\n",
    "    benign = [r['benign_detected'] for r in all_pcap_results]\n",
    "    \n",
    "    x = range(len(pcap_names))\n",
    "    width = 0.35\n",
    "    axes[0].bar([i-width/2 for i in x], attacks, width, label='Attacks', color='red', alpha=0.7)\n",
    "    axes[0].bar([i+width/2 for i in x], benign, width, label='Benign', color='green', alpha=0.7)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(pcap_names, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Flussi')\n",
    "    axes[0].set_title('Distribuzione Rilevamenti')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Grafico 2: Detection Rate\n",
    "    rates = [r.get('detection_rate', 0) for r in all_pcap_results]\n",
    "    colors = ['red' if r > 30 else 'orange' if r > 10 else 'green' for r in rates]\n",
    "    axes[1].bar(x, rates, color=colors, alpha=0.7)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(pcap_names, rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Detection Rate (%)')\n",
    "    axes[1].set_title('Percentuale Attacchi Rilevati')\n",
    "    axes[1].axhline(y=20, color='orange', linestyle='--', alpha=0.5, label='20% atteso')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_ROOT / \"reports\" / \"pcap_test_chart.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    import zipfile\n",
    "    zip_path = PROJECT_ROOT / \"nids_ml_output.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        for f in (PROJECT_ROOT/\"artifacts\").glob(\"*\"): z.write(f, f\"artifacts/{f.name}\")\n",
    "        for d in (PROJECT_ROOT/\"models\").iterdir():\n",
    "            if d.is_dir():\n",
    "                for f in d.glob(\"*\"): z.write(f, f\"models/{d.name}/{f.name}\")\n",
    "        for f in (PROJECT_ROOT/\"reports\").rglob(\"*\"):\n",
    "            if f.is_file(): z.write(f, f\"reports/{f.relative_to(PROJECT_ROOT/'reports')}\")\n",
    "    print(f\"ZIP: {zip_path} ({zip_path.stat().st_size/(1024**2):.1f} MB)\")\n",
    "else:\n",
    "    print(\"Locale - output in cartella progetto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Riepilogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PIPELINE COMPLETATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_report = PROJECT_ROOT / \"models\" / \"best_model\" / \"comparison_results.json\"\n",
    "if best_report.exists():\n",
    "    with open(best_report) as f:\n",
    "        results = json.load(f)\n",
    "    best = max(results, key=lambda x: x.get('score', 0))\n",
    "    print(f\"\\nBest Model: {best['model_name'].upper()}\")\n",
    "    for k, v in best.get('metrics', {}).items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "if all_pcap_results:\n",
    "    tot_fl = sum(r['flows_analyzed'] for r in all_pcap_results)\n",
    "    tot_att = sum(r['attacks_detected'] for r in all_pcap_results)\n",
    "    print(f\"\\nTest PCAP: {tot_fl:,} flussi, {tot_att:,} attacchi ({tot_att/tot_fl*100:.1f}%)\" if tot_fl > 0 else \"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Comandi per uso locale:\")\n",
    "print(\"  # Live capture\")\n",
    "print(\"  sudo python src/sniffer.py --interface eth0 --verbose\")\n",
    "print(\"\")\n",
    "print(\"  # Analisi PCAP\")\n",
    "print(\"  sudo python src/sniffer.py --pcap file.pcap --threshold 0.3 --min-packets 1\")\n",
    "print(\"\")\n",
    "print(\"  # Da Python/notebook\")\n",
    "print(\"  from src.sniffer import analyze_pcap_file\")\n",
    "print(\"  results = analyze_pcap_file('file.pcap', threshold=0.3)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
