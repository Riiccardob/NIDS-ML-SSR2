{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6376134,"sourceType":"datasetVersion","datasetId":3674161},{"sourceId":14556816,"sourceType":"datasetVersion","datasetId":9297701}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NIDS-ML: Pipeline Completa con Multi-Training e Versionamento\n\n## Modalità Disponibili\n\n| MODE | Cosa fa | Quando usarla |\n|------|---------|---------------|\n| `'all'` | Pull + Preproc + Training + Eval + Compare + Sniff | Training completo da zero |\n| `'training'` | Pull + Preproc + Training | Solo training, poi push manuale |\n| `'evaluation'` | Pull + Preproc + Eval + Compare | Valutare modelli già su GitHub |\n| `'sniffing'` | Pull + Preproc + Eval + Compare + Sniff | Testare e confrontare su PCAP |\n\n## Nuove Funzionalità\n\n- **Multi-Training**: Esegui più configurazioni dello stesso algoritmo in una run\n- **Versionamento**: Ogni training salvato con ID univoco (es. `xgboost/cv5_iter100_gpu`)\n- **Timeout 12h**: Interrompe il training prima del crash Kaggle\n- **Compare Tutte Versioni**: Confronta TUTTE le versioni e trova il best assoluto\n- **Grafico Plateau**: Visualizza quando ulteriore training non migliora\n\n## Parametri Importanti\n\n- `TRAINING_CONFIGS`: Lista di configurazioni multi-training\n- `MODELS_TO_EVALUATE`: Lista modelli da valutare (default: tutti quelli disponibili)\n- `MODELS_TO_SNIFF`: Lista modelli per test PCAP con confronto","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CONFIGURAZIONE PRINCIPALE\n# ============================================================================\n\n# MODALITA: 'all', 'training', 'evaluation', 'sniffing'\n# - 'all':        Pull + Preproc + Training + Eval + Compare + Sniff\n# - 'training':   Pull + Preproc + Training\n# - 'evaluation': Pull + Preproc + Eval + Compare\n# - 'sniffing':   Pull + Preproc + Eval + Compare + Sniff (usa modelli da GitHub)\nMODE = 'evaluation'\n\n# Task\nTASK = 'binary'\n\n# ============================================================================\n# MULTI-TRAINING CONFIGURATION\n# ============================================================================\n# Lista di configurazioni da eseguire in sequenza.\n# Ogni training viene salvato con ID univoco: models/xgboost/cv5_iter100_gpu/\n#\n# Formato: {'model': tipo, 'n_iter': N, 'cv': K, 'gpu': bool}\n#\nTRAINING_CONFIGS = [\n    # XGBoost - configurazioni crescenti\n    {'model': 'xgboost', 'n_iter': 10, 'cv': 2, 'gpu': True},\n    {'model': 'xgboost', 'n_iter': 20, 'cv': 3, 'gpu': True},\n    {'model': 'xgboost', 'n_iter': 30, 'cv': 3, 'gpu': True},\n    {'model': 'xgboost', 'n_iter': 50, 'cv': 3, 'gpu': True},\n    {'model': 'xgboost', 'n_iter': 50, 'cv': 5, 'gpu': True},\n    # {'model': 'xgboost', 'n_iter': 100, 'cv': 5, 'gpu': True},  # Training lungo\n    \n    # LightGBM - configurazioni crescenti  \n    # {'model': 'lightgbm', 'n_iter': 20, 'cv': 3, 'gpu': False},\n    # {'model': 'lightgbm', 'n_iter': 50, 'cv': 5, 'gpu': False},\n    \n    # Random Forest - opzionale (molto lento)\n    # {'model': 'random_forest', 'n_iter': 20, 'cv': 3, 'gpu': False},\n]\n\n# ============================================================================\n# MODELLI PER EVALUATION/SNIFFING (se non fai training)\n# ============================================================================\n# Quali modelli valutare (per evaluation e compare)\n# None = tutti quelli disponibili\nMODELS_TO_EVALUATE = None  # o es: ['xgboost', 'lightgbm']\n\n# Quali modelli testare su PCAP (per confronto)\n# None = solo best_model, lista = confronta tutti\nMODELS_TO_SNIFF = None\n# MODELS_TO_SNIFF = ['xgboost', 'lightgbm']  # Confronta questi due\n\n# ============================================================================\n# PARAMETRI GENERALI\n# ============================================================================\nN_FEATURES = 30\nBALANCE_RATIO = 2.0\n\n# ============================================================================\n# PARAMETRI COMPARE\n# ============================================================================\nMAX_FPR = 0.02          # 2% False Positive Rate massimo\nMAX_LATENCY_MS = 2.0    # 2ms latenza massima per sample\n\n# ============================================================================\n# PARAMETRI SNIFFING\n# ============================================================================\nSNIFF_THRESHOLD = 0.5\nSNIFF_MIN_PACKETS = 2\nSNIFF_TIMEOUT = 120\n\n# ============================================================================\n# TIMEOUT KAGGLE (protezione crash)\n# ============================================================================\n# Kaggle ha limite 12h. Il training si interrompe prima per salvare i risultati.\nMAX_RUNTIME_HOURS = 11.5  # Lascia 30 min per eval/compare/push\n\n# ============================================================================\n# STAMPA CONFIGURAZIONE\n# ============================================================================\nprint(\"=\"*60)\nprint(\"CONFIGURAZIONE\")\nprint(\"=\"*60)\nprint(f\"MODE:              {MODE}\")\nprint(f\"Task:              {TASK}\")\n\nif MODE in ['all', 'training']:\n    print(f\"\\nTraining configs ({len(TRAINING_CONFIGS)}):\")\n    for i, cfg in enumerate(TRAINING_CONFIGS, 1):\n        gpu_str = \" [GPU]\" if cfg.get('gpu') else \"\"\n        print(f\"  {i}. {cfg['model']} cv={cfg['cv']} n_iter={cfg['n_iter']}{gpu_str}\")\n\nif MODE in ['all', 'evaluation', 'sniffing']:\n    print(f\"\\nModelli eval:      {MODELS_TO_EVALUATE or 'tutti disponibili'}\")\nif MODE in ['all', 'sniffing']:\n    print(f\"Modelli sniff:     {MODELS_TO_SNIFF or 'solo best_model'}\")\n    \nprint(f\"\\nCompare:           FPR<={MAX_FPR*100}%, Latency<={MAX_LATENCY_MS}ms\")\nprint(f\"Max runtime:       {MAX_RUNTIME_HOURS}h\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:30.351451Z","iopub.execute_input":"2026-01-21T10:12:30.351682Z","iopub.status.idle":"2026-01-21T10:12:30.361612Z","shell.execute_reply.started":"2026-01-21T10:12:30.351659Z","shell.execute_reply":"2026-01-21T10:12:30.360929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 1. Setup Ambiente","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport shutil\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\n# ============================================================================\n# TIMEOUT UTILITIES\n# ============================================================================\nNOTEBOOK_START_TIME = datetime.now()\n\ndef check_timeout(margin_minutes=30):\n    \"\"\"\n    Verifica se stiamo per superare il timeout Kaggle.\n    \n    Returns:\n        Tuple (is_timeout: bool, remaining: timedelta)\n    \"\"\"\n    elapsed = datetime.now() - NOTEBOOK_START_TIME\n    max_allowed = timedelta(hours=MAX_RUNTIME_HOURS) - timedelta(minutes=margin_minutes)\n    remaining = max_allowed - elapsed\n    \n    if elapsed > max_allowed:\n        return True, timedelta(0)\n    return False, remaining\n\ndef format_timedelta(td):\n    \"\"\"Formatta timedelta in stringa leggibile.\"\"\"\n    total_seconds = int(td.total_seconds())\n    if total_seconds < 0:\n        return \"0s\"\n    hours, remainder = divmod(total_seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    if hours > 0:\n        return f\"{hours}h {minutes}m\"\n    elif minutes > 0:\n        return f\"{minutes}m {seconds}s\"\n    return f\"{seconds}s\"\n\nprint(f\"Notebook started:  {NOTEBOOK_START_TIME.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Max runtime:       {MAX_RUNTIME_HOURS}h\")\nprint(f\"Deadline:          {(NOTEBOOK_START_TIME + timedelta(hours=MAX_RUNTIME_HOURS)).strftime('%H:%M:%S')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:30.362946Z","iopub.execute_input":"2026-01-21T10:12:30.363238Z","iopub.status.idle":"2026-01-21T10:12:30.392331Z","shell.execute_reply.started":"2026-01-21T10:12:30.363209Z","shell.execute_reply":"2026-01-21T10:12:30.391516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# DETECT ENVIRONMENT\n# ============================================================================\nif Path(\"/kaggle/input\").exists():\n    ENV = \"kaggle\"\n    PROJECT_ROOT = Path(\"/kaggle/working\")\n    DATA_INPUT = Path(\"/kaggle/input\")\nelif Path(\"/content\").exists():\n    ENV = \"colab\"\n    PROJECT_ROOT = Path(\"/content/NIDS-ML\")\n    DATA_INPUT = Path(\"/content\")\nelse:\n    ENV = \"local\"\n    PROJECT_ROOT = Path.cwd()\n    if not (PROJECT_ROOT / \"src\").exists():\n        PROJECT_ROOT = PROJECT_ROOT.parent\n    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n\nprint(f\"Ambiente: {ENV}\")\nprint(f\"Project root: {PROJECT_ROOT}\")\n\nsys.path.insert(0, str(PROJECT_ROOT))\nos.chdir(PROJECT_ROOT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:30.393181Z","iopub.execute_input":"2026-01-21T10:12:30.393514Z","iopub.status.idle":"2026-01-21T10:12:30.405480Z","shell.execute_reply.started":"2026-01-21T10:12:30.393493Z","shell.execute_reply":"2026-01-21T10:12:30.404776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone/Pull da GitHub\nREPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\n\nif ENV in [\"kaggle\", \"colab\"]:\n    print(f\"Cloning {REPO_URL}...\")\n    !rm -rf temp_repo 2>/dev/null\n    !git clone --depth 1 {REPO_URL} temp_repo 2>/dev/null\n    !cp -r temp_repo/* {PROJECT_ROOT}/ 2>/dev/null\n    !rm -rf temp_repo\n    print(\"Done.\")\n    \n    if (PROJECT_ROOT / \"requirements.txt\").exists():\n        !pip install -q -r {PROJECT_ROOT}/requirements.txt 2>/dev/null\n\n# Crea directory\nfor d in [\"data/raw\", \"data/processed\", \"artifacts\", \"models\", \"logs\", \"reports\"]:\n    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n\n# Mostra modelli presenti da GitHub (incluse versioni)\nprint(f\"\\nModelli da GitHub:\")\nfor name in ['random_forest', 'xgboost', 'lightgbm']:\n    model_dir = PROJECT_ROOT / \"models\" / name\n    if model_dir.exists():\n        # Cerca versioni\n        versions = []\n        for item in model_dir.iterdir():\n            if item.is_dir() and not item.is_symlink():\n                if (item / f\"model_{TASK}.pkl\").exists():\n                    versions.append(item.name)\n        # Check modello root\n        if (model_dir / f\"model_{TASK}.pkl\").exists():\n            versions.insert(0, \"[default]\")\n        if versions:\n            print(f\"  {name}: {', '.join(versions)}\")\n        else:\n            print(f\"  {name}: -\")\n    else:\n        print(f\"  {name}: -\")\n\nif (PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\").exists():\n    print(f\"  best_model: OK\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:30.449825Z","iopub.execute_input":"2026-01-21T10:12:30.450529Z","iopub.status.idle":"2026-01-21T10:12:39.022556Z","shell.execute_reply.started":"2026-01-21T10:12:30.450495Z","shell.execute_reply":"2026-01-21T10:12:39.021732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copia dataset CSV\nif ENV in [\"kaggle\", \"colab\"]:\n    patterns = [\"cicids2017\", \"cic-ids\", \"ids2017\", \"network-intrusion\"]\n    for p in DATA_INPUT.iterdir():\n        if \"pcap\" in p.name.lower():\n            continue\n        if any(pat in p.name.lower() for pat in patterns) and list(p.glob(\"**/*.csv\")):\n            print(f\"Dataset CSV: {p.name}\")\n            for csv in p.glob(\"**/*.csv\"):\n                dest = PROJECT_ROOT / \"data\" / \"raw\" / csv.name\n                if not dest.exists():\n                    shutil.copy(csv, dest)\n            break\n\nprint(f\"CSV disponibili: {len(list((PROJECT_ROOT / 'data' / 'raw').glob('*.csv')))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:39.024243Z","iopub.execute_input":"2026-01-21T10:12:39.024580Z","iopub.status.idle":"2026-01-21T10:12:39.043175Z","shell.execute_reply.started":"2026-01-21T10:12:39.024552Z","shell.execute_reply":"2026-01-21T10:12:39.042735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 2. Preprocessing & Feature Engineering\n\n*Eseguito per MODE: all, training, evaluation, sniffing*","metadata":{}},{"cell_type":"code","source":"%%time\nif MODE in ['all', 'training', 'evaluation', 'sniffing']:\n    from src.preprocessing import main as preprocessing_main, load_processed_data\n    \n    processed_test = PROJECT_ROOT / \"data\" / \"processed\" / \"test.parquet\"\n    if not processed_test.exists():\n        print(\"Preprocessing...\")\n        sys.argv = ['preprocessing.py', '--balance-ratio', str(BALANCE_RATIO), '--n-jobs', '4']\n        preprocessing_main()\n    else:\n        print(\"Dati gia' processati.\")\n    \n    train, val, test, mappings = load_processed_data()\n    print(f\"Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}\")\nelse:\n    print(f\"Skip (MODE={MODE})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:39.044298Z","iopub.execute_input":"2026-01-21T10:12:39.044713Z","iopub.status.idle":"2026-01-21T10:12:40.125149Z","shell.execute_reply.started":"2026-01-21T10:12:39.044689Z","shell.execute_reply":"2026-01-21T10:12:40.124563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nif MODE in ['all', 'training', 'evaluation', 'sniffing']:\n    from src.feature_engineering import main as fe_main, load_artifacts\n    \n    if not (PROJECT_ROOT / \"artifacts\" / \"scaler.pkl\").exists():\n        print(\"Feature engineering...\")\n        sys.argv = ['feature_engineering.py', '--n-features', str(N_FEATURES), '--n-jobs', '4']\n        fe_main()\n    else:\n        print(\"Artifacts gia' presenti.\")\n    \n    scaler, selected_features, _, _ = load_artifacts()\n    print(f\"Feature: {len(selected_features)}\")\nelse:\n    print(f\"Skip (MODE={MODE})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:40.125963Z","iopub.execute_input":"2026-01-21T10:12:40.126269Z","iopub.status.idle":"2026-01-21T10:12:40.134411Z","shell.execute_reply.started":"2026-01-21T10:12:40.126225Z","shell.execute_reply":"2026-01-21T10:12:40.133751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 3. Multi-Training con Versionamento\n\n*Eseguito per MODE: all, training*\n\nOgni training viene salvato con ID univoco basato sui parametri:\n- `models/xgboost/cv3_iter20_gpu/model_binary.pkl`\n- `models/xgboost/cv5_iter100_gpu/model_binary.pkl`\n- `models/lightgbm/cv5_iter50/model_binary.pkl`\n\nSe il timeout si avvicina, il training viene interrotto per salvare i risultati già completati.","metadata":{}},{"cell_type":"code","source":"# Traccia quali modelli sono stati trainati in QUESTA sessione\nTRAINED_THIS_SESSION = []  # [(model_type, version_id), ...]\nSKIPPED_CONFIGS = []       # Configs skippati per timeout\n\nif MODE in ['all', 'training']:\n    print(f\"Training configs da eseguire: {len(TRAINING_CONFIGS)}\")\n    for i, cfg in enumerate(TRAINING_CONFIGS, 1):\n        gpu_str = \" [GPU]\" if cfg.get('gpu') else \"\"\n        print(f\"  {i}. {cfg['model']} cv={cfg['cv']} n_iter={cfg['n_iter']}{gpu_str}\")\nelse:\n    print(f\"Skip training (MODE={MODE})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:40.136054Z","iopub.execute_input":"2026-01-21T10:12:40.136713Z","iopub.status.idle":"2026-01-21T10:12:40.146731Z","shell.execute_reply.started":"2026-01-21T10:12:40.136686Z","shell.execute_reply":"2026-01-21T10:12:40.146031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_version_exists(model_type, n_iter, cv, gpu=False):\n    \"\"\"\n    Verifica se una versione del modello esiste già.\n    \n    Returns:\n        Tuple (exists: bool, version_id: str, version_dir: Path)\n    \"\"\"\n    from src.model_versioning import generate_version_id, get_version_dir\n    \n    extra_params = {'gpu': gpu} if gpu else None\n    version_id = generate_version_id(n_iter, cv, extra_params)\n    version_dir = get_version_dir(model_type, n_iter, cv, extra_params, create=False)\n    \n    model_file = version_dir / f\"model_{TASK}.pkl\"\n    exists = model_file.exists()\n    \n    return exists, version_id, version_dir\n\n\ndef run_single_training(config, config_idx, total_configs):\n    \"\"\"\n    Esegue un singolo training con la configurazione specificata.\n    \n    Returns:\n        bool: True se completato (o già esistente), False se errore/timeout\n    \"\"\"\n    model_type = config['model']\n    n_iter = config['n_iter']\n    cv = config['cv']\n    use_gpu = config.get('gpu', False)\n    \n    # Verifica se esiste già\n    exists, version_id, version_dir = check_version_exists(model_type, n_iter, cv, use_gpu)\n    \n    if exists:\n        print(f\"\\n[{config_idx}/{total_configs}] {model_type}/{version_id}\")\n        print(f\"  -> Già esistente, skip\")\n        TRAINED_THIS_SESSION.append((model_type, version_id))\n        return True\n    \n    # Header\n    print(f\"\\n{'#' * 70}\")\n    print(f\"# TRAINING {config_idx}/{total_configs}: {model_type.upper()}\")\n    print(f\"# Config: cv={cv}, n_iter={n_iter}, gpu={use_gpu}\")\n    print(f\"# Version ID: {version_id}\")\n    print(f\"{'#' * 70}\")\n    \n    # Check timeout (60 min margine per il training)\n    is_timeout, remaining = check_timeout(margin_minutes=60)\n    if is_timeout:\n        print(f\"\\n  TIMEOUT: Tempo insufficiente, training skippato\")\n        SKIPPED_CONFIGS.append(config)\n        return False\n    \n    print(f\"  Tempo rimanente: {format_timedelta(remaining)}\")\n    \n    start_time = datetime.now()\n    \n    try:\n        # Prepara argomenti\n        if model_type == 'xgboost':\n            from src.training.xgboost_model import main as train_main\n            args = ['xgb.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv)]\n            if use_gpu:\n                args.append('--gpu')\n            else:\n                args.extend(['--n-jobs', '4'])\n                \n        elif model_type == 'lightgbm':\n            from src.training.lightgbm_model import main as train_main\n            args = ['lgbm.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--n-jobs', '4']\n            \n        elif model_type == 'random_forest':\n            from src.training.random_forest import main as train_main\n            args = ['rf.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--n-jobs', '4']\n            \n        else:\n            print(f\"  ERRORE: Tipo modello sconosciuto: {model_type}\")\n            SKIPPED_CONFIGS.append(config)\n            return False\n        \n        # Esegui training\n        sys.argv = args\n        train_main()\n        \n        elapsed = datetime.now() - start_time\n        print(f\"\\n  Training completato in {format_timedelta(elapsed)}\")\n        \n        TRAINED_THIS_SESSION.append((model_type, version_id))\n        return True\n        \n    except Exception as e:\n        print(f\"\\n  ERRORE durante training: {e}\")\n        import traceback\n        traceback.print_exc()\n        SKIPPED_CONFIGS.append(config)\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:40.147527Z","iopub.execute_input":"2026-01-21T10:12:40.147856Z","iopub.status.idle":"2026-01-21T10:12:40.164381Z","shell.execute_reply.started":"2026-01-21T10:12:40.147836Z","shell.execute_reply":"2026-01-21T10:12:40.163787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# ============================================================================\n# ESECUZIONE MULTI-TRAINING\n# ============================================================================\nif MODE in ['all', 'training']:\n    total = len(TRAINING_CONFIGS)\n    completed = 0\n    \n    for i, config in enumerate(TRAINING_CONFIGS, 1):\n        # Check timeout prima di ogni training\n        is_timeout, remaining = check_timeout(margin_minutes=45)\n        if is_timeout:\n            print(f\"\\n{'!' * 70}\")\n            print(f\"! TIMEOUT RAGGIUNTO - Training rimanenti skippati\")\n            print(f\"{'!' * 70}\")\n            SKIPPED_CONFIGS.extend(TRAINING_CONFIGS[i-1:])\n            break\n        \n        # Esegui training\n        success = run_single_training(config, i, total)\n        if success:\n            completed += 1\n    \n    # Riepilogo\n    print(f\"\\n{'=' * 70}\")\n    print(f\"TRAINING SUMMARY\")\n    print(f\"{'=' * 70}\")\n    print(f\"Completati:  {completed}/{total}\")\n    print(f\"Skippati:    {len(SKIPPED_CONFIGS)}\")\n    \n    if TRAINED_THIS_SESSION:\n        print(f\"\\nVersioni create/confermate:\")\n        for mt, vid in TRAINED_THIS_SESSION:\n            print(f\"  - {mt}/{vid}\")\n    \n    if SKIPPED_CONFIGS:\n        print(f\"\\nConfigs skippati:\")\n        for cfg in SKIPPED_CONFIGS:\n            print(f\"  - {cfg['model']} cv={cfg['cv']} n_iter={cfg['n_iter']}\")\nelse:\n    print(f\"Skip training (MODE={MODE})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:12:40.165141Z","iopub.execute_input":"2026-01-21T10:12:40.165419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Riepilogo modelli disponibili (incluse tutte le versioni)\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODELLI DISPONIBILI (tutte le versioni)\")\nprint(\"=\"*70)\n\nfrom src.model_versioning import list_model_versions\n\nversions = list_model_versions(task=TASK)\nif versions:\n    print(f\"\\n{'Modello':<15} {'Versione':<25} {'Origine':<15} {'F1':>10}\")\n    print(\"-\"*70)\n    \n    for v in versions:\n        origine = \"TRAINATO ORA\" if (v['model_type'], v['version_id']) in TRAINED_THIS_SESSION else \"GitHub\"\n        f1 = v.get('validation_metrics', {}).get('f1', 0)\n        print(f\"{v['model_type']:<15} {v['version_id']:<25} {origine:<15} {f1:>10.4f}\")\nelse:\n    print(\"Nessun modello trovato\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 4. Evaluation\n\n*Eseguito per MODE: all, evaluation, sniffing*","metadata":{}},{"cell_type":"code","source":"if MODE in ['all', 'evaluation', 'sniffing']:\n    from src.evaluation import main as evaluation_main\n    from src.model_versioning import list_model_versions\n    \n    # Trova tutte le versioni da valutare\n    all_versions = list_model_versions(task=TASK)\n    \n    # Filtra se specificato\n    if MODELS_TO_EVALUATE:\n        versions_to_eval = [v for v in all_versions if v['model_type'] in MODELS_TO_EVALUATE]\n    else:\n        versions_to_eval = all_versions\n    \n    print(f\"Versioni da valutare: {len(versions_to_eval)}\")\n    for v in versions_to_eval:\n        origine = \"TRAINATO ORA\" if (v['model_type'], v['version_id']) in TRAINED_THIS_SESSION else \"GitHub\"\n        print(f\"  - {v['model_type']}/{v['version_id']} ({origine})\")\nelse:\n    print(f\"Skip evaluation (MODE={MODE})\")\n    versions_to_eval = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Esegui evaluation per ogni versione\nif MODE in ['all', 'evaluation', 'sniffing'] and versions_to_eval:\n    for i, v in enumerate(versions_to_eval, 1):\n        full_id = f\"{v['model_type']}/{v['version_id']}\"\n        model_path = v['model_path']\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"EVALUATION [{i}/{len(versions_to_eval)}]: {full_id}\")\n        origine = \"TRAINATO ORA\" if (v['model_type'], v['version_id']) in TRAINED_THIS_SESSION else \"GitHub\"\n        print(f\"Origine modello: {origine}\")\n        print(f\"{'='*60}\")\n        \n        # Check timeout\n        is_timeout, _ = check_timeout(margin_minutes=20)\n        if is_timeout:\n            print(f\"  TIMEOUT - evaluation interrotta\")\n            break\n        \n        sys.argv = ['eval.py', '--model-path', str(model_path), '--task', TASK]\n        try:\n            evaluation_main()\n        except Exception as e:\n            print(f\"ERRORE: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 5. Compare Models (Tutte le Versioni)\n\n*Eseguito per MODE: all, evaluation, sniffing*\n\nConfronta **TUTTE** le versioni disponibili e genera:\n- Ranking per ogni algoritmo\n- Grafico plateau (F1/Recall vs Training Effort)\n- Selezione best_model assoluto","metadata":{}},{"cell_type":"code","source":"%%time\nif MODE in ['all', 'evaluation', 'sniffing']:\n    from src.compare_models import main as compare_main\n    \n    print(f\"Compare: FPR <= {MAX_FPR*100}%, Latency <= {MAX_LATENCY_MS}ms\")\n    sys.argv = ['compare.py', '--max-fpr', str(MAX_FPR), '--max-latency-ms', str(MAX_LATENCY_MS)]\n    compare_main()\nelse:\n    print(f\"Skip compare (MODE={MODE})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mostra best model e grafici\nbest_model_dir = PROJECT_ROOT / \"models\" / \"best_model\"\nif best_model_dir.exists() and (best_model_dir / \"metadata.json\").exists():\n    with open(best_model_dir / \"metadata.json\") as f:\n        meta = json.load(f)\n    print(f\"\\nBEST MODEL: {meta.get('best_model', 'N/A').upper()}\")\n    print(f\"Score: {meta.get('score', 0):.4f}\")\n    print(f\"F1: {meta.get('metrics', {}).get('f1', 0):.4f}\")\n    print(f\"Recall: {meta.get('metrics', {}).get('recall', 0):.4f}\")\n    \n    # Grafici\n    from IPython.display import Image, display\n    for img_name in ['plateau_analysis.png', 'scorecard_comparison.png', 'algorithm_rankings.png']:\n        img_path = best_model_dir / img_name\n        if img_path.exists():\n            print(f\"\\n{img_name}:\")\n            display(Image(filename=str(img_path)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 6. Test Sniffer su PCAP con CONFRONTO\n\n*Eseguito per MODE: all, sniffing*\n\nQuesta sezione permette di **confrontare** le performance di diversi modelli sugli stessi PCAP.","metadata":{}},{"cell_type":"code","source":"# Cerca PCAP\npcap_files = []\n\nif MODE in ['all', 'sniffing'] and ENV in [\"kaggle\", \"colab\"]:\n    pcap_patterns = [\"pcap\", \"cic-ids-2017-pcap\", \"cicids\"]\n    \n    for p in DATA_INPUT.iterdir():\n        name_lower = p.name.lower().replace(\"_\", \"-\")\n        if any(pat in name_lower for pat in pcap_patterns):\n            found = list(p.glob(\"**/*.pcap\")) + list(p.glob(\"**/*.pcapng\"))\n            if found:\n                pcap_files = sorted(found, key=lambda x: x.name)\n                print(f\"Dataset PCAP: {p.name}\")\n                break\n    \n    if pcap_files:\n        print(f\"\\nPCAP disponibili ({len(pcap_files)}):\")\n        for f in pcap_files:\n            print(f\"  - {f.name}: {f.stat().st_size/(1024**2):.1f} MB\")\n    else:\n        print(\"Dataset PCAP non trovato.\")\nelse:\n    print(f\"Skip ricerca PCAP (MODE={MODE} o ambiente locale)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determina quali modelli testare\nmodels_to_sniff = []\n\nif MODE in ['all', 'sniffing'] and pcap_files:\n    if MODELS_TO_SNIFF:\n        # Lista specifica - cerca la migliore versione per ogni tipo\n        for name in MODELS_TO_SNIFF:\n            # Prima cerca best version di questo tipo\n            type_versions = [v for v in list_model_versions(task=TASK) if v['model_type'] == name]\n            if type_versions:\n                # Prendi la migliore per F1\n                best_v = max(type_versions, key=lambda x: x.get('validation_metrics', {}).get('f1', 0))\n                models_to_sniff.append((f\"{name}/{best_v['version_id']}\", best_v['model_path']))\n            else:\n                # Fallback: cerca nella root\n                model_path = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n                if model_path.exists():\n                    models_to_sniff.append((name, model_path))\n                else:\n                    print(f\"WARNING: {name} non trovato, skip\")\n    else:\n        # Solo best_model\n        best_path = PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\"\n        if best_path.exists():\n            models_to_sniff.append(('best_model', best_path))\n        else:\n            # Fallback: primo disponibile\n            for name in ['lightgbm', 'xgboost', 'random_forest']:\n                p = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n                if p.exists():\n                    models_to_sniff.append((name, p))\n                    break\n    \n    print(f\"\\nModelli per test PCAP:\")\n    for name, path in models_to_sniff:\n        print(f\"  - {name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Test PCAP con CONFRONTO tra modelli\nall_sniff_results = {}  # {model_name: [results per pcap]}\n\nif MODE in ['all', 'sniffing'] and pcap_files and models_to_sniff:\n    from src.sniffer import analyze_pcap_file\n    \n    for model_name, model_path in models_to_sniff:\n        print(f\"\\n{'#'*70}\")\n        print(f\"# MODELLO: {model_name.upper()}\")\n        print(f\"{'#'*70}\")\n        \n        model_results = []\n        \n        for pcap_path in pcap_files:\n            print(f\"\\n  Testing: {pcap_path.name}...\")\n            \n            # Check timeout\n            is_timeout, _ = check_timeout(margin_minutes=20)\n            if is_timeout:\n                print(f\"  TIMEOUT - PCAP rimanenti skippati\")\n                break\n            \n            try:\n                result = analyze_pcap_file(\n                    pcap_path=str(pcap_path),\n                    model_path=str(model_path),\n                    threshold=SNIFF_THRESHOLD,\n                    timeout=SNIFF_TIMEOUT,\n                    min_packets=SNIFF_MIN_PACKETS,\n                    verbose=False,\n                    progress_interval=500000,\n                    show_progress=True\n                )\n                result['model'] = model_name\n                model_results.append(result)\n                \n                rate = result.get('detection_rate', 0)\n                print(f\"  -> Flows: {result['flows_analyzed']:,}, Attacks: {result['attacks_detected']:,} ({rate:.1f}%)\")\n                \n            except Exception as e:\n                print(f\"  ERRORE: {e}\")\n        \n        all_sniff_results[model_name] = model_results\nelse:\n    print(\"Test PCAP non eseguito\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TABELLA CONFRONTO MODELLI SU PCAP\nif all_sniff_results:\n    print(\"\\n\" + \"=\"*90)\n    print(\"CONFRONTO PERFORMANCE SU PCAP\")\n    print(\"=\"*90)\n    \n    # Raccogli tutti i PCAP\n    all_pcaps = set()\n    for results in all_sniff_results.values():\n        for r in results:\n            all_pcaps.add(r['pcap'])\n    all_pcaps = sorted(all_pcaps)\n    \n    # Header\n    models = list(all_sniff_results.keys())\n    header = f\"{'PCAP':<35}\"\n    for m in models:\n        header += f\" | {m:^20}\"\n    print(header)\n    print(\"-\" * len(header))\n    \n    # Per ogni PCAP\n    totals = {m: {'flows': 0, 'attacks': 0} for m in models}\n    \n    for pcap_name in all_pcaps:\n        row = f\"{pcap_name[:35]:<35}\"\n        for m in models:\n            # Trova risultato per questo modello e pcap\n            result = None\n            for r in all_sniff_results.get(m, []):\n                if r['pcap'] == pcap_name:\n                    result = r\n                    break\n            \n            if result:\n                attacks = result['attacks_detected']\n                flows = result['flows_analyzed']\n                rate = result.get('detection_rate', 0)\n                row += f\" | {attacks:>6} / {flows:<6} ({rate:>5.1f}%)\"\n                totals[m]['flows'] += flows\n                totals[m]['attacks'] += attacks\n            else:\n                row += f\" | {'N/A':^20}\"\n        print(row)\n    \n    # Totali\n    print(\"-\" * len(header))\n    row = f\"{'TOTALE':<35}\"\n    for m in models:\n        t = totals[m]\n        if t['flows'] > 0:\n            rate = t['attacks'] / t['flows'] * 100\n            row += f\" | {t['attacks']:>6} / {t['flows']:<6} ({rate:>5.1f}%)\"\n        else:\n            row += f\" | {'N/A':^20}\"\n    print(row)\n    \n    # Salva risultati\n    comparison_data = {\n        'timestamp': datetime.now().isoformat(),\n        'models': models,\n        'parameters': {\n            'threshold': SNIFF_THRESHOLD,\n            'min_packets': SNIFF_MIN_PACKETS,\n            'timeout': SNIFF_TIMEOUT\n        },\n        'results': {k: v for k, v in all_sniff_results.items()},\n        'totals': totals\n    }\n    \n    with open(PROJECT_ROOT / \"reports\" / \"pcap_comparison.json\", 'w') as f:\n        json.dump(comparison_data, f, indent=2, default=str)\n    print(f\"\\nRisultati salvati in: reports/pcap_comparison.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 7. Download Output","metadata":{}},{"cell_type":"code","source":"import zipfile\n\nif ENV in [\"kaggle\", \"colab\"]:\n    zip_path = PROJECT_ROOT / \"nids_ml_output.zip\"\n    \n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n        # Artifacts\n        for f in (PROJECT_ROOT / \"artifacts\").glob(\"*\"):\n            if f.is_file():\n                z.write(f, f\"artifacts/{f.name}\")\n        \n        # Models (tutti, incluse versioni)\n        for model_dir in (PROJECT_ROOT / \"models\").iterdir():\n            if model_dir.is_dir():\n                for f in model_dir.rglob(\"*\"):\n                    if f.is_file():\n                        rel_path = f.relative_to(PROJECT_ROOT / \"models\")\n                        z.write(f, f\"models/{rel_path}\")\n        \n        # Reports\n        for f in (PROJECT_ROOT / \"reports\").rglob(\"*\"):\n            if f.is_file():\n                rel_path = f.relative_to(PROJECT_ROOT / \"reports\")\n                z.write(f, f\"reports/{rel_path}\")\n    \n    print(f\"ZIP: {zip_path.name} ({zip_path.stat().st_size/(1024**2):.1f} MB)\")\n    print(f\"\\nPer usare i modelli:\")\n    print(f\"  1. Scarica lo ZIP\")\n    print(f\"  2. Estrai e copia models/ nel repo locale\")\n    print(f\"  3. git add models/ && git commit && git push\")\nelse:\n    print(\"Locale - output nelle cartelle del progetto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 8. Riepilogo","metadata":{}},{"cell_type":"code","source":"elapsed = datetime.now() - NOTEBOOK_START_TIME\n\nprint(\"=\"*70)\nprint(\"RIEPILOGO ESECUZIONE\")\nprint(\"=\"*70)\n\nprint(f\"\\nTempo totale: {format_timedelta(elapsed)}\")\nprint(f\"Modalità: {MODE}\")\n\n# Training\nif TRAINED_THIS_SESSION:\n    print(f\"\\nTrainati in questa sessione ({len(TRAINED_THIS_SESSION)}):\")\n    for mt, vid in TRAINED_THIS_SESSION:\n        print(f\"  - {mt}/{vid}\")\n\nif SKIPPED_CONFIGS:\n    print(f\"\\nSkippati per timeout ({len(SKIPPED_CONFIGS)}):\")\n    for cfg in SKIPPED_CONFIGS:\n        print(f\"  - {cfg['model']} cv={cfg['cv']} n_iter={cfg['n_iter']}\")\n\n# Best model\nif (PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\").exists():\n    with open(PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\") as f:\n        meta = json.load(f)\n    print(f\"\\nBest Model: {meta.get('best_model', 'N/A').upper()}\")\n    print(f\"  Score: {meta.get('score', 0):.4f}\")\n\n# PCAP comparison\nif all_sniff_results:\n    print(f\"\\nTest PCAP eseguito su {len(models_to_sniff)} modelli\")\n    for m in all_sniff_results:\n        t = totals.get(m, {})\n        if t.get('flows', 0) > 0:\n            rate = t['attacks'] / t['flows'] * 100\n            print(f\"  {m}: {t['attacks']:,} attacchi su {t['flows']:,} flussi ({rate:.1f}%)\")\n\nprint(\"\\n\" + \"=\"*70)\nif MODE in ['all', 'training'] and TRAINED_THIS_SESSION:\n    print(\"PROSSIMI STEP:\")\n    print(\"1. Scarica lo ZIP\")\n    print(\"2. Estrai models/ e copia nel repo locale\")\n    print(\"3. git add models/ && git commit -m 'Add trained models' && git push\")\nelse:\n    print(\"Per usare il modello in locale:\")\n    print(\"  sudo python src/sniffer.py --interface eth0 --verbose\")\nprint(\"=\"*70)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}