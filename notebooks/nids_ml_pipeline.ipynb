{
 "metadata": {
  "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {"cell_type": "markdown", "source": "# NIDS-ML: Pipeline con Multi-Training e Versionamento\n\n## Modalita\n| MODE | Cosa fa |\n|------|---------|  \n| `'all'` | Preproc + Multi-Training + Eval + Compare + Sniff |\n| `'training'` | Preproc + Multi-Training |\n| `'evaluation'` | Preproc + Eval + Compare |\n| `'sniffing'` | Preproc + Eval + Compare + Sniff |", "metadata": {}},
  {"cell_type": "code", "source": "# ============================================================================\n# CONFIGURAZIONE PRINCIPALE\n# ============================================================================\n\n# MODALITA: 'all', 'training', 'evaluation', 'sniffing'\nMODE = 'all'\nTASK = 'binary'\n\n# ============================================================================\n# MULTI-TRAINING: Lista configurazioni da eseguire\n# Ogni training salvato come: models/xgboost/cv5_iter100_gpu/\n# ============================================================================\nTRAINING_CONFIGS = [\n    {'model': 'xgboost', 'n_iter': 20, 'cv': 3, 'gpu': True},\n    {'model': 'xgboost', 'n_iter': 50, 'cv': 5, 'gpu': True},\n    {'model': 'lightgbm', 'n_iter': 20, 'cv': 3, 'gpu': False},\n    {'model': 'lightgbm', 'n_iter': 50, 'cv': 5, 'gpu': False},\n]\n\n# ============================================================================\n# MODELLI PER EVAL/SNIFF (se non fai training)\n# ============================================================================\nMODELS_TO_EVALUATE = None  # None = tutti disponibili\nMODELS_TO_SNIFF = None     # None = solo best_model\n\n# ============================================================================\n# PARAMETRI\n# ============================================================================\nN_FEATURES = 30\nBALANCE_RATIO = 2.0\nMAX_FPR = 0.02\nMAX_LATENCY_MS = 2.0\n\n# Sniffing\nSNIFF_THRESHOLD = 0.5\nSNIFF_MIN_PACKETS = 2\nSNIFF_TIMEOUT = 120\nSNIFF_VERBOSE = False\n\n# Timeout Kaggle (protezione crash)\nMAX_RUNTIME_HOURS = 11.5\n\n# Git\nGIT_PUSH_ENABLED = False\nGIT_USER = \"Riiccardob\"\n\nprint(f\"MODE: {MODE}, Task: {TASK}\")\nprint(f\"Training configs: {len(TRAINING_CONFIGS)}\")\nfor i, c in enumerate(TRAINING_CONFIGS, 1):\n    print(f\"  {i}. {c['model']} cv={c['cv']} n_iter={c['n_iter']} gpu={c.get('gpu', False)}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Setup\nimport os, sys, json, shutil\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nNOTEBOOK_START = datetime.now()\n\ndef check_timeout(margin_min=30):\n    elapsed = datetime.now() - NOTEBOOK_START\n    limit = timedelta(hours=MAX_RUNTIME_HOURS) - timedelta(minutes=margin_min)\n    return elapsed > limit, limit - elapsed\n\ndef format_td(td):\n    s = int(td.total_seconds())\n    return f\"{s//3600}h {(s%3600)//60}m\" if s > 0 else \"0s\"\n\nif Path(\"/kaggle/input\").exists():\n    ENV, PROJECT_ROOT, DATA_INPUT = \"kaggle\", Path(\"/kaggle/working\"), Path(\"/kaggle/input\")\nelif Path(\"/content\").exists():\n    ENV, PROJECT_ROOT, DATA_INPUT = \"colab\", Path(\"/content/NIDS-ML\"), Path(\"/content\")\nelse:\n    ENV = \"local\"\n    PROJECT_ROOT = Path.cwd()\n    if not (PROJECT_ROOT / \"src\").exists(): PROJECT_ROOT = PROJECT_ROOT.parent\n    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n\nsys.path.insert(0, str(PROJECT_ROOT))\nos.chdir(PROJECT_ROOT)\nprint(f\"ENV: {ENV}, ROOT: {PROJECT_ROOT}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Clone repo\nREPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\nif ENV in [\"kaggle\", \"colab\"]:\n    !rm -rf temp_repo 2>/dev/null; git clone --depth 1 {REPO_URL} temp_repo 2>/dev/null\n    !cp -r temp_repo/* {PROJECT_ROOT}/ 2>/dev/null; rm -rf temp_repo\n    if (PROJECT_ROOT / \"requirements.txt\").exists():\n        !pip install -q -r {PROJECT_ROOT}/requirements.txt 2>/dev/null\nfor d in [\"data/raw\", \"data/processed\", \"artifacts\", \"models\", \"logs\", \"reports\"]:\n    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\nprint(\"Setup completato\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Copy CSV dataset\nif ENV in [\"kaggle\", \"colab\"]:\n    for p in DATA_INPUT.iterdir():\n        if \"pcap\" not in p.name.lower() and any(x in p.name.lower() for x in [\"cicids\", \"cic-ids\", \"ids2017\"]):\n            for csv in p.glob(\"**/*.csv\"):\n                dest = PROJECT_ROOT / \"data\" / \"raw\" / csv.name\n                if not dest.exists(): shutil.copy(csv, dest)\n            break\nprint(f\"CSV: {len(list((PROJECT_ROOT / 'data' / 'raw').glob('*.csv')))}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Preprocessing\nif MODE in ['all', 'training', 'evaluation', 'sniffing']:\n    from src.preprocessing import main as preproc_main, load_processed_data\n    if not (PROJECT_ROOT / \"data\" / \"processed\" / \"test.parquet\").exists():\n        sys.argv = ['p.py', '--balance-ratio', str(BALANCE_RATIO), '--n-jobs', '4']\n        preproc_main()\n    train, val, test, mappings = load_processed_data()\n    print(f\"Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Feature Engineering\nif MODE in ['all', 'training', 'evaluation', 'sniffing']:\n    from src.feature_engineering import main as fe_main, load_artifacts\n    if not (PROJECT_ROOT / \"artifacts\" / \"scaler.pkl\").exists():\n        sys.argv = ['fe.py', '--n-features', str(N_FEATURES), '--n-jobs', '4']\n        fe_main()\n    scaler, selected_features, _, scaler_columns = load_artifacts()\n    print(f\"Features: {len(selected_features)}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Multi-Training setup\nTRAINED_THIS_SESSION = []\nSKIPPED = []\n\nif MODE in ['all', 'training']:\n    from src.model_versioning import generate_version_id, get_version_dir\n    print(f\"Training {len(TRAINING_CONFIGS)} configs...\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Multi-Training execution\nif MODE in ['all', 'training']:\n    for i, cfg in enumerate(TRAINING_CONFIGS, 1):\n        mt, ni, cv, gpu = cfg['model'], cfg['n_iter'], cfg['cv'], cfg.get('gpu', False)\n        extra = {'gpu': gpu} if gpu else None\n        vid = generate_version_id(ni, cv, extra)\n        vdir = get_version_dir(mt, ni, cv, extra, create=False)\n        \n        if (vdir / f\"model_{TASK}.pkl\").exists():\n            print(f\"[{i}/{len(TRAINING_CONFIGS)}] {mt}/{vid} exists, skip\")\n            TRAINED_THIS_SESSION.append((mt, vid))\n            continue\n        \n        is_timeout, remaining = check_timeout(60)\n        if is_timeout:\n            print(f\"TIMEOUT - skip remaining\"); SKIPPED = TRAINING_CONFIGS[i-1:]; break\n        \n        print(f\"\\n[{i}/{len(TRAINING_CONFIGS)}] Training {mt}/{vid} (remaining: {format_td(remaining)})\")\n        try:\n            if mt == 'xgboost':\n                from src.training.xgboost_model import main as train_fn\n                args = ['x.py', '--task', TASK, '--n-iter', str(ni), '--cv', str(cv)]\n                args.append('--gpu') if gpu else args.extend(['--n-jobs', '4'])\n            elif mt == 'lightgbm':\n                from src.training.lightgbm_model import main as train_fn\n                args = ['l.py', '--task', TASK, '--n-iter', str(ni), '--cv', str(cv), '--n-jobs', '4']\n            elif mt == 'random_forest':\n                from src.training.random_forest import main as train_fn\n                args = ['r.py', '--task', TASK, '--n-iter', str(ni), '--cv', str(cv), '--n-jobs', '4']\n            sys.argv = args\n            train_fn()\n            TRAINED_THIS_SESSION.append((mt, vid))\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n    print(f\"\\nTrained: {len(TRAINED_THIS_SESSION)}, Skipped: {len(SKIPPED)}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Show versions\nfrom src.model_versioning import print_versions_summary\nprint_versions_summary(task=TASK)", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Evaluation\nif MODE in ['all', 'evaluation', 'sniffing']:\n    from src.evaluation import main as eval_main\n    from src.model_versioning import list_model_versions\n    versions = list_model_versions(task=TASK)\n    print(f\"Evaluating {len(versions)} versions...\")\n    for v in versions:\n        print(f\"  Eval: {v['model_type']}/{v['version_id']}\")\n        sys.argv = ['e.py', '--model-path', str(v['model_path']), '--task', TASK]\n        try: eval_main()\n        except Exception as e: print(f\"    Error: {e}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Compare all versions\nif MODE in ['all', 'evaluation', 'sniffing']:\n    from src.compare_models import main as compare_main\n    print(f\"Compare: FPR<={MAX_FPR*100}%, Latency<={MAX_LATENCY_MS}ms\")\n    sys.argv = ['c.py', '--max-fpr', str(MAX_FPR), '--max-latency-ms', str(MAX_LATENCY_MS)]\n    compare_main()", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Show results\nfrom IPython.display import Image, display\nbest_dir = PROJECT_ROOT / \"models\" / \"best_model\"\nif (best_dir / \"metadata.json\").exists():\n    with open(best_dir / \"metadata.json\") as f:\n        meta = json.load(f)\n    print(f\"BEST: {meta.get('best_model')} (score={meta.get('score', 0):.4f})\")\nfor img in ['plateau_analysis.png', 'scorecard_comparison.png', 'algorithm_rankings.png']:\n    if (best_dir / img).exists(): display(Image(filename=str(best_dir / img)))", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Find PCAP\npcap_files = []\nif MODE in ['all', 'sniffing'] and ENV in [\"kaggle\", \"colab\"]:\n    for p in DATA_INPUT.iterdir():\n        if any(x in p.name.lower() for x in [\"pcap\", \"cic-ids\"]):\n            pcap_files = sorted(p.glob(\"**/*.pcap\"))[:3]\n            if pcap_files: print(f\"PCAP: {[f.name for f in pcap_files]}\"); break\n    if not pcap_files: print(\"No PCAP found\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "%%time\n# Sniffing\nall_sniff_results = {}\nif MODE in ['all', 'sniffing'] and pcap_files:\n    from src.sniffer import analyze_pcap_file\n    best_model = PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\"\n    if best_model.exists():\n        for pcap in pcap_files:\n            print(f\"\\nTesting: {pcap.name}\")\n            try:\n                r = analyze_pcap_file(\n                    pcap_path=str(pcap),\n                    model_path=str(best_model),\n                    threshold=SNIFF_THRESHOLD,\n                    timeout=SNIFF_TIMEOUT,\n                    min_packets=SNIFF_MIN_PACKETS,\n                    verbose=SNIFF_VERBOSE,\n                    progress_interval=500000,\n                    show_progress=True\n                )\n                all_sniff_results[pcap.name] = r\n                print(f\"  Flows: {r['flows_analyzed']:,}, Attacks: {r['attacks_detected']:,} ({r.get('detection_rate',0):.1f}%)\")\n            except Exception as e:\n                print(f\"  Error: {e}\")\n    else:\n        print(\"best_model not found\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Download ZIP\nimport zipfile\nif ENV in [\"kaggle\", \"colab\"]:\n    zip_path = PROJECT_ROOT / \"nids_output.zip\"\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n        for d in [\"artifacts\", \"models\", \"reports\"]:\n            for f in (PROJECT_ROOT / d).rglob(\"*\"):\n                if f.is_file(): z.write(f, f\"{d}/{f.relative_to(PROJECT_ROOT / d)}\")\n    print(f\"ZIP: {zip_path.name} ({zip_path.stat().st_size/(1024**2):.1f} MB)\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null},
  {"cell_type": "code", "source": "# Summary\nelapsed = datetime.now() - NOTEBOOK_START\nprint(f\"\\n{'='*60}\\nCOMPLETED in {format_td(elapsed)}\")\nprint(f\"Trained: {[f'{m}/{v}' for m,v in TRAINED_THIS_SESSION]}\")\nif (PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\").exists():\n    with open(PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\") as f:\n        print(f\"Best: {json.load(f).get('best_model')}\")", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}
 ]
}
