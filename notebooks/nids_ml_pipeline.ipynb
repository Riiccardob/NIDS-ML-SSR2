{
 "metadata": {
  "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# NIDS-ML: Pipeline Completa\n\n## Modalità Disponibili\n\n| MODE | Cosa fa | Quando usarla |\n|------|---------|---------------|\n| `'all'` | Pull + Preproc + Training + Eval + Compare + Sniff | Training completo da zero |\n| `'training'` | Pull + Preproc + Training | Solo training, poi push manuale |\n| `'evaluation'` | Pull + Eval + Compare | Modelli già su GitHub |\n| `'sniffing'` | Pull + Test PCAP con confronto | Testare/confrontare modelli |\n\n## Parametri Importanti\n\n- `MODELS_TO_TRAIN`: Lista modelli da trainare (es. `['xgboost']` o `['xgboost', 'lightgbm']`)\n- `MODELS_TO_EVALUATE`: Lista modelli da valutare (default: tutti quelli disponibili)\n- `MODELS_TO_SNIFF`: Lista modelli per test PCAP con confronto",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# CONFIGURAZIONE PRINCIPALE\n# ============================================================================\n\n# MODALITA: 'all', 'training', 'evaluation', 'sniffing'\nMODE = 'all'\n\n# Task\nTASK = 'binary'\n\n# ---- MODELLI ----\n# Quali modelli trainare (solo per MODE='all' o 'training')\n# Opzioni: 'random_forest', 'xgboost', 'lightgbm'\nMODELS_TO_TRAIN = ['xgboost', 'lightgbm']  # RF e' lento, skip\n\n# Quali modelli valutare (per evaluation e compare)\n# None = tutti quelli disponibili\nMODELS_TO_EVALUATE = None  # o es: ['xgboost', 'lightgbm']\n\n# Quali modelli testare su PCAP (per confronto)\n# None = solo best_model, lista = confronta tutti\nMODELS_TO_SNIFF = ['xgboost', 'lightgbm']  # Confronta questi due\n\n# ---- PARAMETRI TRAINING ----\nTRAINING_PARAMS = {\n    'random_forest': {'n_iter': 30, 'cv': 3},   # RF lento\n    'xgboost': {'n_iter': 50, 'cv': 5, 'gpu': True},\n    'lightgbm': {'n_iter': 50, 'cv': 5}\n}\n\n# ---- PARAMETRI GENERALI ----\nN_FEATURES = 30\nBALANCE_RATIO = 2.0\n\n# ---- PARAMETRI COMPARE ----\nMAX_FPR = 0.02\nMAX_LATENCY_MS = 2.0\n\n# ---- PARAMETRI SNIFFING ----\nSNIFF_THRESHOLD = 0.5\nSNIFF_MIN_PACKETS = 2\nSNIFF_TIMEOUT = 60\n\nprint(f\"=\"*60)\nprint(f\"CONFIGURAZIONE\")\nprint(f\"=\"*60)\nprint(f\"MODE:              {MODE}\")\nprint(f\"Task:              {TASK}\")\nif MODE in ['all', 'training']:\n    print(f\"Modelli training:  {MODELS_TO_TRAIN}\")\nif MODE in ['all', 'evaluation']:\n    print(f\"Modelli eval:      {MODELS_TO_EVALUATE or 'tutti disponibili'}\")\nif MODE in ['all', 'sniffing']:\n    print(f\"Modelli sniff:     {MODELS_TO_SNIFF or 'solo best_model'}\")\nprint(f\"Compare:           FPR<={MAX_FPR*100}%, Latency<={MAX_LATENCY_MS}ms\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 1. Setup Ambiente",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nif Path(\"/kaggle/input\").exists():\n    ENV = \"kaggle\"\n    PROJECT_ROOT = Path(\"/kaggle/working\")\n    DATA_INPUT = Path(\"/kaggle/input\")\nelif Path(\"/content\").exists():\n    ENV = \"colab\"\n    PROJECT_ROOT = Path(\"/content/NIDS-ML\")\n    DATA_INPUT = Path(\"/content\")\nelse:\n    ENV = \"local\"\n    PROJECT_ROOT = Path.cwd()\n    if not (PROJECT_ROOT / \"src\").exists():\n        PROJECT_ROOT = PROJECT_ROOT.parent\n    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n\nprint(f\"Ambiente: {ENV}\")\nprint(f\"Project root: {PROJECT_ROOT}\")\n\nsys.path.insert(0, str(PROJECT_ROOT))\nos.chdir(PROJECT_ROOT)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Clone/Pull da GitHub\nREPO_URL = \"https://github.com/Riiccardob/NIDS-ML-SSR2\"\n\nif ENV in [\"kaggle\", \"colab\"]:\n    print(f\"Cloning {REPO_URL}...\")\n    !rm -rf temp_repo 2>/dev/null\n    !git clone --depth 1 {REPO_URL} temp_repo 2>/dev/null\n    !cp -r temp_repo/* {PROJECT_ROOT}/ 2>/dev/null\n    !rm -rf temp_repo\n    print(\"Done.\")\n    \n    if (PROJECT_ROOT / \"requirements.txt\").exists():\n        !pip install -q -r {PROJECT_ROOT}/requirements.txt 2>/dev/null\n\n# Crea directory\nfor d in [\"data/raw\", \"data/processed\", \"artifacts\", \"models\", \"logs\", \"reports\"]:\n    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n\n# Mostra modelli presenti da GitHub\nprint(f\"\\nModelli da GitHub:\")\nfor name in ['random_forest', 'xgboost', 'lightgbm', 'best_model']:\n    model_dir = PROJECT_ROOT / \"models\" / name\n    if model_dir.exists() and list(model_dir.glob(\"*.pkl\")):\n        print(f\"  {name}: OK\")\n    else:\n        print(f\"  {name}: -\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Copia dataset CSV\nif ENV in [\"kaggle\", \"colab\"]:\n    patterns = [\"cicids2017\", \"cic-ids\", \"ids2017\", \"network-intrusion\"]\n    for p in DATA_INPUT.iterdir():\n        if \"pcap\" in p.name.lower():\n            continue\n        if any(pat in p.name.lower() for pat in patterns) and list(p.glob(\"**/*.csv\")):\n            print(f\"Dataset CSV: {p.name}\")\n            for csv in p.glob(\"**/*.csv\"):\n                dest = PROJECT_ROOT / \"data\" / \"raw\" / csv.name\n                if not dest.exists():\n                    !cp \"{csv}\" \"{dest}\"\n            break\n\nprint(f\"CSV disponibili: {len(list((PROJECT_ROOT / 'data' / 'raw').glob('*.csv')))}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 2. Preprocessing & Feature Engineering\n\n*Eseguito per MODE: all, training, evaluation*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\nif MODE in ['all', 'training', 'evaluation']:\n    from src.preprocessing import main as preprocessing_main, load_processed_data\n    \n    processed_test = PROJECT_ROOT / \"data\" / \"processed\" / \"test.parquet\"\n    if not processed_test.exists():\n        print(\"Preprocessing...\")\n        sys.argv = ['preprocessing.py', '--balance-ratio', str(BALANCE_RATIO), '--n-jobs', '4']\n        preprocessing_main()\n    else:\n        print(\"Dati gia' processati.\")\n    \n    train, val, test, mappings = load_processed_data()\n    print(f\"Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}\")\nelse:\n    print(f\"Skip (MODE={MODE})\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\nif MODE in ['all', 'training', 'evaluation']:\n    from src.feature_engineering import main as fe_main, load_artifacts\n    \n    if not (PROJECT_ROOT / \"artifacts\" / \"scaler.pkl\").exists():\n        print(\"Feature engineering...\")\n        sys.argv = ['feature_engineering.py', '--n-features', str(N_FEATURES), '--n-jobs', '4']\n        fe_main()\n    else:\n        print(\"Artifacts gia' presenti.\")\n    \n    scaler, selected_features, _, _ = load_artifacts()\n    print(f\"Feature: {len(selected_features)}\")\nelse:\n    print(f\"Skip (MODE={MODE})\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 3. Training\n\n*Eseguito per MODE: all, training*\n\nI modelli trainati vengono salvati in `/kaggle/working/models/` (NON su GitHub).\nPer usarli successivamente, scarica lo ZIP e pusha su GitHub.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Traccia quali modelli sono stati trainati in QUESTA sessione\n# Serve per sapere se usare modelli locali o da GitHub\nTRAINED_THIS_SESSION = []\n\nif MODE in ['all', 'training']:\n    print(f\"Modelli da trainare: {MODELS_TO_TRAIN}\")\nelse:\n    print(f\"Skip training (MODE={MODE})\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n# RANDOM FOREST\nif MODE in ['all', 'training'] and 'random_forest' in MODELS_TO_TRAIN:\n    from src.training.random_forest import main as rf_main\n    \n    params = TRAINING_PARAMS.get('random_forest', {})\n    n_iter = params.get('n_iter', 30)\n    cv = params.get('cv', 3)\n    \n    print(f\"Training Random Forest: n_iter={n_iter}, cv={cv}\")\n    sys.argv = ['rf.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--n-jobs', '4']\n    rf_main()\n    TRAINED_THIS_SESSION.append('random_forest')\nelse:\n    print(\"Skip Random Forest\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n# XGBOOST\nif MODE in ['all', 'training'] and 'xgboost' in MODELS_TO_TRAIN:\n    from src.training.xgboost_model import main as xgb_main\n    \n    params = TRAINING_PARAMS.get('xgboost', {})\n    n_iter = params.get('n_iter', 50)\n    cv = params.get('cv', 5)\n    use_gpu = params.get('gpu', True)\n    \n    print(f\"Training XGBoost: n_iter={n_iter}, cv={cv}, GPU={use_gpu}\")\n    if use_gpu:\n        sys.argv = ['xgb.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--gpu']\n    else:\n        sys.argv = ['xgb.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--n-jobs', '4']\n    xgb_main()\n    TRAINED_THIS_SESSION.append('xgboost')\nelse:\n    print(\"Skip XGBoost\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n# LIGHTGBM\nif MODE in ['all', 'training'] and 'lightgbm' in MODELS_TO_TRAIN:\n    from src.training.lightgbm_model import main as lgbm_main\n    \n    params = TRAINING_PARAMS.get('lightgbm', {})\n    n_iter = params.get('n_iter', 50)\n    cv = params.get('cv', 5)\n    \n    print(f\"Training LightGBM: n_iter={n_iter}, cv={cv}\")\n    sys.argv = ['lgbm.py', '--task', TASK, '--n-iter', str(n_iter), '--cv', str(cv), '--n-jobs', '4']\n    lgbm_main()\n    TRAINED_THIS_SESSION.append('lightgbm')\nelse:\n    print(\"Skip LightGBM\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Riepilogo modelli disponibili\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODELLI DISPONIBILI\")\nprint(\"=\"*60)\nprint(f\"{'Modello':<20} {'Origine':<15} {'F1':>10} {'Accuracy':>10}\")\nprint(\"-\"*60)\n\nfor name in ['random_forest', 'xgboost', 'lightgbm']:\n    model_path = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n    results_path = PROJECT_ROOT / \"models\" / name / f\"results_{TASK}.json\"\n    \n    if model_path.exists():\n        origine = \"TRAINATO ORA\" if name in TRAINED_THIS_SESSION else \"GitHub\"\n        f1, acc = \"-\", \"-\"\n        if results_path.exists():\n            with open(results_path) as f:\n                r = json.load(f)\n            m = r.get('validation_metrics', {})\n            f1 = f\"{m.get('f1', 0):.4f}\"\n            acc = f\"{m.get('accuracy', 0):.4f}\"\n        print(f\"{name:<20} {origine:<15} {f1:>10} {acc:>10}\")\n    else:\n        print(f\"{name:<20} {'MANCANTE':<15}\")\n\nprint(f\"\\nTrainati in questa sessione: {TRAINED_THIS_SESSION or 'nessuno'}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 4. Evaluation\n\n*Eseguito per MODE: all, evaluation*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if MODE in ['all', 'evaluation']:\n    from src.evaluation import main as evaluation_main\n    \n    # Determina quali modelli valutare\n    if MODELS_TO_EVALUATE:\n        models_eval = MODELS_TO_EVALUATE\n    else:\n        # Tutti quelli disponibili\n        models_eval = []\n        for name in ['random_forest', 'xgboost', 'lightgbm']:\n            if (PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\").exists():\n                models_eval.append(name)\n    \n    print(f\"Modelli da valutare: {models_eval}\")\nelse:\n    print(f\"Skip evaluation (MODE={MODE})\")\n    models_eval = []",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n# Esegui evaluation\nif MODE in ['all', 'evaluation'] and models_eval:\n    for name in models_eval:\n        model_path = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"EVALUATION: {name}\")\n        origine = \"TRAINATO ORA\" if name in TRAINED_THIS_SESSION else \"GitHub\"\n        print(f\"Origine modello: {origine}\")\n        print(f\"{'='*60}\")\n        \n        sys.argv = ['eval.py', '--model-path', str(model_path), '--task', TASK]\n        try:\n            evaluation_main()\n        except Exception as e:\n            print(f\"ERRORE: {e}\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5. Compare Models\n\n*Eseguito per MODE: all, evaluation*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\nif MODE in ['all', 'evaluation']:\n    from src.compare_models import main as compare_main\n    \n    print(f\"Compare: FPR <= {MAX_FPR*100}%, Latency <= {MAX_LATENCY_MS}ms\")\n    sys.argv = ['compare.py', '--max-fpr', str(MAX_FPR), '--max-latency-ms', str(MAX_LATENCY_MS)]\n    compare_main()\nelse:\n    print(f\"Skip compare (MODE={MODE})\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostra best model\nbest_model_dir = PROJECT_ROOT / \"models\" / \"best_model\"\nif best_model_dir.exists() and (best_model_dir / \"metadata.json\").exists():\n    with open(best_model_dir / \"metadata.json\") as f:\n        meta = json.load(f)\n    print(f\"\\nBEST MODEL: {meta.get('best_model', 'N/A').upper()}\")\n    print(f\"Score: {meta.get('score', 0):.4f}\")\n    \n    # Scorecard\n    from IPython.display import Image, display\n    scorecard = best_model_dir / \"scorecard_comparison.png\"\n    if scorecard.exists():\n        display(Image(filename=str(scorecard)))",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 6. Test Sniffer su PCAP con CONFRONTO\n\n*Eseguito per MODE: all, sniffing*\n\nQuesta sezione permette di **confrontare** le performance di diversi modelli sugli stessi PCAP.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cerca PCAP\npcap_files = []\n\nif MODE in ['all', 'sniffing'] and ENV in [\"kaggle\", \"colab\"]:\n    pcap_patterns = [\"pcap\", \"cic-ids-2017-pcap\", \"cicids\"]\n    \n    for p in DATA_INPUT.iterdir():\n        name_lower = p.name.lower().replace(\"_\", \"-\")\n        if any(pat in name_lower for pat in pcap_patterns):\n            found = list(p.glob(\"**/*.pcap\")) + list(p.glob(\"**/*.pcapng\"))\n            if found:\n                pcap_files = sorted(found, key=lambda x: x.name)\n                print(f\"Dataset PCAP: {p.name}\")\n                break\n    \n    if pcap_files:\n        print(f\"\\nPCAP disponibili ({len(pcap_files)}):\")\n        for f in pcap_files:\n            print(f\"  - {f.name}: {f.stat().st_size/(1024**2):.1f} MB\")\n    else:\n        print(\"Dataset PCAP non trovato.\")\nelse:\n    print(f\"Skip ricerca PCAP (MODE={MODE} o ambiente locale)\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Determina quali modelli testare\nmodels_to_sniff = []\n\nif MODE in ['all', 'sniffing'] and pcap_files:\n    if MODELS_TO_SNIFF:\n        # Lista specifica\n        for name in MODELS_TO_SNIFF:\n            model_path = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n            if model_path.exists():\n                models_to_sniff.append((name, model_path))\n            else:\n                print(f\"WARNING: {name} non trovato, skip\")\n    else:\n        # Solo best_model\n        best_path = PROJECT_ROOT / \"models\" / \"best_model\" / f\"model_{TASK}.pkl\"\n        if best_path.exists():\n            models_to_sniff.append(('best_model', best_path))\n        else:\n            # Fallback: primo disponibile\n            for name in ['lightgbm', 'xgboost', 'random_forest']:\n                p = PROJECT_ROOT / \"models\" / name / f\"model_{TASK}.pkl\"\n                if p.exists():\n                    models_to_sniff.append((name, p))\n                    break\n    \n    print(f\"\\nModelli per test PCAP:\")\n    for name, path in models_to_sniff:\n        origine = \"TRAINATO ORA\" if name in TRAINED_THIS_SESSION else \"GitHub\"\n        print(f\"  - {name} ({origine})\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n# Test PCAP con CONFRONTO tra modelli\nall_sniff_results = {}  # {model_name: [results per pcap]}\n\nif MODE in ['all', 'sniffing'] and pcap_files and models_to_sniff:\n    from src.sniffer import analyze_pcap_file\n    \n    for model_name, model_path in models_to_sniff:\n        print(f\"\\n{'#'*70}\")\n        print(f\"# MODELLO: {model_name.upper()}\")\n        origine = \"TRAINATO ORA\" if model_name in TRAINED_THIS_SESSION else \"GitHub\"\n        print(f\"# Origine: {origine}\")\n        print(f\"{'#'*70}\")\n        \n        model_results = []\n        \n        for pcap_path in pcap_files:\n            print(f\"\\n  Testing: {pcap_path.name}...\")\n            \n            try:\n                result = analyze_pcap_file(\n                    pcap_path=str(pcap_path),\n                    model_path=str(model_path),\n                    threshold=SNIFF_THRESHOLD,\n                    timeout=SNIFF_TIMEOUT,\n                    min_packets=SNIFF_MIN_PACKETS,\n                    progress_interval=100000\n                )\n                result['model'] = model_name\n                result['model_origin'] = origine\n                model_results.append(result)\n                \n                rate = result.get('detection_rate', 0)\n                print(f\"  -> Flows: {result['flows_analyzed']:,}, Attacks: {result['attacks_detected']:,} ({rate:.1f}%)\")\n                \n            except Exception as e:\n                print(f\"  ERRORE: {e}\")\n        \n        all_sniff_results[model_name] = model_results\nelse:\n    print(\"Test PCAP non eseguito\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# TABELLA CONFRONTO MODELLI SU PCAP\nif all_sniff_results:\n    print(\"\\n\" + \"=\"*90)\n    print(\"CONFRONTO PERFORMANCE SU PCAP\")\n    print(\"=\"*90)\n    \n    # Raccogli tutti i PCAP\n    all_pcaps = set()\n    for results in all_sniff_results.values():\n        for r in results:\n            all_pcaps.add(r['pcap'])\n    all_pcaps = sorted(all_pcaps)\n    \n    # Header\n    models = list(all_sniff_results.keys())\n    header = f\"{'PCAP':<35}\"\n    for m in models:\n        header += f\" | {m:^20}\"\n    print(header)\n    print(\"-\" * len(header))\n    \n    # Per ogni PCAP\n    totals = {m: {'flows': 0, 'attacks': 0} for m in models}\n    \n    for pcap_name in all_pcaps:\n        row = f\"{pcap_name[:35]:<35}\"\n        for m in models:\n            # Trova risultato per questo modello e pcap\n            result = None\n            for r in all_sniff_results.get(m, []):\n                if r['pcap'] == pcap_name:\n                    result = r\n                    break\n            \n            if result:\n                attacks = result['attacks_detected']\n                flows = result['flows_analyzed']\n                rate = result.get('detection_rate', 0)\n                row += f\" | {attacks:>6} / {flows:<6} ({rate:>5.1f}%)\"\n                totals[m]['flows'] += flows\n                totals[m]['attacks'] += attacks\n            else:\n                row += f\" | {'N/A':^20}\"\n        print(row)\n    \n    # Totali\n    print(\"-\" * len(header))\n    row = f\"{'TOTALE':<35}\"\n    for m in models:\n        t = totals[m]\n        if t['flows'] > 0:\n            rate = t['attacks'] / t['flows'] * 100\n            row += f\" | {t['attacks']:>6} / {t['flows']:<6} ({rate:>5.1f}%)\"\n        else:\n            row += f\" | {'N/A':^20}\"\n    print(row)\n    \n    # Statistiche\n    print(\"\\n\" + \"=\"*90)\n    print(\"STATISTICHE CONFRONTO\")\n    print(\"=\"*90)\n    print(f\"\\n{'Modello':<20} {'Origine':<15} {'Tot Flows':>12} {'Tot Attacks':>12} {'Rate':>10}\")\n    print(\"-\"*70)\n    \n    best_rate = 0\n    best_model = None\n    \n    for m in models:\n        t = totals[m]\n        origine = \"TRAINATO ORA\" if m in TRAINED_THIS_SESSION else \"GitHub\"\n        if t['flows'] > 0:\n            rate = t['attacks'] / t['flows'] * 100\n            print(f\"{m:<20} {origine:<15} {t['flows']:>12,} {t['attacks']:>12,} {rate:>9.2f}%\")\n            if rate > best_rate:\n                best_rate = rate\n                best_model = m\n        else:\n            print(f\"{m:<20} {origine:<15} {'N/A':>12}\")\n    \n    if best_model and len(models) > 1:\n        print(f\"\\nMigliore su PCAP: {best_model.upper()} ({best_rate:.2f}% detection rate)\")\n    \n    # Salva risultati\n    comparison_data = {\n        'timestamp': datetime.now().isoformat(),\n        'models': models,\n        'parameters': {\n            'threshold': SNIFF_THRESHOLD,\n            'min_packets': SNIFF_MIN_PACKETS,\n            'timeout': SNIFF_TIMEOUT\n        },\n        'results': all_sniff_results,\n        'totals': totals\n    }\n    \n    with open(PROJECT_ROOT / \"reports\" / \"pcap_comparison.json\", 'w') as f:\n        json.dump(comparison_data, f, indent=2, default=str)\n    print(f\"\\nRisultati salvati in: reports/pcap_comparison.json\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7. Download Output",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import zipfile\n\nif ENV in [\"kaggle\", \"colab\"]:\n    zip_path = PROJECT_ROOT / \"nids_ml_output.zip\"\n    \n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n        # Artifacts\n        for f in (PROJECT_ROOT / \"artifacts\").glob(\"*\"):\n            z.write(f, f\"artifacts/{f.name}\")\n        \n        # Models (tutti)\n        for model_dir in (PROJECT_ROOT / \"models\").iterdir():\n            if model_dir.is_dir():\n                for f in model_dir.glob(\"*\"):\n                    z.write(f, f\"models/{model_dir.name}/{f.name}\")\n        \n        # Reports\n        for f in (PROJECT_ROOT / \"reports\").rglob(\"*\"):\n            if f.is_file():\n                z.write(f, f\"reports/{f.relative_to(PROJECT_ROOT / 'reports')}\")\n    \n    print(f\"ZIP: {zip_path.name} ({zip_path.stat().st_size/(1024**2):.1f} MB)\")\nelse:\n    print(\"Locale - output nelle cartelle del progetto\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8. Riepilogo",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"RIEPILOGO ESECUZIONE\")\nprint(\"=\"*70)\n\nprint(f\"\\nModalita': {MODE}\")\nprint(f\"Trainati in questa sessione: {TRAINED_THIS_SESSION or 'nessuno'}\")\n\n# Best model\nif (PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\").exists():\n    with open(PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\") as f:\n        meta = json.load(f)\n    print(f\"\\nBest Model (da compare): {meta.get('best_model', 'N/A').upper()}\")\n\n# PCAP comparison\nif all_sniff_results:\n    print(f\"\\nTest PCAP eseguito su {len(models_to_sniff)} modelli\")\n    for m in all_sniff_results:\n        t = totals.get(m, {})\n        if t.get('flows', 0) > 0:\n            rate = t['attacks'] / t['flows'] * 100\n            print(f\"  {m}: {t['attacks']:,} attacchi su {t['flows']:,} flussi ({rate:.1f}%)\")\n\nprint(\"\\n\" + \"=\"*70)\nif MODE in ['all', 'training'] and TRAINED_THIS_SESSION:\n    print(\"PROSSIMI STEP:\")\n    print(\"1. Scarica lo ZIP\")\n    print(\"2. Estrai models/ e artifacts/\")\n    print(\"3. git add models/ artifacts/ && git commit && git push\")\nelse:\n    print(\"Per usare il modello in locale:\")\n    print(\"  sudo python src/sniffer.py --interface eth0 --verbose\")\nprint(\"=\"*70)",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  }
 ]
}
