# ðŸ“˜ Guida Notebook NIDS-ML Pipeline

## ðŸŽ¯ Overview

Il notebook `nids_pipeline_complete.ipynb` esegue l'intera pipeline di preparazione dati per NIDS-ML, dalla raccolta del dataset raw fino alla generazione dei dataset pronti per il training.

### ðŸ“‹ Cosa fa il notebook:

1. âœ… **Environment Detection** - Rileva automaticamente Kaggle vs Locale
2. âœ… **Repository Setup** - Scarica solo `src/` da GitHub (su Kaggle)
3. âœ… **Dataset Import** - Copia dataset da Kaggle Input o verifica locale
4. âœ… **Preprocessing** - Esegue `preprocessing.py`
5. âœ… **Feature Engineering** - Esegue `feature_engineering.py` con Statistical + RobustScaler
6. âœ… **Validation** - Verifica artifacts e mostra summary
7. âœ… **Export** - Crea ZIP con artifacts (solo Kaggle)

---

## ðŸš€ Quick Start

### Su Kaggle:

1. **Crea nuovo Notebook** su Kaggle
2. **Aggiungi Dataset**: "Network Intrusion Dataset" (il tuo dataset pubblico)
3. **Import**: Copia il contenuto di `nids_pipeline_complete.ipynb`
4. **Configura** (prima cella):
   ```python
   CLEAN_RUN = True
   USE_STATISTICAL = True
   USE_ROBUST = True
   N_FEATURES = 30
   RF_ESTIMATORS = 100
   ```
5. **Run All**: Esegui tutte le celle

### In Locale:

1. **Posizionati** nella root del repository
2. **Verifica** che `data/raw/*.csv` contenga i dataset
3. **Apri** il notebook con Jupyter:
   ```bash
   jupyter notebook nids_pipeline_complete.ipynb
   ```
4. **Configura** e **Run All**

---

## âš™ï¸ Configurazione

### Parametri Principali (Cella 1):

```python
# Clean Run
CLEAN_RUN = True  # True = riparte da zero, False = riusa esistenti

# Repository (solo Kaggle)
REPO_URL = "https://github.com/riiccardob/nids-ml-ssr2"
BRANCH = "main"

# Dataset Paths
KAGGLE_DATASET_PATH = "/kaggle/input/network-intrusion-dataset/"
LOCAL_DATASET_PATH = "data/raw"

# Feature Engineering
USE_STATISTICAL = True   # Statistical preprocessing (CONSIGLIATO)
USE_ROBUST = True        # RobustScaler (CONSIGLIATO)
N_FEATURES = 30          # Feature da selezionare
RF_ESTIMATORS = 100      # Alberi Random Forest
```

### Opzioni Avanzate:

#### CLEAN_RUN

| Valore | Comportamento |
|--------|---------------|
| `True` | Cancella `data/processed` e `artifacts`, riparte da zero |
| `False` | Riusa file esistenti se presenti (utile per debugging) |

**Quando usare `True`:**
- Prima esecuzione
- Cambio configurazione feature engineering
- Problemi con artifacts corrotti

**Quando usare `False`:**
- Re-run dopo errore in fase successiva
- Test su step specifico
- Sviluppo/debugging

#### Feature Engineering Config

| Parametro | Range | Default | Descrizione |
|-----------|-------|---------|-------------|
| `USE_STATISTICAL` | bool | `True` | Statistical preprocessing (variance + correlation) |
| `USE_ROBUST` | bool | `True` | RobustScaler (migliore per outlier) |
| `N_FEATURES` | 5-100 | 30 | Numero feature da selezionare |
| `RF_ESTIMATORS` | 10-1000 | 100 | Alberi Random Forest per importance |

**Configurazione CONSIGLIATA:**
```python
USE_STATISTICAL = True
USE_ROBUST = True
N_FEATURES = 30
RF_ESTIMATORS = 100
```

---

## ðŸ“‚ Struttura Output

Dopo l'esecuzione completa:

```
/kaggle/working/  (o directory locale)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ cicids2017_*.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train.parquet
â”‚       â”œâ”€â”€ val.parquet
â”‚       â”œâ”€â”€ test.parquet
â”‚       â”œâ”€â”€ train_ready.parquet  â† Dataset pronti per training
â”‚       â”œâ”€â”€ val_ready.parquet
â”‚       â””â”€â”€ test_ready.parquet
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”œâ”€â”€ selected_features.json
â”‚   â”œâ”€â”€ feature_importances.json
â”‚   â”œâ”€â”€ scaler_columns.json
â”‚   â”œâ”€â”€ column_checksum.json
â”‚   â””â”€â”€ statistical_preprocessing_info.json  â† Se USE_STATISTICAL=True
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ timing/
â”‚       â”œâ”€â”€ preprocessing_*.json
â”‚       â””â”€â”€ feature_engineering_v2_*.json
â”œâ”€â”€ src/  (solo Kaggle, scaricato da GitHub)
â””â”€â”€ pipeline_artifacts.zip  (solo Kaggle, per download)
```

---

## ðŸ” Validazione Output

Il notebook verifica automaticamente:

### 1. Dataset Ready
```
TRAIN - Shape:     706,632 samples x 31 features  (30 + target)
VAL   - Shape:     151,422 samples x 31 features
TEST  - Shape:     151,422 samples x 31 features
```

### 2. Feature Selection
```
Selezionate: 30 features
```

### 3. Top-10 Feature Importances
```
  1. Bwd Packet Length Std              0.107700
  2. Bwd Packet Length Max              0.075900
  3. Packet Length Variance             0.061300
  ...
```

### 4. Scaler Info
```
Tipo: RobustScaler
Statistical Preprocessing: ATTIVO
  - Feature ridotte: 15.2%
  - Varianza rimossa: 3
  - Correlazione rimossa: 9
```

---

## ðŸ› Troubleshooting

### Problema: "Dataset non trovato"

**Kaggle:**
```python
# Verifica path in configurazione
KAGGLE_DATASET_PATH = "/kaggle/input/network-intrusion-dataset/"

# Controlla che il dataset sia aggiunto al notebook
# Kaggle UI: Add Data > Search "network intrusion"
```

**Locale:**
```bash
# Verifica presenza CSV
ls data/raw/*.csv

# Se mancano, scarica o copia i file CSV
```

### Problema: "src/ non trovata"

**Kaggle:**
```python
# Verifica URL e branch
REPO_URL = "https://github.com/riiccardob/nids-ml-ssr2"
BRANCH = "main"

# Forza re-download cancellando src/
import shutil
shutil.rmtree("src")  # Poi ri-esegui cella setup
```

**Locale:**
```bash
# Assicurati di essere nella root del repo
pwd  # Deve mostrare .../nids-ml-ssr2

# Verifica presenza src/
ls src/
```

### Problema: "Memory Error"

**Kaggle:**
- Usa notebook con **30GB RAM** (Settings > Accelerator > None, ma aumenta RAM)
- Riduci `RF_ESTIMATORS` (es. 50 invece di 100)
- Assicurati di non avere altri notebook running

**Locale:**
- Chiudi applicazioni pesanti
- Riduci `RF_ESTIMATORS`
- Monitora RAM con `htop` o Task Manager

### Problema: "Preprocessing fallito"

**Verifica:**
```python
# Controlla log dettagliato
!cat logs/*.log  # Se esiste
```

**Soluzioni:**
- CSV corrotti? Ri-scarica dataset
- Encoding problemi? Verifica CSV con `head -n 10 data/raw/file.csv`
- Memoria insufficiente? Vedi sezione Memory Error

### Problema: "Feature Engineering lento"

**Normale per grandi dataset:**
- RF con 100 alberi su 700k samples richiede 2-3 minuti
- Statistical preprocessing Ã¨ veloce (<10s)

**Accelera:**
```python
RF_ESTIMATORS = 50  # Invece di 100
N_FEATURES = 20     # Invece di 30
```

---

## ðŸ“Š Metriche & Performance

### Tempi Attesi (Kaggle, 4 CPU, 30GB RAM):

| Step | Tempo | Dettagli |
|------|-------|----------|
| Setup Repo | ~30s | Download + unzip + pip install |
| Dataset Import | ~10s | Copy CSV (se non cached) |
| Preprocessing | ~60s | Caricamento + pulizia + split |
| Feature Engineering | ~150s | Statistical (10s) + Scaling (5s) + RF (135s) |
| **TOTALE** | **~4 min** | Prima esecuzione |

**Run successive (CLEAN_RUN=False):** ~10s (solo validation)

### Dimensioni Output:

| File | Dimensione | Comprimibile |
|------|------------|--------------|
| train.parquet | ~150 MB | No (giÃ  compresso) |
| val.parquet | ~35 MB | No |
| test.parquet | ~35 MB | No |
| train_ready.parquet | ~50 MB | No |
| artifacts/*.pkl | ~5 MB | SÃ¬ |
| **pipeline_artifacts.zip** | **~100 MB** | - |

---

## ðŸŽ“ Best Practices

### 1. Development Workflow

```python
# Prima iterazione: tutto pulito
CLEAN_RUN = True
# ... esegui pipeline completa

# Debugging: riusa preprocessing
CLEAN_RUN = False
# ... modifica solo feature engineering
```

### 2. Configurazione Ottimale

**Per massima qualitÃ :**
```python
USE_STATISTICAL = True
USE_ROBUST = True
N_FEATURES = 30
RF_ESTIMATORS = 200  # PiÃ¹ alberi = piÃ¹ tempo ma migliori feature
```

**Per velocitÃ  (testing):**
```python
USE_STATISTICAL = True
USE_ROBUST = True
N_FEATURES = 20
RF_ESTIMATORS = 50
```

**Per baseline (confronto):**
```python
USE_STATISTICAL = False
USE_ROBUST = False
N_FEATURES = 30
RF_ESTIMATORS = 100
```

### 3. Su Kaggle

- âœ… Usa **Session Persistence** (salva automaticamente tra run)
- âœ… Abilita **Internet** per download repository
- âœ… Monitora **RAM usage** (Kaggle limite 30GB)
- âœ… Scarica **pipeline_artifacts.zip** al termine (backup)

### 4. In Locale

- âœ… Usa **ambiente virtuale** Python
- âœ… Installa requirements: `pip install -r requirements.txt`
- âœ… Verifica **versioni librerie** (potrebbero differire da Kaggle)
- âœ… Backup periodico di `artifacts/`

---

## ðŸ”„ Integrazione con Training Notebooks

Dopo aver completato questo notebook, i prossimi step sono:

### 1. Random Forest Tuning
```python
# Nuovo notebook: nids_training_random_forest.ipynb
# - Carica artifacts/ e data/processed/
# - Hyperparameter tuning con Optuna
# - Salva best model in models/random_forest/
```

### 2. XGBoost Tuning
```python
# Nuovo notebook: nids_training_xgboost.ipynb
# - Stessa struttura
# - Tuning specifico per XGBoost
```

### 3. LightGBM Tuning
```python
# Nuovo notebook: nids_training_lightgbm.ipynb
# - Stessa struttura
# - Tuning specifico per LightGBM
```

**Vantaggi separazione:**
- âœ… Parallelizzazione: run su 3 notebook Kaggle simultanei
- âœ… Riutilizzo: stesso preprocessing per tutti i modelli
- âœ… Confronto: metriche indipendenti per ogni algoritmo
- âœ… Debugging: isolamento errori per modello

---

## ðŸ“ Note Finali

### Differenze Kaggle vs Locale:

| Aspetto | Kaggle | Locale |
|---------|--------|--------|
| Download repo | Automatico | Manuale (git clone) |
| Dataset path | `/kaggle/input/...` | `data/raw/` |
| Working dir | `/kaggle/working/` | Repository root |
| Export artifacts | ZIP automatico | Non necessario |
| Persistenza | Session Persistence | Permanente |

### CompatibilitÃ :

- âœ… **Python**: 3.8+
- âœ… **Pandas**: 1.3+
- âœ… **Scikit-learn**: 1.0+
- âœ… **NumPy**: 1.21+
- âœ… **Kaggle Docker**: `gcr.io/kaggle-gpu-images/python` (latest)

### Sicurezza:

- âš ï¸ **Non committare** file `.ipynb` con output (contengono dati)
- âš ï¸ **Non condividere** artifacts con dati sensibili
- âœ… **Usa** `.gitignore` per `*.parquet`, `artifacts/`

---

## â“ FAQ

**Q: Posso modificare i file in `src/` su Kaggle?**
A: SÃ¬, ma le modifiche vanno perse al restart. Meglio modificare nel repo e ri-scaricare.

**Q: Quanto spazio disco serve in locale?**
A: ~2GB (dataset raw ~1GB + processed ~1GB + artifacts ~100MB)

**Q: Posso usare GPU?**
A: No, la pipeline non usa GPU. CPU Ã¨ sufficiente.

**Q: CLEAN_RUN cancella anche il dataset raw?**
A: No, cancella solo `data/processed` e `artifacts`. Il dataset raw Ã¨ preservato.

**Q: Posso testare su subset del dataset?**
A: SÃ¬, modifica `preprocessing.py` per usare `sample(frac=0.1)` durante lo sviluppo.

---

## ðŸ“š Risorse

- **Repository**: https://github.com/riiccardob/nids-ml-ssr2
- **Dataset**: Kaggle Dataset "Network Intrusion Detection"
- **Documentazione Scikit-learn**: https://scikit-learn.org/stable/
- **Kaggle Notebooks Docs**: https://www.kaggle.com/docs/notebooks

---

**Creato da:** NIDS-ML Team  
**Ultima modifica:** 2026-01-28  
**Versione Notebook:** 2.0
