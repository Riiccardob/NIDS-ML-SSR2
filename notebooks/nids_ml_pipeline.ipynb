{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS-ML: Network Intrusion Detection System\n",
    "\n",
    "Questo notebook esegue la pipeline completa usando i moduli del progetto.\n",
    "\n",
    "**Requisiti**:\n",
    "- Dataset CIC-IDS2017 aggiunto al notebook\n",
    "- Cartella `src/` del progetto caricata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Su Kaggle, installa dipendenze mancanti se necessario\n",
    "# !pip install -q xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Rileva ambiente\n",
    "if Path(\"/kaggle/input\").exists():\n",
    "    # Kaggle\n",
    "    ENV = \"kaggle\"\n",
    "    PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "    DATA_INPUT = Path(\"/kaggle/input\")\n",
    "elif Path(\"/content\").exists():\n",
    "    # Google Colab\n",
    "    ENV = \"colab\"\n",
    "    PROJECT_ROOT = Path(\"/content/NIDS-ML\")\n",
    "    DATA_INPUT = Path(\"/content\")\n",
    "else:\n",
    "    # Locale\n",
    "    ENV = \"local\"\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    DATA_INPUT = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Ambiente: {ENV}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data input: {DATA_INPUT}\")\n",
    "\n",
    "# Aggiungi src al path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Su Kaggle/Colab: clona o carica il progetto\n",
    "if ENV == \"kaggle\":\n",
    "    # Opzione 1: Se hai caricato il progetto come dataset\n",
    "    # Cerca la cartella src nei dataset aggiunti\n",
    "    for p in DATA_INPUT.iterdir():\n",
    "        if (p / \"src\").exists():\n",
    "            print(f\"Progetto trovato in: {p}\")\n",
    "            # Copia src nella working directory\n",
    "            !cp -r {p}/src {PROJECT_ROOT}/\n",
    "            !cp -r {p}/config.yaml {PROJECT_ROOT}/ 2>/dev/null || true\n",
    "            break\n",
    "    else:\n",
    "        print(\"ATTENZIONE: Cartella src non trovata!\")\n",
    "        print(\"Carica il progetto come dataset Kaggle.\")\n",
    "\n",
    "elif ENV == \"colab\":\n",
    "    # Clona da GitHub (se disponibile) o carica manualmente\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        print(\"Carica la cartella del progetto in /content/NIDS-ML\")\n",
    "\n",
    "# Verifica\n",
    "if (PROJECT_ROOT / \"src\").exists():\n",
    "    print(f\"\\nModuli disponibili:\")\n",
    "    for f in sorted((PROJECT_ROOT / \"src\").glob(\"*.py\")):\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cartella src/ non trovata!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea struttura directory\n",
    "for d in [\"data/raw\", \"data/processed\", \"artifacts\", \"models\", \"logs\", \"reports\"]:\n",
    "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"Directory create.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trova e linka dataset CIC-IDS2017\n",
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    # Cerca il dataset\n",
    "    dataset_patterns = [\"cicids2017\", \"cic-ids-2017\", \"cicids\", \"ids2017\"]\n",
    "    dataset_path = None\n",
    "    \n",
    "    for p in DATA_INPUT.iterdir():\n",
    "        name_lower = p.name.lower()\n",
    "        if any(pat in name_lower for pat in dataset_patterns):\n",
    "            # Verifica che contenga CSV\n",
    "            if list(p.glob(\"**/*.csv\")):\n",
    "                dataset_path = p\n",
    "                break\n",
    "    \n",
    "    if dataset_path:\n",
    "        print(f\"Dataset trovato: {dataset_path}\")\n",
    "        # Crea symlink a data/raw\n",
    "        raw_dir = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "        for csv in dataset_path.glob(\"**/*.csv\"):\n",
    "            dest = raw_dir / csv.name\n",
    "            if not dest.exists():\n",
    "                !cp \"{csv}\" \"{dest}\"\n",
    "        print(f\"CSV copiati in: {raw_dir}\")\n",
    "    else:\n",
    "        print(\"ERRORE: Dataset CIC-IDS2017 non trovato!\")\n",
    "        print(\"Aggiungi il dataset al notebook Kaggle.\")\n",
    "\n",
    "# Verifica CSV\n",
    "csv_files = list((PROJECT_ROOT / \"data\" / \"raw\").glob(\"*.csv\"))\n",
    "print(f\"\\nCSV disponibili: {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024**2)\n",
    "    print(f\"  - {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import moduli progetto\n",
    "from src.preprocessing import main as preprocessing_main\n",
    "from src.preprocessing import load_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Esegui preprocessing\n",
    "# Equivalente a: python src/preprocessing.py --balance-ratio 2.0 --n-jobs 4\n",
    "\n",
    "import sys\n",
    "sys.argv = ['preprocessing.py', '--balance-ratio', '2.0', '--n-jobs', '4']\n",
    "\n",
    "preprocessing_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica output\n",
    "train, val, test, mappings = load_processed_data()\n",
    "print(f\"Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import main as feature_engineering_main\n",
    "from src.feature_engineering import load_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Esegui feature engineering\n",
    "# Equivalente a: python src/feature_engineering.py --n-features 30 --n-jobs 4\n",
    "\n",
    "sys.argv = ['feature_engineering.py', '--n-features', '30', '--rf-estimators', '100', '--n-jobs', '4']\n",
    "\n",
    "feature_engineering_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica artifacts\n",
    "scaler, selected_features, importances, scaler_columns = load_artifacts()\n",
    "print(f\"Feature selezionate: {len(selected_features)}\")\n",
    "print(f\"Colonne scaler: {len(scaler_columns)}\")\n",
    "print(f\"\\nTop 10 feature:\")\n",
    "for i, feat in enumerate(selected_features[:10]):\n",
    "    print(f\"  {i+1:2}. {feat}: {importances[feat]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.random_forest import main as rf_main\n",
    "from src.training.xgboost_model import main as xgb_main\n",
    "from src.training.lightgbm_model import main as lgbm_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training Random Forest\n",
    "# Equivalente a: python src/training/random_forest.py --n-iter 20 --cv 3 --n-jobs 4\n",
    "\n",
    "sys.argv = ['random_forest.py', '--task', 'binary', '--n-iter', '20', '--cv', '3', '--n-jobs', '4']\n",
    "\n",
    "rf_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training XGBoost\n",
    "# Equivalente a: python src/training/xgboost_model.py --n-iter 20 --cv 3 --n-jobs 4\n",
    "\n",
    "sys.argv = ['xgboost_model.py', '--task', 'binary', '--n-iter', '20', '--cv', '3', '--n-jobs', '4']\n",
    "\n",
    "xgb_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training LightGBM\n",
    "# Equivalente a: python src/training/lightgbm_model.py --n-iter 20 --cv 3 --n-jobs 4\n",
    "\n",
    "sys.argv = ['lightgbm_model.py', '--task', 'binary', '--n-iter', '20', '--cv', '3', '--n-jobs', '4']\n",
    "\n",
    "lgbm_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica modelli salvati\n",
    "import json\n",
    "\n",
    "print(\"Modelli addestrati:\")\n",
    "for model_dir in (PROJECT_ROOT / \"models\").iterdir():\n",
    "    if model_dir.is_dir() and model_dir.name != \"best_model\":\n",
    "        results_file = model_dir / \"results_binary.json\"\n",
    "        if results_file.exists():\n",
    "            with open(results_file) as f:\n",
    "                results = json.load(f)\n",
    "            metrics = results.get('validation_metrics', {})\n",
    "            print(f\"\\n  {model_dir.name}:\")\n",
    "            print(f\"    Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "            print(f\"    F1:       {metrics.get('f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import main as evaluation_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluation Random Forest\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/random_forest/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluation XGBoost\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/xgboost/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluation LightGBM\n",
    "sys.argv = ['evaluation.py', '--model-path', 'models/lightgbm/model_binary.pkl']\n",
    "evaluation_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confronto e Selezione Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.compare_models import main as compare_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Confronto modelli con scorecard\n",
    "# Equivalente a: python src/compare_models.py --max-fpr 0.01 --max-latency-ms 1.0\n",
    "\n",
    "sys.argv = ['compare_models.py', '--max-fpr', '0.02', '--max-latency-ms', '1.0']\n",
    "\n",
    "compare_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica best model\n",
    "best_model_dir = PROJECT_ROOT / \"models\" / \"best_model\"\n",
    "if best_model_dir.exists():\n",
    "    print(f\"Best model salvato in: {best_model_dir}\")\n",
    "    print(f\"\\nFile:\")\n",
    "    for f in best_model_dir.iterdir():\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    # Mostra metadata\n",
    "    metadata_file = best_model_dir / \"metadata.json\"\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file) as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"\\nBest model: {metadata.get('best_model', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Timing Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.timing import main as timing_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera report timing\n",
    "sys.argv = ['timing.py', '--report']\n",
    "\n",
    "try:\n",
    "    timing_main()\n",
    "except Exception as e:\n",
    "    print(f\"Timing report non disponibile: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizzazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Mostra grafici generati\n",
    "reports_dir = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "# Scorecard comparison\n",
    "scorecard_img = PROJECT_ROOT / \"models\" / \"best_model\" / \"scorecard_comparison.png\"\n",
    "if scorecard_img.exists():\n",
    "    print(\"Scorecard Comparison:\")\n",
    "    display(Image(filename=str(scorecard_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra confusion matrix e ROC del best model\n",
    "best_model_name = \"lightgbm\"  # Cambia se diverso\n",
    "\n",
    "# Cerca il nome corretto\n",
    "metadata_file = PROJECT_ROOT / \"models\" / \"best_model\" / \"metadata.json\"\n",
    "if metadata_file.exists():\n",
    "    with open(metadata_file) as f:\n",
    "        best_model_name = json.load(f).get('best_model', 'lightgbm')\n",
    "\n",
    "report_dir = reports_dir / best_model_name\n",
    "\n",
    "if report_dir.exists():\n",
    "    for img_name in [\"confusion_matrix_binary.png\", \"roc_curve_binary.png\", \"feature_importance_binary.png\"]:\n",
    "        img_path = report_dir / img_name\n",
    "        if img_path.exists():\n",
    "            print(f\"\\n{img_name}:\")\n",
    "            display(Image(filename=str(img_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Output (Kaggle/Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV in [\"kaggle\", \"colab\"]:\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    \n",
    "    # Crea ZIP con tutto l'output\n",
    "    zip_path = PROJECT_ROOT / \"nids_ml_output.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Artifacts\n",
    "        for f in (PROJECT_ROOT / \"artifacts\").glob(\"*\"):\n",
    "            zipf.write(f, f\"artifacts/{f.name}\")\n",
    "        \n",
    "        # Models\n",
    "        for model_dir in (PROJECT_ROOT / \"models\").iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                for f in model_dir.glob(\"*\"):\n",
    "                    zipf.write(f, f\"models/{model_dir.name}/{f.name}\")\n",
    "        \n",
    "        # Reports\n",
    "        for f in (PROJECT_ROOT / \"reports\").rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                rel_path = f.relative_to(PROJECT_ROOT / \"reports\")\n",
    "                zipf.write(f, f\"reports/{rel_path}\")\n",
    "        \n",
    "        # Logs timing\n",
    "        timing_dir = PROJECT_ROOT / \"logs\" / \"timing\"\n",
    "        if timing_dir.exists():\n",
    "            for f in timing_dir.glob(\"*\"):\n",
    "                zipf.write(f, f\"logs/timing/{f.name}\")\n",
    "    \n",
    "    zip_size = zip_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"ZIP creato: {zip_path}\")\n",
    "    print(f\"Dimensione: {zip_size:.1f} MB\")\n",
    "    print(f\"\\nScarica dalla tab 'Output' (Kaggle) o Files (Colab)\")\n",
    "else:\n",
    "    print(\"Ambiente locale - output gia disponibile nella cartella del progetto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Riepilogo Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PIPELINE COMPLETATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mostra metriche best model\n",
    "best_report = PROJECT_ROOT / \"models\" / \"best_model\" / \"comparison_results.json\"\n",
    "if best_report.exists():\n",
    "    with open(best_report) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Trova il best\n",
    "    best = max(results, key=lambda x: x.get('score', 0))\n",
    "    \n",
    "    print(f\"\\nBest Model: {best['model_name'].upper()}\")\n",
    "    print(f\"\\nMetriche:\")\n",
    "    for k, v in best.get('metrics', {}).items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k:25}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"\\nLatenza:\")\n",
    "    for k, v in best.get('latency', {}).items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k:25}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Per usare lo sniffer in locale:\")\n",
    "print(\"  sudo python src/sniffer.py --interface eth0 --verbose\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
