================================================================================
NIDS-ML - GUIDA COMANDI COMPLETA
================================================================================

INDICE:
1. Setup Iniziale
2. Preprocessing
3. Feature Engineering
4. Hyperparameter Tuning (NUOVO)
5. Training Modelli
6. Evaluation
7. Model Comparison
8. Best Model Selection
9. Sniffer

================================================================================
1. SETUP INIZIALE
================================================================================

# Installazione dipendenze
pip install -r requirements.txt

# Verifica installazione
python -c "import sklearn, xgboost, lightgbm, scapy; print('OK')"

# Struttura directory
mkdir -p data/raw data/processed models/{random_forest,xgboost,lightgbm} \
         artifacts tuning_results logs reports

================================================================================
2. PREPROCESSING
================================================================================

# Standard (bilanciamento 2:1)
python src/preprocessing.py

# Senza bilanciamento
python src/preprocessing.py --no-balance

# Con chunk (RAM limitata)
python src/preprocessing.py --chunk-size 100000

# Custom split
python src/preprocessing.py --test-size 0.2 --val-size 0.2

# Limita CPU
python src/preprocessing.py --n-jobs 4

OUTPUT:
  - data/processed/train.parquet
  - data/processed/val.parquet
  - data/processed/test.parquet
  - data/processed/label_mappings.json

================================================================================
3. FEATURE ENGINEERING
================================================================================

# Standard (30 feature)
python src/feature_engineering.py

# Piu feature
python src/feature_engineering.py --n-features 50

# Test veloce
python src/feature_engineering.py --rf-estimators 20

# Limita risorse
python src/feature_engineering.py --n-jobs 4 --max-ram 80

OUTPUT:
  - artifacts/scaler.pkl
  - artifacts/selected_features.json
  - artifacts/feature_importances.json
  - artifacts/scaler_columns.json  (IMPORTANTE per sniffer!)
  - data/processed/train_ready.parquet
  - data/processed/val_ready.parquet
  - data/processed/test_ready.parquet

================================================================================
4. HYPERPARAMETER TUNING (NUOVO)
================================================================================

NOTA: Il tuning ottimizza i parametri usando metrica composita:
      70% F2-Score + 30% Latency Score

# Random Search (veloce, 10-30 min)
python src/hyperparameter_tuning.py --model xgboost --method random --n-iter 50

# Bayesian Optimization (accurato, 1-3 ore)
python src/hyperparameter_tuning.py --model lightgbm --method bayesian --n-trials 100

# Con timeout
python src/hyperparameter_tuning.py --model random_forest --method bayesian \
    --n-trials 200 --timeout 7200  # 2 ore

# Custom CV e constraint latency
python src/hyperparameter_tuning.py --model xgboost --method random \
    --n-iter 30 --cv 5 --max-latency-ms 0.5

OUTPUT:
  - tuning_results/{model_type}/{method}_iter{N}_cv{K}_{timestamp}.json
  Esempio: tuning_results/xgboost/random_iter50_cv5_2026-01-24_20.02.json

STRUTTURA FILE TUNING:
{
  "model_type": "xgboost",
  "task": "binary",
  "tuning_timestamp": "2026-01-24T20:02:15",
  "tuning_method": "random_search",
  "best_params": {
    "n_estimators": 300,
    "max_depth": 10,
    "learning_rate": 0.05,
    ...
  },
  "best_score": 0.9845,
  "best_f2_score": 0.9912,
  "best_latency_ms": 0.0023,
  "search_config": {...}
}

================================================================================
5. TRAINING MODELLI
================================================================================

NOTA: Ora ci sono DUE modalita di training:
  1. Random Search (legacy): cerca parametri durante il training
  2. Tuned Params (RACCOMANDATO): usa parametri da hyperparameter_tuning.py

--------------------------------------------------------------------------------
5.1 RANDOM FOREST
--------------------------------------------------------------------------------

# Con parametri tuned (piu recente)
python src/training/random_forest.py --use-tuned-params

# Con config specifica
python src/training/random_forest.py --use-tuned-params \
    --tuning-config random_iter50_cv5_2026-01-24_20.02.json

# Con timestamp parziale
python src/training/random_forest.py --use-tuned-params \
    --tuning-timestamp 2026-01-24_20.02

# Lista config disponibili
python src/training/random_forest.py --list-configs

# Random search (legacy)
python src/training/random_forest.py --n-iter 20 --cv 3

# Test veloce
python src/training/random_forest.py --n-iter 5 --cv 2 --n-jobs 4

OUTPUT CON TUNED PARAMS:
  - models/random_forest/{version_id}/model_binary.pkl
  - models/random_forest/{version_id}/results_binary.json
  - models/random_forest/{version_id}/features_binary.json
  - models/random_forest/{version_id}/tuning_source.json
  
  version_id = nome del file tuning (es: random_iter50_cv5_2026-01-24_20.02)

OUTPUT SENZA TUNED PARAMS (legacy):
  - models/random_forest/model_binary.pkl
  - models/random_forest/results_binary.json
  - models/random_forest/features_binary.json

--------------------------------------------------------------------------------
5.2 XGBOOST
--------------------------------------------------------------------------------

# Con parametri tuned (piu recente)
python src/training/xgboost_model.py --use-tuned-params

# Con config specifica
python src/training/xgboost_model.py --use-tuned-params \
    --tuning-config bayesian_trials100_cv5_2026-01-24_21.15.json

# Con GPU (se disponibile)
python src/training/xgboost_model.py --use-tuned-params --gpu

# Random search (legacy)
python src/training/xgboost_model.py --n-iter 20 --cv 3

# Senza early stopping
python src/training/xgboost_model.py --use-tuned-params --no-early-stopping

--------------------------------------------------------------------------------
5.3 LIGHTGBM
--------------------------------------------------------------------------------

# Con parametri tuned (piu recente)
python src/training/lightgbm_model.py --use-tuned-params

# Con config specifica
python src/training/lightgbm_model.py --use-tuned-params \
    --tuning-config random_iter50_cv5_2026-01-24_22.30.json

# Random search (legacy)
python src/training/lightgbm_model.py --n-iter 20 --cv 3

--------------------------------------------------------------------------------
5.4 MULTICLASSE (opzionale)
--------------------------------------------------------------------------------

# Tuning multiclasse
python src/hyperparameter_tuning.py --model xgboost --task multiclass \
    --method random --n-iter 30

# Training multiclasse con tuned params
python src/training/xgboost_model.py --task multiclass --use-tuned-params

================================================================================
6. EVALUATION
================================================================================

# Valuta singolo modello
python src/evaluation.py --model-path models/random_forest/model_binary.pkl

# Valuta tutte le versioni di un tipo
python src/evaluation.py --model-type xgboost

# Valuta TUTTE le versioni di TUTTI i modelli
python src/evaluation.py --model-type all

# Multiclasse
python src/evaluation.py --model-path models/xgboost/model_multiclass.pkl \
    --task multiclass

OUTPUT:
  - reports/{model_type}/{version_id}/confusion_matrix_binary.png
  - reports/{model_type}/{version_id}/roc_curve_binary.png
  - reports/{model_type}/{version_id}/pr_curve_binary.png
  - reports/{model_type}/{version_id}/feature_importance_binary.png
  - reports/{model_type}/{version_id}/report_{model_type}_binary.json
  - reports/{model_type}/{version_id}/report_{model_type}_binary.txt
  - reports/{model_type}/{version_id}/latency_binary.json

ESEMPIO OUTPUT --model-type all:
  Valuta tutti i modelli trovati in models/ e sottocartelle:
  - models/random_forest/model_binary.pkl
  - models/random_forest/random_iter50_cv5_2026-01-24_20.02/model_binary.pkl
  - models/xgboost/bayesian_trials100_cv5_2026-01-24_21.15/model_binary.pkl
  - models/lightgbm/random_iter30_cv5_2026-01-24_22.30/model_binary.pkl
  ...

================================================================================
7. MODEL COMPARISON
================================================================================

NOTA: Confronta TUTTE le versioni usando Scorecard con Hard Constraints:
  - Hard Constraint 1: FPR <= 1%
  - Hard Constraint 2: Latency <= 1.0ms/sample
  - Score = 70% F2-Score + 30% Latency Score (solo se passa constraints)

# Confronto standard
python src/compare_models.py

# Custom constraints
python src/compare_models.py --max-fpr 0.005 --max-latency-ms 0.5

# Multiclasse
python src/compare_models.py --task multiclass

OUTPUT:
  - models/best_model/model_binary.pkl  (copia del migliore)
  - models/best_model/results_binary.json
  - models/best_model/features_binary.json
  - models/best_model/metadata.json  (info sulla selezione)
  - models/best_model/all_versions_comparison.json  (tutti i risultati)
  - models/best_model/comparison_report.txt
  - models/best_model/scorecard_comparison.png
  - models/best_model/algorithm_rankings.png
  - models/best_model/plateau_analysis.png

ESEMPIO OUTPUT:
  Versione                            F1        Recall    Latency      Score      Status
  -----------------------------------------------------------------------------------
  xgboost/bayesian_trials100_...    0.9945    0.9989    0.0018      0.9823     PASS
  lightgbm/random_iter50_...        0.9932    0.9976    0.0021      0.9801     PASS
  random_forest/random_iter30_...   0.9928    0.9971    0.0025      0.9785     PASS
  xgboost/random_iter20_...         0.9920    0.9965    0.0022      0.9772     PASS
  ...

  BEST MODEL: xgboost/bayesian_trials100_cv5_2026-01-24_21.15

================================================================================
8. BEST MODEL SELECTION
================================================================================

Dopo compare_models.py, il best model e automaticamente copiato in:
  models/best_model/

Puoi verificare quale modello e stato selezionato:
  cat models/best_model/metadata.json

Per usare un altro modello manualmente:
  cp models/xgboost/{version_id}/model_binary.pkl models/best_model/
  cp models/xgboost/{version_id}/results_binary.json models/best_model/
  cp models/xgboost/{version_id}/features_binary.json models/best_model/

Lista tutte le versioni disponibili:
  python -c "from src.model_versioning import print_versions_summary; print_versions_summary()"

OUTPUT:
  VERSIONI MODELLI DISPONIBILI
  ================================================================================
  Tipo            Versione                                      F1        Recall
  ---------------------------------------------------------------------------------
  xgboost         bayesian_trials100_cv5_2026-01-24_21.15      0.9945    0.9989
  xgboost         random_iter50_cv5_2026-01-24_20.02           0.9932    0.9976
  ---------------------------------------------------------------------------------
  lightgbm        random_iter50_cv5_2026-01-24_22.30           0.9928    0.9971
  ---------------------------------------------------------------------------------
  random_forest   random_iter30_cv3_2026-01-24_19.45           0.9920    0.9965
  ================================================================================
  Migliore: xgboost/bayesian_trials100_cv5_2026-01-24_21.15 (F1=0.9945)

================================================================================
9. SNIFFER
================================================================================

NOTA: Lo sniffer usa SEMPRE il modello in models/best_model/ (salvo override)


# Analisi PCAP
python src/sniffer.py --pcap capture.pcap --verbose

# Con modello specifico
python src/sniffer.py --pcap capture.pcap \
    --model-path models/xgboost/bayesian_trials100_cv5_2026-01-24_21.15/model_binary.pkl

# Cattura live: Detection mode (richiede root)
||| sudo $(which python) src/sniffer.py |||
sudo python src/sniffer.py --interface eth0

# Cattura live: Prevention mode (blocca IP)
sudo python src/sniffer.py --interface eth0 --mode prevention

# Custom threshold e timeout
python src/sniffer.py --pcap capture.pcap --threshold 0.7 --timeout 30

# Verbose (mostra anche benigni)
python src/sniffer.py --pcap capture.pcap --verbose

OUTPUT:
  - logs/sniffer_{session_id}.log
  - logs/attacks_{session_id}.log
  - logs/flows_{session_id}.jsonl
  - logs/summary_{session_id}.json

================================================================================
WORKFLOW COMPLETO RACCOMANDATO
================================================================================

# 1. Preprocessing
python src/preprocessing.py

# 2. Feature Engineering
python src/feature_engineering.py

# 3. Hyperparameter Tuning (tutti e 3 gli algoritmi)
for model in xgboost lightgbm random_forest; do
    python src/hyperparameter_tuning.py --model $model --method random --n-iter 50
done

# 4. Training con parametri tuned
python src/training/xgboost_model.py --use-tuned-params
python src/training/lightgbm_model.py --use-tuned-params
python src/training/random_forest.py --use-tuned-params

# 5. Evaluation
python src/evaluation.py --model-type all

# 6. Confronto e selezione best model
python src/compare_models.py

# 7. Test sniffer
python src/sniffer.py --pcap data/raw/Friday-WorkingHours-Morning.pcap

================================================================================
TROUBLESHOOTING
================================================================================

# Artifacts mancanti
Se "scaler_columns.json mancante":
  python src/feature_engineering.py  # Rigenera artifacts

# Config tuning non trovata
Se "Config file non trovato":
  python src/training/xgboost_model.py --list-configs  # Lista disponibili
  python src/hyperparameter_tuning.py --model xgboost --method random --n-iter 50  # Crea nuova

# Modello non compatibile
Se "Feature mismatch" durante sniffer:
  Verifica che il modello sia stato trainato con gli artifacts correnti
  Rigenera artifacts + ritraina il modello se necessario

# RAM insufficiente
  python src/preprocessing.py --chunk-size 50000
  python src/feature_engineering.py --max-ram 70 --n-jobs 2

# CPU overload
  python src/training/xgboost_model.py --n-jobs 2
  python src/hyperparameter_tuning.py --model xgboost --n-jobs 2

================================================================================
FILE E DIRECTORY IMPORTANTI
================================================================================

data/
├── raw/                    # CSV CIC-IDS2017 originali
└── processed/              # Dataset processati
    ├── train.parquet
    ├── val.parquet
    ├── test.parquet
    └── label_mappings.json

artifacts/                  # Oggetti feature engineering
├── scaler.pkl              # StandardScaler fitted
├── selected_features.json  # Lista 30 feature selezionate
├── feature_importances.json
└── scaler_columns.json     # IMPORTANTE: colonne usate per fit scaler

tuning_results/             # Risultati hyperparameter tuning
├── xgboost/
│   ├── random_iter50_cv5_2026-01-24_20.02.json
│   └── bayesian_trials100_cv5_2026-01-24_21.15.json
├── lightgbm/
│   └── random_iter50_cv5_2026-01-24_22.30.json
└── random_forest/
    └── random_iter30_cv3_2026-01-24_19.45.json

models/                     # Modelli trainati
├── xgboost/
│   ├── model_binary.pkl    # Legacy (senza tuned params)
│   ├── results_binary.json
│   ├── bayesian_trials100_cv5_2026-01-24_21.15/  # Con tuned params
│   │   ├── model_binary.pkl
│   │   ├── results_binary.json
│   │   ├── features_binary.json
│   │   └── tuning_source.json  # Link al file tuning usato
│   └── random_iter50_cv5_2026-01-24_20.02/
│       └── ...
├── lightgbm/
│   └── ...
├── random_forest/
│   └── ...
└── best_model/             # Best model selezionato
    ├── model_binary.pkl
    ├── results_binary.json
    ├── features_binary.json
    ├── metadata.json
    ├── all_versions_comparison.json
    ├── comparison_report.txt
    └── *.png (grafici)

reports/                    # Report evaluation
└── {model_type}/{version_id}/
    ├── confusion_matrix_binary.png
    ├── roc_curve_binary.png
    ├── pr_curve_binary.png
    ├── feature_importance_binary.png
    ├── report_{model_type}_binary.json
    └── latency_binary.json

logs/                       # Log esecuzioni
├── timing/                 # Timing dettagliati
└── sniffer_{session_id}.log

================================================================================
TIPS E BEST PRACTICES
================================================================================

1. Hyperparameter Tuning
   - Usa Random Search (veloce) per esplorazione iniziale
   - Usa Bayesian (lento ma accurato) per fine-tuning
   - n_iter=50 e generalmente sufficiente per Random Search
   - n_trials=100-200 per Bayesian Optimization

2. Training
   - Usa SEMPRE --use-tuned-params dopo aver fatto tuning
   - Lista config con --list-configs prima di trainare
   - Usa --tuning-timestamp per selezionare config specifica

3. Versioning
   - Ogni training con tuned params crea una nuova versione
   - Nome versione = nome file tuning
   - Versioni multiple dello stesso algoritmo sono OK e desiderate

4. Comparison
   - Esegui compare_models.py dopo aver trainato piu versioni
   - Analizza i grafici plateau_analysis.png per capire quando fermare tuning
   - Verifica che il best model passi i constraints

5. Sniffer
   - Testa SEMPRE su PCAP prima di usare in live
   - Verifica che scaler_columns.json esista negli artifacts
   - Usa --verbose per debug, --quiet per produzione

================================================================================