{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS-ML: Phase 1 - Hyperparameter Tuning Pipeline\n",
    "\n",
    "This notebook runs the complete pipeline for hyperparameter tuning:\n",
    "1. **Preprocessing**: Clean and prepare raw data\n",
    "2. **Feature Engineering**: Statistical preprocessing, scaling, and feature selection\n",
    "3. **Hyperparameter Tuning**: Bayesian optimization with Optuna\n",
    "\n",
    "**Compatible with:** Local environment and Kaggle notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration & Parameters\n",
    "\n",
    "**EDIT THIS CELL to configure your run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PIPELINE CONFIGURATION\n",
      "======================================================================\n",
      "Model Type:       lightgbm\n",
      "N Trials:         100\n",
      "Timeout:          60s (0.02 hours)\n",
      "CV Folds:         5\n",
      "Kaggle Dataset:   network-intrusion-dataset\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Model Selection\n",
    "MODEL_TYPE = 'lightgbm'  # Options: 'lightgbm', 'random_forest', 'xgboost'\n",
    "\n",
    "# Hyperparameter Tuning Limits\n",
    "N_TRIALS = 5000           # Number of optimization trials\n",
    "TIMEOUT = 41400         # Timeout in seconds (3600s = 1 hour)\n",
    "CV_FOLDS = 5             # Cross-validation folds\n",
    "\n",
    "# Note: Tuning will stop when EITHER N_TRIALS or TIMEOUT is reached (whichever comes first)\n",
    "\n",
    "# Kaggle Dataset Configuration\n",
    "KAGGLE_DATASET_NAME = 'network-intrusion-dataset'  # Change if your dataset has a different name\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model Type:       {MODEL_TYPE}\")\n",
    "print(f\"N Trials:         {N_TRIALS}\")\n",
    "print(f\"Timeout:          {TIMEOUT}s ({TIMEOUT/3600:.2f} hours)\")\n",
    "print(f\"CV Folds:         {CV_FOLDS}\")\n",
    "print(f\"Kaggle Dataset:   {KAGGLE_DATASET_NAME}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Environment Detection & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENVIRONMENT DETECTION\n",
      "======================================================================\n",
      "Detected Environment: LOCAL\n",
      "\n",
      "Repository Root:  /home/enea/Desktop/NIDS-ML-SSR2\n",
      "Raw Data Path:    /home/enea/Desktop/NIDS-ML-SSR2/data/raw\n",
      "Working Dir:      /home/enea/Desktop/NIDS-ML-SSR2\n",
      "\n",
      "‚úì 'src' module found at: /home/enea/Desktop/NIDS-ML-SSR2/src\n",
      "\n",
      "‚úì Found 8 CSV file(s) in raw data directory\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# DETECT ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Kaggle or Local.\"\"\"\n",
    "    if Path(\"/kaggle/input\").exists():\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        return \"local\"\n",
    "\n",
    "ENV = detect_environment()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Detected Environment: {ENV.upper()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP PATHS\n",
    "# ============================================================================\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    # Kaggle environment\n",
    "    WORKING_DIR = Path(\"/kaggle/working\")\n",
    "    RAW_DATA_DIR = Path(f\"/kaggle/input/{KAGGLE_DATASET_NAME}\")\n",
    "    \n",
    "    # Check if src directory exists, if not, download it from GitHub\n",
    "    SRC_DIR = WORKING_DIR / \"src\"\n",
    "    \n",
    "    if not SRC_DIR.exists():\n",
    "        print(\"\\n‚ö†Ô∏è  'src' directory not found. Downloading from GitHub...\")\n",
    "        import subprocess\n",
    "        import shutil\n",
    "        \n",
    "        try:\n",
    "            # Clone repository to temporary location\n",
    "            temp_repo = WORKING_DIR / \"temp_repo\"\n",
    "            clone_result = subprocess.run(\n",
    "                [\"git\", \"clone\", \"--depth\", \"1\", \n",
    "                 \"https://github.com/Riiccardob/NIDS-ML-SSR2.git\", \n",
    "                 str(temp_repo)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Copy only src/ directory to working directory\n",
    "            shutil.copytree(temp_repo / \"src\", SRC_DIR)\n",
    "            \n",
    "            # Clean up temporary repository\n",
    "            shutil.rmtree(temp_repo)\n",
    "            \n",
    "            print(\"‚úì src/ directory downloaded successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå ERROR downloading repository: {e.stderr}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR setting up src/: {e}\")\n",
    "            # Clean up if something went wrong\n",
    "            if temp_repo.exists():\n",
    "                shutil.rmtree(temp_repo)\n",
    "            raise\n",
    "    \n",
    "    # Add working directory to sys.path (src/ is directly in working dir)\n",
    "    REPO_ROOT = WORKING_DIR\n",
    "    if str(REPO_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(REPO_ROOT))\n",
    "    \n",
    "    # Set working directory\n",
    "    os.chdir(REPO_ROOT)\n",
    "    \n",
    "else:\n",
    "    # Local environment\n",
    "    # Assume notebook is in project root or notebooks/ subdirectory\n",
    "    CURRENT_DIR = Path.cwd()\n",
    "    \n",
    "    # Check if we're in notebooks/ subdirectory\n",
    "    if CURRENT_DIR.name == \"notebooks\":\n",
    "        REPO_ROOT = CURRENT_DIR.parent\n",
    "    else:\n",
    "        REPO_ROOT = CURRENT_DIR\n",
    "    \n",
    "    RAW_DATA_DIR = REPO_ROOT / \"data\" / \"raw\"\n",
    "    \n",
    "    # Add repository root to sys.path\n",
    "    if str(REPO_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(REPO_ROOT))\n",
    "    \n",
    "    # Set working directory to repo root\n",
    "    os.chdir(REPO_ROOT)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nRepository Root:  {REPO_ROOT}\")\n",
    "print(f\"Raw Data Path:    {RAW_DATA_DIR}\")\n",
    "print(f\"Working Dir:      {os.getcwd()}\")\n",
    "\n",
    "# Verify src is importable\n",
    "try:\n",
    "    import src\n",
    "    print(f\"\\n‚úì 'src' module found at: {src.__path__[0]}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå ERROR: Cannot import 'src' module\")\n",
    "    print(f\"   {e}\")\n",
    "    print(f\"\\n   Current sys.path:\")\n",
    "    for p in sys.path[:5]:\n",
    "        print(f\"     - {p}\")\n",
    "    raise\n",
    "\n",
    "# Verify raw data exists\n",
    "if not RAW_DATA_DIR.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Raw data directory not found: {RAW_DATA_DIR}\")\n",
    "    if ENV == \"kaggle\":\n",
    "        print(f\"   Make sure the dataset '{KAGGLE_DATASET_NAME}' is attached to this notebook.\")\n",
    "    else:\n",
    "        print(f\"   Make sure CSV files are in: {RAW_DATA_DIR}\")\n",
    "else:\n",
    "    csv_files = list(RAW_DATA_DIR.glob(\"*.csv\"))\n",
    "    print(f\"\\n‚úì Found {len(csv_files)} CSV file(s) in raw data directory\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODULES IMPORTED SUCCESSFULLY\n",
      "======================================================================\n",
      "‚úì preprocessing\n",
      "‚úì feature_engineering\n",
      "‚úì hyperparameter_tuning\n",
      "‚úì utils\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Project imports\n",
    "from src import preprocessing\n",
    "from src import feature_engineering\n",
    "from src import hyperparameter_tuning\n",
    "from src import utils\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODULES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úì preprocessing\")\n",
    "print(f\"‚úì feature_engineering\")\n",
    "print(f\"‚úì hyperparameter_tuning\")\n",
    "print(f\"‚úì utils\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Preprocessing\n",
    "\n",
    "Loads raw CSV files, cleans data, encodes labels, and splits into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# STEP 1: PREPROCESSING\n",
      "######################################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING CIC-IDS2017\n",
      "============================================================\n",
      "\n",
      "Parametri:\n",
      "  Input:         /home/enea/Desktop/NIDS-ML-SSR2/data/raw\n",
      "  Output:        /home/enea/Desktop/NIDS-ML-SSR2/data/processed\n",
      "  Balance:       Si (ratio 2.0:1)\n",
      "  Chunk size:    Disabilitato\n",
      "  Split:         70/15/15\n",
      "  CPU cores:     14/16\n",
      "\n",
      "1. Caricamento CSV da /home/enea/Desktop/NIDS-ML-SSR2/data/raw...\n",
      "2026-01-28 19:48:22 | INFO     | Trovati 8 file CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caricamento CSV: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:18<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 19:48:40 | INFO     | Concatenazione 8 DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 19:48:41 | INFO     | Dataset combinato: 2,830,743 righe, 79 colonne\n",
      "2026-01-28 19:48:41 | INFO     | Memoria: 1.7 GB\n",
      "\n",
      "2. Pulizia dati...\n",
      "2026-01-28 19:48:41 | INFO     | Inizio pulizia dati...\n",
      "2026-01-28 19:48:41 | INFO     | Rimosse 1 colonne identificative\n",
      "2026-01-28 19:48:42 | INFO     | Rimosse 2,867 righe con valori infiniti\n",
      "2026-01-28 19:49:01 | INFO     | Rimosse 594,712 righe duplicate\n",
      "2026-01-28 19:49:01 | INFO     | Pulizia completata: 2,830,743 -> 2,233,164 righe\n",
      "\n",
      "3. Encoding label...\n",
      "2026-01-28 19:49:01 | INFO     | Encoding label...\n",
      "2026-01-28 19:49:02 | INFO     | Classi trovate: 15\n",
      "2026-01-28 19:49:02 | INFO     | Distribuzione binaria: Benign=1,896,672, Attack=336,492\n",
      "\n",
      "   Distribuzione classi:\n",
      "   - BENIGN: 1,896,672 (84.93%)\n",
      "   - DoS Hulk: 172,846 (7.74%)\n",
      "   - DDoS: 128,014 (5.73%)\n",
      "   - DoS GoldenEye: 10,286 (0.46%)\n",
      "   - FTP-Patator: 5,931 (0.27%)\n",
      "   - DoS slowloris: 5,385 (0.24%)\n",
      "   - DoS Slowhttptest: 5,228 (0.23%)\n",
      "   - SSH-Patator: 3,219 (0.14%)\n",
      "   - PortScan: 1,956 (0.09%)\n",
      "   - Web Attack ÔøΩ Brute Force: 1,470 (0.07%)\n",
      "   - Bot: 1,437 (0.06%)\n",
      "   - Web Attack ÔøΩ XSS: 652 (0.03%)\n",
      "   - Infiltration: 36 (0.00%)\n",
      "   - Web Attack ÔøΩ Sql Injection: 21 (0.00%)\n",
      "   - Heartbleed: 11 (0.00%)\n",
      "\n",
      "4. Bilanciamento (ratio 2.0:1)...\n",
      "2026-01-28 19:49:02 | INFO     | Bilanciamento dataset (ratio 2.0:1)...\n",
      "2026-01-28 19:49:04 | INFO     | Undersampling classe 0: 1,896,672 -> 672,984\n",
      "2026-01-28 19:49:04 | INFO     | Dataset bilanciato: 1,009,476 righe\n",
      "\n",
      "5. Split train/val/test...\n",
      "2026-01-28 19:49:04 | INFO     | Split dataset (70/15/15)...\n",
      "2026-01-28 19:49:06 | INFO     | Train: 706,632 | Val: 151,422 | Test: 151,422\n",
      "\n",
      "6. Salvataggio...\n",
      "2026-01-28 19:49:08 | INFO     | Dataset salvati in /home/enea/Desktop/NIDS-ML-SSR2/data/processed\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETATO\n",
      "============================================================\n",
      "\n",
      "Output: /home/enea/Desktop/NIDS-ML-SSR2/data/processed\n",
      "Train:  706,632 righe\n",
      "Val:    151,422 righe\n",
      "Test:   151,422 righe\n",
      "Timing: /home/enea/Desktop/NIDS-ML-SSR2/logs/timing/preprocessing_20260128_194822.json\n",
      "\n",
      "Prossimo step: python src/feature_engineering.py\n",
      "\n",
      "--------------------------------------------------\n",
      "TIMING SUMMARY - preprocessing\n",
      "--------------------------------------------------\n",
      "Session: 20260128_194822\n",
      "Total time: 46.13s\n",
      "\n",
      "Operations:\n",
      "  caricamento_csv               :    19.09s ( 41.4%)\n",
      "  pulizia_dati                  :    20.28s ( 44.0%)\n",
      "  encoding_label                :     0.72s (  1.6%)\n",
      "  bilanciamento                 :     2.47s (  5.4%)\n",
      "  split_dataset                 :     1.71s (  3.7%)\n",
      "  salvataggio                   :     1.83s (  4.0%)\n",
      "\n",
      "Metrics:\n",
      "  train_rows: 706632\n",
      "  val_rows: 151422\n",
      "  test_rows: 151422\n",
      "--------------------------------------------------\n",
      "2026-01-28 19:49:08 | INFO     | CPU: 2.5% | RAM: 66.8% | Disponibile: 4.5GB | Core attivi: 14/16\n",
      "\n",
      "‚úì Preprocessing completed in 0:00:46.232650\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# STEP 1: PREPROCESSING\")\n",
    "print(\"#\" * 70 + \"\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Set up arguments for preprocessing.main()\n",
    "sys.argv = [\n",
    "    'preprocessing.py',\n",
    "    '--input-dir', str(RAW_DATA_DIR),\n",
    "    '--n-jobs', '4'\n",
    "]\n",
    "\n",
    "# Run preprocessing\n",
    "try:\n",
    "    preprocessing.main()\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"\\n‚úì Preprocessing completed in {elapsed}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR during preprocessing: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Feature Engineering\n",
    "\n",
    "Applies statistical preprocessing, scaling (RobustScaler), and feature selection (Random Forest Importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# STEP 2: FEATURE ENGINEERING\n",
      "######################################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING v2\n",
      "============================================================\n",
      "\n",
      "Parametri:\n",
      "  Statistical preprocessing: ON (DEFAULT)\n",
      "    - Variance threshold:    0.0\n",
      "    - Correlation threshold: 0.95\n",
      "  Scaler:                    RobustScaler (DEFAULT)\n",
      "  Metodo selezione:          Random Forest Importance\n",
      "  Feature da selezionare:    30\n",
      "  RF estimators:             100\n",
      "  CPU cores:                 4/16\n",
      "\n",
      "1. Caricamento dati preprocessati...\n",
      "2026-01-28 19:49:08 | INFO     | Caricati: train=706,632, val=151,422, test=151,422\n",
      "   Train: 706,632 | Val: 151,422 | Test: 151,422\n",
      "2026-01-28 19:49:08 | INFO     | CPU: 4.5% | RAM: 74.8% | Disponibile: 3.4GB | Core attivi: 14/16\n",
      "\n",
      "2. Esecuzione pipeline feature engineering...\n",
      "2026-01-28 19:49:08 | INFO     | Feature iniziali: 77\n",
      "2026-01-28 19:49:09 | INFO     | ============================================================\n",
      "2026-01-28 19:49:09 | INFO     | STATISTICAL PREPROCESSING\n",
      "2026-01-28 19:49:09 | INFO     | ============================================================\n",
      "2026-01-28 19:49:09 | INFO     | Step 1: Removing low-variance features (threshold=0.0)...\n",
      "2026-01-28 19:49:09 | INFO     |   Removed 8 low-variance features\n",
      "2026-01-28 19:49:09 | INFO     |   Remaining: 69\n",
      "2026-01-28 19:49:09 | INFO     |   Removed features: ['Bwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Avg Bulk Rate', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk']\n",
      "2026-01-28 19:49:09 | INFO     | Step 2: Removing high-correlation features (threshold=0.95)...\n",
      "2026-01-28 19:49:13 | INFO     |   Removed 25 high-correlation features\n",
      "2026-01-28 19:49:13 | INFO     |   Remaining: 44\n",
      "2026-01-28 19:49:13 | INFO     | ============================================================\n",
      "2026-01-28 19:49:13 | INFO     | Statistical preprocessing completed:\n",
      "2026-01-28 19:49:13 | INFO     |   77 ‚Üí 44 features (42.9% reduction)\n",
      "2026-01-28 19:49:13 | INFO     | ============================================================\n",
      "2026-01-28 19:49:13 | INFO     | Dopo statistical: 44 feature\n",
      "2026-01-28 19:49:13 | INFO     | ‚úì Scaler verr√† fittato su 44 colonne\n",
      "2026-01-28 19:49:13 | INFO     | Using RobustScaler (outlier-resistant) - DEFAULT\n",
      "2026-01-28 19:49:14 | INFO     | Scaler fitted su 44 feature\n",
      "2026-01-28 19:49:14 | INFO     | Selezione feature con Random Forest importance\n",
      "2026-01-28 19:49:14 | INFO     | Feature selection con Random Forest (n_estimators=100)...\n",
      "2026-01-28 19:49:55 | INFO     | Selezionate 30 feature (top 30)\n",
      "2026-01-28 19:49:55 | INFO     | Top 5: ['Packet Length Variance', 'Avg Bwd Segment Size', 'Average Packet Size', 'Subflow Bwd Bytes', 'Fwd Header Length']\n",
      "2026-01-28 19:49:55 | INFO     | Salvato: scaler.pkl\n",
      "2026-01-28 19:49:55 | INFO     | Salvato: selected_features.json\n",
      "2026-01-28 19:49:55 | INFO     | Salvato: scaler_columns.json (44 colonne)\n",
      "2026-01-28 19:49:55 | INFO     | ‚úì Salvato checksum colonne: 2fd9541623be6663\n",
      "2026-01-28 19:49:55 | INFO     | Salvato: feature_importances.json\n",
      "2026-01-28 19:49:55 | INFO     | Salvato: statistical_preprocessing_info.json\n",
      "2026-01-28 19:49:55 | INFO     | ‚úì Feature engineering completato\n",
      "2026-01-28 19:49:55 | INFO     | ‚úì Artifacts salvati con 44 scaler_columns\n",
      "\n",
      "3. Salvataggio dataset pronti per training...\n",
      "   Salvati in /home/enea/Desktop/NIDS-ML-SSR2/data/processed\n",
      "\n",
      "   Top 10 feature selezionate:\n",
      "2026-01-28 19:49:56 | INFO     | ‚úì Checksum colonne verificato: 2fd9541623be6663\n",
      "2026-01-28 19:49:56 | INFO     | Caricati artifacts da /home/enea/Desktop/NIDS-ML-SSR2/artifacts\n",
      "    1. Packet Length Variance: 0.1326\n",
      "    2. Avg Bwd Segment Size: 0.1218\n",
      "    3. Average Packet Size: 0.0971\n",
      "    4. Subflow Bwd Bytes: 0.0925\n",
      "    5. Fwd Header Length: 0.0580\n",
      "    6. Bwd Packets/s: 0.0461\n",
      "    7. Fwd IAT Mean: 0.0386\n",
      "    8. Fwd IAT Std: 0.0341\n",
      "    9. Fwd Packet Length Max: 0.0308\n",
      "   10. Bwd Packet Length Min: 0.0290\n",
      "\n",
      "‚úì Scaler columns salvate: 44\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETATO\n",
      "============================================================\n",
      "\n",
      "Artifacts: /home/enea/Desktop/NIDS-ML-SSR2/artifacts\n",
      "Shape:     (706,632, 30)\n",
      "Timing:    /home/enea/Desktop/NIDS-ML-SSR2/logs/timing/feature_engineering_v2_20260128_194908.json\n",
      "\n",
      "‚úì Configurazione v2 (DEFAULT):\n",
      "   - Statistical preprocessing: ON\n",
      "   - RobustScaler: ON\n",
      "\n",
      "Prossimo step: python src/training/random_forest.py\n",
      "\n",
      "--------------------------------------------------\n",
      "TIMING SUMMARY - feature_engineering_v2\n",
      "--------------------------------------------------\n",
      "Session: 20260128_194908\n",
      "Total time: 48.26s\n",
      "\n",
      "Operations:\n",
      "  caricamento_dati              :     0.31s (  0.6%)\n",
      "  feature_engineering_pipeline  :    46.89s ( 97.2%)\n",
      "  salvataggio_dataset           :     0.96s (  2.0%)\n",
      "\n",
      "Metrics:\n",
      "  train_samples: 706632\n",
      "  n_features_selected: 30\n",
      "  n_scaler_columns: 44\n",
      "--------------------------------------------------\n",
      "2026-01-28 19:49:56 | INFO     | CPU: 3.2% | RAM: 77.6% | Disponibile: 3.0GB | Core attivi: 14/16\n",
      "\n",
      "‚úì Feature engineering completed in 0:00:48.404233\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"#\" * 70 + \"\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Set up arguments for feature_engineering.main()\n",
    "# Using defaults: statistical preprocessing ON, RobustScaler ON\n",
    "sys.argv = [\n",
    "    'feature_engineering.py',\n",
    "    '--n-jobs', '4'\n",
    "]\n",
    "\n",
    "# Run feature engineering\n",
    "try:\n",
    "    feature_engineering.main()\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"\\n‚úì Feature engineering completed in {elapsed}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR during feature engineering: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Hyperparameter Tuning\n",
    "\n",
    "Runs Bayesian optimization with Optuna to find optimal hyperparameters.\n",
    "\n",
    "**Metric:** 70% F2-Score + 30% Latency (composite score)\n",
    "\n",
    "**Stop Condition:** Whichever comes first: N_TRIALS or TIMEOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# STEP 3: HYPERPARAMETER TUNING\n",
      "######################################################################\n",
      "\n",
      "Model:     lightgbm\n",
      "N Trials:  100\n",
      "Timeout:   60s (0.02h)\n",
      "CV Folds:  5\n",
      "\n",
      "Note: Will stop when EITHER limit is reached\n",
      "\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "Modello:      lightgbm\n",
      "Metodo:       Bayesian Optimization (Optuna)\n",
      "Metrica:      70% F2-Score + 30% Latency (composite)\n",
      "Task:         binary\n",
      "CV:           5\n",
      "Max Latency:  1.0ms/sample\n",
      "CPU:          4/16\n",
      "Limiti:       100 trials OPPURE 60s (0.0h)\n",
      "              (si ferma al primo raggiunto)\n",
      "\n",
      "1. Caricamento dati...\n",
      "2026-01-28 19:49:57 | INFO     | Caricati: train=706,632, val=151,422, test=151,422\n",
      "2. Preparazione feature...\n",
      "2026-01-28 19:49:57 | INFO     | ‚úì Checksum colonne verificato: 2fd9541623be6663\n",
      "2026-01-28 19:49:57 | INFO     | Caricati artifacts da /home/enea/Desktop/NIDS-ML-SSR2/artifacts\n",
      "   Scaler feature alignment: 44 feature richieste\n",
      "   Shape: (706632, 30)\n",
      "\n",
      "3. Tuning (Bayesian Optuna)...\n",
      "   NOTA: Misura latency durante CV, rallenta il processo\n",
      "2026-01-28 19:49:57 | INFO     | Bayesian Optimization (Optuna): cv=5\n",
      "2026-01-28 19:49:57 | INFO     | Metrica: 70% F2-Score + 30% Latency\n",
      "2026-01-28 19:49:57 | INFO     | Max latency constraint: 1.0ms/sample\n",
      "2026-01-28 19:49:57 | INFO     | Limiti: 100 trials OPPURE 60s (0.0h) - si ferma al primo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enea/Desktop/NIDS-ML-SSR2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Best trial: 16. Best value: 0.998877:  19%|‚ñà‚ñâ        | 19/100 [01:02<04:24,  3.27s/it, 62.13/60 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 19:51:00 | INFO     | Best composite score: 0.9989\n",
      "2026-01-28 19:51:00 | INFO     |   - F2-Score: 0.9989\n",
      "2026-01-28 19:51:00 | INFO     |   - Latency: 0.0001ms/sample\n",
      "2026-01-28 19:51:00 | INFO     | Completed trials: 19\n",
      "2026-01-28 19:51:00 | INFO     | Best params: {'n_estimators': 8, 'max_depth': 22, 'learning_rate': 0.16980141538927607, 'num_leaves': 297, 'subsample': 0.9909175233513504, 'colsample_bytree': 0.8832829939444796, 'min_child_samples': 27, 'reg_alpha': 0.016109046194566698, 'reg_lambda': 3.709876048497122e-07, 'class_weight': 'balanced'}\n",
      "\n",
      "4. Salvataggio risultati...\n",
      "\n",
      "======================================================================\n",
      "TUNING COMPLETATO\n",
      "======================================================================\n",
      "\n",
      "Best composite score: 0.9989\n",
      "  - F2-Score: 0.9989\n",
      "  - Latency:  0.0001ms/sample\n",
      "\n",
      "Trials completati: 19\n",
      "Tempo totale: 62.1s (0.02h)\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 8\n",
      "  max_depth: 22\n",
      "  learning_rate: 0.16980141538927607\n",
      "  num_leaves: 297\n",
      "  subsample: 0.9909175233513504\n",
      "  colsample_bytree: 0.8832829939444796\n",
      "  min_child_samples: 27\n",
      "  reg_alpha: 0.016109046194566698\n",
      "  reg_lambda: 3.709876048497122e-07\n",
      "  class_weight: balanced\n",
      "\n",
      "Risultati salvati: /home/enea/Desktop/NIDS-ML-SSR2/tuning_results/lightgbm/bayesian_trials19_cv5_2026-01-28_19.51.json\n",
      "\n",
      "Prossimo step:\n",
      "  python src/training/lightgbm.py\n",
      "\n",
      "‚úì Hyperparameter tuning completed in 0:01:03.904902\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# STEP 3: HYPERPARAMETER TUNING\")\n",
    "print(\"#\" * 70 + \"\\n\")\n",
    "\n",
    "print(f\"Model:     {MODEL_TYPE}\")\n",
    "print(f\"N Trials:  {N_TRIALS}\")\n",
    "print(f\"Timeout:   {TIMEOUT}s ({TIMEOUT/3600:.2f}h)\")\n",
    "print(f\"CV Folds:  {CV_FOLDS}\")\n",
    "print(f\"\\nNote: Will stop when EITHER limit is reached\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Set up arguments for hyperparameter_tuning.main()\n",
    "sys.argv = [\n",
    "    'hyperparameter_tuning.py',\n",
    "    '--model', MODEL_TYPE,\n",
    "    '--n-trials', str(N_TRIALS),\n",
    "    '--timeout', str(TIMEOUT),\n",
    "    '--cv', str(CV_FOLDS),\n",
    "    '--n-jobs', '4'\n",
    "]\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "try:\n",
    "    hyperparameter_tuning.main()\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"\\n‚úì Hyperparameter tuning completed in {elapsed}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR during hyperparameter tuning: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete!\n",
    "\n",
    "**Output Location:**\n",
    "- Tuning results saved in: `tuning_results/<model_type>/`\n",
    "- Processed data in: `data/processed/`\n",
    "- Feature engineering artifacts in: `artifacts/`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Review the tuning results JSON file in `tuning_results/<model_type>/`\n",
    "2. Run the training script with the tuned parameters:\n",
    "   ```bash\n",
    "   python src/training/<model_type>_model.py\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PIPELINE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úì Latest tuning result: bayesian_trials19_cv5_2026-01-28_19.51.json\n",
      "\n",
      "Key Metrics:\n",
      "  Best Composite Score: 0.9989\n",
      "  Best F2-Score:        0.9989\n",
      "  Best Latency:         0.0001 ms/sample\n",
      "  Trials Completed:     19\n",
      "  Search Time:          62.1s\n",
      "\n",
      "Best Parameters:\n",
      "  n_estimators        : 8\n",
      "  max_depth           : 22\n",
      "  learning_rate       : 0.16980141538927607\n",
      "  num_leaves          : 297\n",
      "  subsample           : 0.9909175233513504\n",
      "  colsample_bytree    : 0.8832829939444796\n",
      "  min_child_samples   : 27\n",
      "  reg_alpha           : 0.016109046194566698\n",
      "  reg_lambda          : 3.709876048497122e-07\n",
      "  class_weight        : balanced\n",
      "\n",
      "======================================================================\n",
      "NEXT STEP: Run training with tuned parameters\n",
      "======================================================================\n",
      "\n",
      "Command:\n",
      "  python src/training/lightgbm_model.py\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find the most recent tuning result\n",
    "tuning_dir = Path(f\"tuning_results/{MODEL_TYPE}\")\n",
    "\n",
    "if tuning_dir.exists():\n",
    "    json_files = sorted(tuning_dir.glob(\"*.json\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    if json_files:\n",
    "        latest_result = json_files[0]\n",
    "        \n",
    "        print(f\"\\n‚úì Latest tuning result: {latest_result.name}\")\n",
    "        \n",
    "        # Load and display key metrics\n",
    "        try:\n",
    "            with open(latest_result, 'r') as f:\n",
    "                result_data = json.load(f)\n",
    "            \n",
    "            print(f\"\\nKey Metrics:\")\n",
    "            print(f\"  Best Composite Score: {result_data.get('best_score', 'N/A'):.4f}\")\n",
    "            print(f\"  Best F2-Score:        {result_data.get('best_f2_score', 'N/A'):.4f}\")\n",
    "            print(f\"  Best Latency:         {result_data.get('best_latency_ms', 'N/A'):.4f} ms/sample\")\n",
    "            print(f\"  Trials Completed:     {result_data.get('search_config', {}).get('n_trials', 'N/A')}\")\n",
    "            print(f\"  Search Time:          {result_data.get('search_config', {}).get('search_time_seconds', 0):.1f}s\")\n",
    "            \n",
    "            print(f\"\\nBest Parameters:\")\n",
    "            for param, value in list(result_data.get('best_params', {}).items())[:10]:\n",
    "                print(f\"  {param:20}: {value}\")\n",
    "            \n",
    "            if len(result_data.get('best_params', {})) > 10:\n",
    "                print(f\"  ... and {len(result_data.get('best_params', {})) - 10} more parameters\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  (Could not load result details: {e})\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No tuning results found in {tuning_dir}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Tuning results directory not found: {tuning_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEP: Run training with tuned parameters\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCommand:\")\n",
    "print(f\"  python src/training/{MODEL_TYPE}_model.py\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Package Results (Kaggle Only)\n",
    "\n",
    "Creates a zip file with artifacts and tuning results for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PACKAGING SKIPPED (Local Environment)\n",
      "======================================================================\n",
      "\n",
      "Results are already in your local directories:\n",
      "  - artifacts/\n",
      "  - tuning_results/lightgbm/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if ENV == \"kaggle\":\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PACKAGING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create timestamp for unique filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"nids_tuning_{MODEL_TYPE}_{timestamp}.zip\"\n",
    "    zip_path = Path(\"/kaggle/working\") / zip_filename\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        \n",
    "        # Add artifacts directory\n",
    "        artifacts_dir = Path(\"artifacts\")\n",
    "        if artifacts_dir.exists():\n",
    "            print(f\"\\nüì¶ Adding artifacts...\")\n",
    "            for file in artifacts_dir.rglob(\"*\"):\n",
    "                if file.is_file():\n",
    "                    arcname = file.relative_to(Path.cwd())\n",
    "                    zipf.write(file, arcname)\n",
    "                    print(f\"   ‚úì {arcname}\")\n",
    "        \n",
    "        # Add tuning results for this model\n",
    "        tuning_dir = Path(f\"tuning_results/{MODEL_TYPE}\")\n",
    "        if tuning_dir.exists():\n",
    "            print(f\"\\nüì¶ Adding tuning results...\")\n",
    "            for file in tuning_dir.glob(\"*.json\"):\n",
    "                if file.is_file():\n",
    "                    arcname = file.relative_to(Path.cwd())\n",
    "                    zipf.write(file, arcname)\n",
    "                    print(f\"   ‚úì {arcname}\")\n",
    "        \n",
    "        # Add processed data info (just metadata, not the actual large files)\n",
    "        processed_dir = Path(\"data/processed\")\n",
    "        if processed_dir.exists():\n",
    "            # Add only JSON files (mappings, etc.)\n",
    "            print(f\"\\nüì¶ Adding data metadata...\")\n",
    "            for file in processed_dir.glob(\"*.json\"):\n",
    "                if file.is_file():\n",
    "                    arcname = file.relative_to(Path.cwd())\n",
    "                    zipf.write(file, arcname)\n",
    "                    print(f\"   ‚úì {arcname}\")\n",
    "    \n",
    "    # Get file size\n",
    "    size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì RESULTS PACKAGED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nZip file: {zip_filename}\")\n",
    "    print(f\"Size:     {size_mb:.2f} MB\")\n",
    "    print(f\"Location: /kaggle/working/{zip_filename}\")\n",
    "    print(\"\\nDownload this file from Kaggle output to use in local training.\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PACKAGING SKIPPED (Local Environment)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nResults are already in your local directories:\")\n",
    "    print(\"  - artifacts/\")\n",
    "    print(f\"  - tuning_results/{MODEL_TYPE}/\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
